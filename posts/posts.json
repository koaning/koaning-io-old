[
  {
    "path": "posts/research-advocacy/",
    "title": "Being a Research Advocate.",
    "description": "What lies in \"Research Advocacy\"?",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2022-07-28",
    "categories": [],
    "contents": "\nIn my previous role I worked at Rasa as a “Research Advocate”. I don’t know of anybody else with the same job title and it was certainly an unconventional job.\nSo many people keep asking me the same question:\n In this blogpost I’d like to shed some light on that, while also giving some advice to anyone who wants to be in the world of developer advocacy.\nBefore going there though, I need to acknowledge that during my final week at Rasa a very large layoff got announced. It came out of the blue and while it did not affect me (I was changing companies already) it did affect a lot of my former colleagues. The situation is beyond awkward for both those who were let go as well as those who stayed behind. Rasa treated me very well during my time there, but the layoff should’ve been handled better.\nWith that said, let’s talk about research advocacy.\nHow it started\nRasa is a company that makes software that allows you to build virtual assistants. Just like elasticsearch makes software to run your own search engine, Rasa allows you to create your own chatbot.\nUnlike elasticsearch, a significant chunk of the Rasa stack revolves around a machine learning pipeline that needs to be customized. If a user misinterprets an aspect of machine learning, or misconfigures the pipeline, the chatbot will perform poorly.\nExtra details.\nThe fact that there’s a lot of hype in the field wasn’t helping but it also doesn’t help that Rasa’s algorithmic stack is relatively complex. Just before I joined, Rasa was just about to release their DIET algorithm for intent classification and entity extraction. They also had another algorithm, called TED, which could predict the following action in the conversation by using the entire conversation so far.\nBoth algorithms had clever ideas in them, achieved good results and even had papers written about them. They used the transformer architecture under the hood as well as a trick from the starspace paper. The design for both algorithms was very sensible, but it was a lot to take in.\nThis posed a challenge for the company. If users don’t appreciate the design choices in these ML algorithms, they may accidentally misinterpret a feature and end up with a poorly performing assistant. The research team wrote papers about their research, but that’s not sufficient for the general audience. There was a consensus in the company that Rasa needed to invest into educational material such these algorithms were better understood. They were building a developer relations team, and they wanted somebody to focus into their algorithms.\nAround the same time, their CTO discovered my drawdata experiment, read one of my blog posts and reached out. Coincidentally, when he reached out, I was in Berlin. So I dropped by their office to have a Club Matte to exchange ideas. I remember having an enjoyable conversation about the dangers of hype in machine learning before heading back to the conference I attended.\nThere’s a funny side-note to that moment: I was completely unaware that there was even an intent to try to hire me. I recall that the CTO was interested in “collaborating,” but it took me well over a month before I realized that he wasn’t talking about a blog post.\n\nMy wife would later comment that I indeed have a very hard time realizing when somebody is flirting with me.\nThe Job\nEventually, I accepted the job offer. But it was clear the role itself was an experiment from the beginning. There was a hole to be filled, but it wasn’t clear “how.”\nRasa made a job description, but it was so experimental that the CTO wasn’t even sure what job title to give it. We all knew I had to do “something between devrel and research,” which is why they decided on “Research Advocate,” but nobody really knew what that meant. The job description was clear on what kind of person might fit the role, but the actual activities were a bit vague.\n\nThis comment still exists in the original Google doc. This was normal because they invented a new role but it also meant that part of being the research advocate was to figure out what a research advocate does.\nA snippet of the job description for the role.\nResearch Advocate\nWe don’t draw a hard line between our research and engineering teams, we all work on the same stack and share work, knowledge, and tools. Communicating well about the latest research in our field (both our own work and others’) is extremely valuable for our community. NLP is a fast-moving field, and we are looking for a Research Advocate to help our community understand “what does this new result mean for me?”.\nAs a Research Advocate you will report to the head of developer relations, and will work closely with our Developer Relations and Research teams on Rasa’s developer education efforts, focused on research. You will build prototypes, demos, give talks, write blog posts, and create any other content you believe will best serve our worldwide developer community.\nABOUT YOU\nYou have a technical background, you love digging into the new research topics, replicating and testing results, and building new things from scratch. Above all, you take pride in helping others to understand difficult concepts and build great things. You are excited about exploring different research areas and communicating the results to wide and diverse developer audiences.\nRequired skills:\nAdvanced knowledge in Python\nIndustry experience in applying machine learning algorithms on various problems\nExperience in public speaking and/or technical writing\nStrong communication skills and ability to explain complex ideas, algorithms, and technologies to diverse audiences\nYou are comfortable with mathematics behind machine learning\nTHINGS YOU WILL DO\nWe’re a startup, so you’ll have to be comfortable rolling up your sleeves and doing whatever is required to support our mission. However, you can definitely expect to:\nCreating technical education content explaining different research topics spanning across the field of conversational AI\nReplicate and test models described in different research papers\nTinker around and build new projects using the bleeding edge models and technologies\nWork closely with the research and developer relations teams to help us shape the direction of developer education at Rasa\nPERSONAL PROJECTS\nThe ideal candidate for this role will very likely have an established online presence in the data science / machine learning / developer community and will likely want to continue to work on hobby/personal projects. That’s great and we encourage that. For any projects in collaboration with a 3rd party, we would discuss beforehand and agree that there is no conflict of interest.\nThe Work\nI figured as a first step, I should work on educational content. Rasa had some new algorithms in their stack and developers needed to appreciate how they worked. This led me to work on the Algorithm Whiteboard playlist on Youtube. The goal was to eventually release a video per week, but to start making educational content to get users familiar with NLP in chatbotland.\nThe content would be planned ahead, reviewed and scheduled but my schedule left about a day per week for me to run benchmarks and experiments.\nI quickly found a good use for this time. While working on the content for word embeddings I noticed that I needed a tool that would help me visualize how different embeddings behave. This tool was missing, but I really had a use-case for it in my series.\nThis led me to create a helper library called whatlies. It would help you figured out what lies in word embeddings, hence the name.\n\nThe tool turned out to be very convenient and it made very regular appearances in the Youtube channel. But the library was a two-way street. When the community started asking questions about subword embeddings, it was very easy to add support for them. With support added, it was very easy to make content.\nAs the whatlies library started adding support for more and more types of embeddings it also started supporting more and more types of language models. I wanted a quick way to test these tools for classification use-cases so I made these embeddings scikit-learn compatible. The library was already setup in a component structure, so this implementation was super easy to make. The library also had some contributions from outside.\n\nShoutout to Masoud for adding the huggingface support!\nThe scikit-learn API for whatlies.\nimport numpy as np\nfrom whatlies.language import SpacyLanguage\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\npipe = Pipeline([\n    (\"embed\", SpacyLanguage(\"en_core_web_md\")),\n    (\"model\", LogisticRegression())\n])\n\nX = [\n    \"i really like this post\",\n    \"thanks for that comment\",\n    \"i enjoy this friendly forum\",\n    \"this is a bad post\",\n    \"i dislike this article\",\n    \"this is not well written\"\n]\n\ny = np.array([1, 1, 1, 0, 0, 0])\n\n# This pipeline now uses embeddings from spaCy\npipe.fit(X, y)\nThis scikit-learn feature made the library much more popular. But it also led me to wonder how hard it could be to add support for these embeddings in Rasa. The goal of Rasa is to be language independant, and by offering these embeddings we could get feedback from the non-English community. This is great because by making it easier for our community to try out the embeddings, we won’t need to ask for datasets to run the benchmarks ourselves.\nThe research team was eager to give the community more experimental tools, but they were also a bit worried about adding many embeddings to the Rasa library as core components. By adding it to our main library, we wouldn’t just need to support them long term, we would also need to be able to advise our users on when to use them. But since we don’t speak the 275 languages that some of our embeddings support, it felt better to host it as a plugin instead. That way, our community wouldn’t need to implement all the tools, but they could report back and let us know what worked. This is how the rasa-nlu-examples repository got started.\n There was now a library for my experimental Rasa plugins. That meant that whenever I came up with a new trick to try I’d have a place to share the code. This led to a bunch of extra tokenisers, entity extractors and intent classifiers. The research team was excited by this project because they suddenly had a central place where experimental tools could be shared with the community. By sharing these items as a plugin it was clear that the tools were experimental but because it was only a pip install away we were still able to get a lot of feedback. If the feedback was good, we could consider adding it to the core library.\nThe loop\nEventually I started understanding that there was a pattern to what I was doing.\nSometimes I noticed the need for a tool as I was working on content.\nSometimes I was working on a tool, which could immediately be turned into content!\n\nIt also started occuring to me that I didn’t need to play by the same rules as some of my colleagues. Since my primary focus was education it didn’t matter what I was working on, so as long as I could produce relevant content on a regular basis.\nThis led me to work on even more videos, plugins, tools and Rasa features. Here’s a selection.\nI made taipo, a data augmentation/quality tool for Rasa that supports typo generation, transliteration and bad label detection. Each topic was explained in more detail on the algorithm whiteboard.\nI added a rasa-action-examples repository to highlight how custom Python code could be used from a chatbot. It lists a demo that can generate buttons as a fallback during a conversation or resort to a search engine on behalf of the user.\nI made rasalit which is a collection of streamlit apps that help users understand their Rasa models better. This tool made an appearance whenever I was running a grid-search for a Rasa model.\nI added spaCy 3.0 support to Rasa. This added support for more languages but also made it easier for folks to re-use their trained spaCy models in a chatbot as well.\nI improved the rasa --version command by giving it more elaborate output that’s relevant for debugging. This made it much easier for us to understand Github issues.\nI also made a popular bulk labelling demo which combines a trick from my human-learn together with whatlies that allows you to assign a name to clusters of text. The demo comes with a user-interface and led to a lot of happy users. It turned out to be a very useful tool when you’re just getting started with a virtual assistant and you’re interested in finding intents to get started with.\nI also wrote pytest-duration-insights. This was a personal project, but it was inspired by some of the efforts at Rasa to reduce the unit-test times. I later learned that it was used at a bunch of places, including the scipy project.\nWhy so productive?\nAll of this code made a fair amount of impact. But I think the reason why I’ve been able to work on this code in the first place is related to the “we don’t know what it means to be a research advocate”-aspect of my role. Because my role mainly required me to make content, nobody complained if I was doing something experimental in order to make content. As long as the content was relevant, it’d be time well spent!\nThat also meant, and this is crucial, that I never had to ask permission from anyone to work on a tool. I would later hear stories from former colleagues that they were slightly jealous of me in this regard. Most researchers or engineers in the company would need to attend meetings and defend features before they could be implemented. But since all of my tools were a by-product from the content, I was free to keep building plugins. Just as long as the steady stream of content kept coming.\nIt’s not like I never had meetings, but I want to make an observation out of this because it might serve as great advice for people who want to enter developer advocacy land. It’s likely that your job will revolve around making interesting content, but that typically gives you more freedom to get creative with the stack. Use that freedom!\nI should also admit that Rasa was the first time I was doing both developer relations and NLP. I was familiar enough with machine learning to be able to make progress on my own. But I was unfamiliar enough with NLP that I didn’t really know what I was doing. This combination allowed me to easily just “try out a bunch of things”. If the approach failed, I could still make relevant content, given that I could explain why it failed. Clearly describing failure scenarios is a great theme for the field that we’re in.\n\nHint: if you want to learn about NLP, I now advise folks to also peek at linguistics. Language is more than a bag of words.\nProcess\n\nAs the company started growing, I learned that my by-products got re-used within the company at a bunch of unexpected places.\nThe research team started using the algorithm whiteboard as part of their on-boarding procedure. If the videos helped our community members understand our algorithms, they could also help new employees understand the internals quicker.\nThe whatlies library was submitted and accepted as a paper (thanks to the collaboration with Rachael and Thomas) at the NLP Open Source Software workshop at ACL 2020.\nSome of my bulk labelling tools were used by some of my colleagues in the “community success”-team to help clients kickstart their assistants. Turns out: these experiments really work!\nSome of the videos started getting very popular. It seems that my way of explaining attention mechanisms literally caught the attention of folks. So much so that our recruiter told me it’s how some of our applicants learned about Rasa.\nSome of my algorithm whiteboard videos could be moved into smaller playlists. There was a small sequence on transformers and another on bias in word embeddings. This turned out to be beneficial when we were building our own learning center. By having these smaller video series ready, we were able to re-package them as courses on the landing page. That meant that the learning center was able to launch with multiple courses right away.\nThe embeddings in the rasa-nlu-examples library didn’t just get used by our community, it also got the attention from some of our prospects. Turns out, a lot of companies have non-English customers and they need more than “just” English pipelines. Having a plugin around makes it easier to try out some components certainly helps getting started.\nAgain, my primary task at Rasa was related to education, but it was also the reason why many of these secondary benefits were possible.\nIt prompts an interesting question: what if my primary target was determined solely by quarterly plans? If my role wasn’t so experimental and unconventional, would I still have been bale to make such an impact “by accident?”\nPrivilege\nI should immediately admit that I’ve been quite privileged both the assigned role as well as the group of folks around me. Without them it’s very unlikely that I would’ve made the same impact. For starters; I had an entire research team at my disposal who were more than happy to review my work or to refer any relevant papers or blog posts if need be. Each researcher would work on a specific part of the Rasa pipeline and would also have a different way of looking at a problem, which certainly worked as a source of inspiration. The semantic map embeddings from Johannes remains one of my favorite experiments of all time.\nThen there’s the developer advocacy team. My boss Juste had been doing developer advocacy well before I was even aware that it was a job title. Because of this, she really had a keen eye for what would, and what wouldn’t work as great content. My direct colleague Dr. Rachael Tatman has a Phd in linguistics and was leading developer relations at Kaggle before joining Rasa. In my time at Rasa I contributed a good playlist, but the reason we reached over a million views and 20,000 subscribers was because of the careful planning and content creation that she had been working on. I learned a ton from her, both on the devrel front and the NLP front! We also had a dedicated community manager in the team, Emma, who helped us a great deal by organising events as well as our contributor program.\nManaging a community is lot of work, but because we each had our focus we were a very productive team.\nOther things I’ve learned at Rasa\nI learned to draw my favorite venn diagram in data science.\n\nDeveloper advocacy is much bigger than I anticipated. There are conferences for it and as well as specific analytics tools for the field.\nRasa made a conscious decision to keep the developer relations team and the marketing team separate. Both teams had members who were tasked to make content, and the teams certainly collaborated at times, but it was set up so that both teams could focus on different audiences. Looking back, this seems like a strategically valid choice. You end up with a specialization in both teams that seems very beneficial.\nWhile much of my content involved algorithms, the solutions to many of the problems were usually more related to data quality. You need more than pretrained language models to make meaningful predictions.\nThere’s a rediculous amount of hype in ML. There’s still a lot of folks saying that king - man + woman = queen even though there’s ample evidence that many embeddings don’t show this behavior at all.\n\nRasa had a great Slack culture. People took the effort of thinking before writing, messages had proper spelling and people genuinely tried to keep the threads on topic. In a remote work environment it really makes a big difference.\nThe WACOM tablet was the best investment in communication I’ve ever made. It was of great help in my videos, but it was also amazing when answering questions on the Rasa forum.\n\nRemote work can totally work, but it helps if the work is easily distributable. Doing content work fits perfectly in this because the only bottleneck is the review. But larger changes to the code base need a lot of syncing, which is hard when the workforce is distributed across timezones.\nLanguage is more than a bag of words.\nDoing support work is great. It makes you understand the pain points in your software and it’s a great source of inspiration for content. More people should do it.\nNine times out of ten, the solution to a chat bot problem appears naturally when you just label some data yourself.\nTraining universally useful pre-trained language models is friggin’ hard while training a model for a small but well understood use-case is usually much easier.\n\nMany people like to complain about bad chatbots, but there are some true gems out there. The coolest story I know of involves a large company with an HR department that people didn’t like interacting with. The solution? The HR team would try to automate one HR task a month via the company Slack. The thing that worked very well is that the distance between the chatbot designer and the chatbot user is short, so the feedback was direct. And after a few months, you can imagine the time that was saved internally.\nMany NLP tricks that work for English will totally fail in other languages.\nLike is often the case, a linear model works very well! After running some benchmarks, I felt compelled to leave with an on-brand final PR that implements LogisticRegression for the Rasa stack.\nLooking Back\n\nI don’t really know what it might mean to be a research advocate elsewhere, but at Rasa I’ve seen how the role caused meaningful contributions just by being defined in such an unconventional way. My gut says many other companies might benefit from doing something similar, but it helps to acknowledge that I was in a pretty lucky situation at Rasa.\nLooking Forward\n\nAfter two years of this work, part of me was ready for a change of scenery. My interests started moving beyond chatbots and I wanted to put more focus on engineering, ML user-interfaces, and bulk labelling tricks in general machine learning applications.\nThat’s why March 1st I’ve joined Explosion as a Machine Learning Engineer.\nAnd guess what I’m doing there? I’ve been making some content and it just so happens like there’s a few by-products! There’s spacy-reports, cluestar, embetter and you can even run install bulk now.\n\nThis gives you a proper, but lightweight, bulk labelling interface locally. I’m super excited to work on this! There’s even a full tutorial for it on the Explosion Youtube that shows you how you can use it together with Prodigy.\nSo it looks like I might be able to work in a similar fashion, but for the spaCy, thinc and Prodigy projects. I’ll still be making content now and again, but a large part of my job will also be spent working on consultancy via their tailored pipelines offering. Check it out if you haven’t already!\nAppendix: Whatlies\nThe whatlies project has moved to my personal account so that I can keep maintaining it. Odds are that I’ll keep whatlies around for the visualisation tools but that I’ll port the embeddings to a new library called embetter.\n\n\n\n",
    "preview": "posts/research-advocacy/landing.png",
    "last_modified": "2022-07-30T08:21:19+02:00",
    "input_file": {},
    "preview_width": 867,
    "preview_height": 303
  },
  {
    "path": "posts/enjoy-the-silence/",
    "title": "Enjoy the Silence",
    "description": "GridSearch is Not Enough: Part 7",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2022-01-08",
    "categories": [],
    "contents": "\n\nI frequently write blog posts on why you need more than grid-search to judge a machine learning model properly. This blogpost continues the topic by demonstrating yet another reason; it’s not robust against noise.\nThe full saga:\nPart 1 Part 2 Part 3 Part 4 Part 5 Part 6\nAn Experiment\nWe’re going to do some experiments that are related to noise. The idea is that the training dataset you’re learning from may not perfectly reflect what you might have in production. We should expect drift, and we’d like our models to be robust to changes. To simulate this, we will add noise to our data to see what might happen.\nPart 1: Noise on X_valid\nLet’s start with an experiment where we add noise to the validation set. We will use the make_classification function from scikit-learn to generate a dataset. We’ll split it into X_train and X_valid. Then the X_train set is kept as-is and will be used to train a logistic regression, decision tree, and a random forest.\nCode for training the models.\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=1000, n_features=5, random_state=42)\n\nclassifiers = {\n    \"logreg\": LogisticRegression(),\n    \"tree\": DecisionTreeClassifier(), \n    \"forest\": RandomForestClassifier()\n}\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)\n\nclassifiers[\"tree\"].fit(X_train, y_train)\nclassifiers[\"forest\"].fit(X_train, y_train)\nclassifiers[\"logreg\"].fit(X_train, y_train);\nNow that the models are done training, we’ll check how well different algorithms score on X_valid. We’re not going to stop there though! We’re also going to check if the performance changes if we add noise to X_valid.\nCode for collecting the results\nnp.random.seed(42)\n\ndata = []\nfor noise in np.linspace(0.001, 10, 50):\n    X_tmp = (X_valid + np.random.normal(0, noise, X_valid.shape))\n    for clf in classifiers.keys():\n        data.append({\n            \"classifier\": clf,\n            \"performance\": np.mean(classifiers[clf].predict(X_tmp) == y_valid), \n            \"noise\": noise\n        })\nCode for plotting the results\nimport altair as alt \n\npltr = pd.DataFrame(data)\n\nbase = (alt.Chart(pltr)\n .mark_point()\n .encode(x=\"noise\", y=\"performance\", color=\"classifier\"))\n\np = (base + base.transform_loess('noise', 'performance', groupby=['classifier']).mark_line(size=4))\np.interactive().properties(width=700, title=\"effect of X_test noise on prediction\")\nHere’s what the results of the experiment look like (note that you can click/drag/zoom):\n\nEach point resembles a simulation. The point’s color reflects the machine learning algorithm used to train it. The x-axis shows the noise, and the y-axis shows the accuracy of the validation set. To reduce the jitter, we’ve also added a smoothed loess curve to highlight the global pattern.\nThere are a few things to notice.\nIf there is no noise, the random forest seems to have the highest accuracy.\nLogistic regression is the worst performing model if there’s no noise.\nIf we increase the noise, the decision tree takes a big hit. The random forest also has trouble, but the decision tree seems more brittle.\nThe logistic regression also suffers from the noise, but it’s the most robust out of the three. There’s a considerable margin at higher noise levels.\nYou could certainly argue that this simulation does not reflect reality and that one shouldn’t draw too many conclusions for general machine learning practices. That’s fair.\nBut what will happen when the model is in production? New data that’s coming in is likely going to differ from the old information that you trained on. If that’s something that holds in general, you do want to have algorithms that can withstand changes. That means that maybe, just maybe, we may prefer a simple logistic regression because it’s more robust against noise.\nPart 2: Noise on X_train\nLet’s re-run the experiment but with a twist. Let’s now add noise on X_train. We will now keep X_valid as-is.\nNew code for getting the results\nnp.random.seed(42)\n\ndata = []\nfor noise in np.linspace(0.001, 10, 50):\n    classifiers = {\n        \"logreg\": LogisticRegression(),\n        \"tree\": DecisionTreeClassifier(), \n        \"forest\": RandomForestClassifier()\n    }\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n\n    X_tmp = (X_train + np.random.normal(0, noise, X_train.shape))\n    classifiers[\"tree\"].fit(X_tmp, y_train)\n    classifiers[\"forest\"].fit(X_tmp, y_train)\n    classifiers[\"logreg\"].fit(X_tmp, y_train)\n    \n    for clf in classifiers.keys():\n        data.append({\n            \"classifier\": clf,\n            \"performance\": np.mean(classifiers[clf].predict(X_valid) == y_valid), \n            \"noise\": noise\n        })\n\nThis chart again shows that the logistic regression is more robust to noise, but the differences are more drastic now! This makes sense if you consider that the models can now learn the patterns of the noise instead of the pattern that we’re interested in. It’s much easier to deal with noise if you’re sure that you’ve learned on a pure dataset. If the noise is on the data you’re learning from, there’s more opportunity to overfit on a wrong pattern.\nPart 3: What about GridSearch?\nWhat happens if we train on noisy data, but we also perform a grid-search on the parameters? Will that improve things?\nNew code for getting the results\nfrom sklearn.model_selection import GridSearchCV\n\nnp.random.seed(42)\n\nfor noise in tqdm(noise_range):\n    classifiers = {\n        \"logreg\": GridSearchCV(LogisticRegression(), param_grid={\"C\": [0.1, 1.0, 2.0, 3.0]}, n_jobs=-1),\n        \"tree\": GridSearchCV(DecisionTreeClassifier(), param_grid={\"criterion\": [\"gini\", \"entropy\"], \"splitter\": [\"best\", \"random\"]}, n_jobs=-1),\n        \"forest\": GridSearchCV(RandomForestClassifier(), param_grid={\"n_estimators\": [100, 200, 300, 400]}, n_jobs=-1),\n    }\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n\n    X_tmp = (X_train + np.random.normal(0, noise, X_train.shape))\n    classifiers[\"tree\"].fit(X_tmp, y_train)\n    classifiers[\"forest\"].fit(X_tmp, y_train)\n    classifiers[\"logreg\"].fit(X_tmp, y_train)\n    \n    for clf in classifiers.keys():\n        data.append({\n            \"classifier\": clf,\n            \"performance\": np.mean(classifiers[clf].predict(X_valid) == y_valid), \n            \"noise\": noise,\n            \"situation\": \"noise-and-gridsearch\"\n        })\n\nThe pattern persists. Logistic regression seems the most robust against noise, despite not being the best performing model when there’s zero noise.\nDiscussion\nAgain, I don’t want to suggest that my simulation perfectly reflects reality. It’s extremely unlikely that your dataset is similar to make_classification from scikit learn. It’s evenly absurd to assume that you’re going to see Gaussian noise in production over what you saw in your training data. So please take that grain of salt in mind.\nBut what I do hope to point out here is that a grid search assumes that your data does not change. If it’s likely that your inbound data changes in production, then you’ll need more than standard hyperparameter tuning to find a helpful model. That means that you might select a model that cannot handle noise. That is an effect that this simulation does demonstrate, and it’s a valid concern. It’s also my impression that it’s uncommon in the industry to check for this.\nIt’d be a swell idea to simulate changes that one might expect in production to confirm if your model is robust against them. I don’t want to suggest it’s a straightforward exercise. After all, it’s very tricky to simulate the future without making some assumptions. But not doing this exercise might give you an optimal model… but only if there’s absolute silence.\nAppendix\nPart of the insipration of this blogpost came from this gem of a paper from 2006;\nClassifier Technology and the Illusion of Progress - David J. Hand.\nIt’s a good read, especially the latter part where there’s a discussion of common failures in machine learning. Not much has changed since 2006.\n\n\n\n",
    "preview": "posts/enjoy-the-silence/enjoy-the-silence.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1076,
    "preview_height": 346
  },
  {
    "path": "posts/beyond-broken/",
    "title": "Beyond Broken",
    "description": "Horrible Remedies for Broken Recommenders",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2021-10-21",
    "categories": [],
    "contents": "\nThis is not a happy story.\nAbout a month ago, my then-pregnant partner and I learned that our baby’s heart had stopped beating. Our child, Minne, was to be stillborn.\nIt’s been an emotional rollercoaster, and I would kindly ask everyone not to bring it up if they meet with me in real life. There is, however, one anecdote in this story that deserves to be said out loud.\nI want to talk about how freaking hard it is to get the internet to turn off ads for baby products after having a stillborn child or a miscarriage. I’m happy to report that I may have found a remedy to the problem, but it’s truly a sad state of affairs.\nWhat I’m talking about\nWe did a lot of “how to be a good parent” searches during our pregnancy. It is, therefore, no surprise that online stores figured out that we may be in the market for baby products and that ads started popping up. That said, after what we’ve been through, the last thing we want is to be reminded of baby’s.\nThere are many culprits, but I’d like to zoom in on the ads from bol.com. You may not have heard of the website if you’re not Dutch, but it’s the “Amazon” of the Netherlands. I’m picking this store, not because they are the only culprit, but because their ads went beyond their own website. The day after it happened my partner and I were desperately looking for distraction on YouTube. We were eager to watch a new episode of a playlist we enjoy only to get we’d get ads for bol.com diapers in the middle of a video that was supposed to calm us down.\nI was initially a bit surprised that I even got this ads in the first place. Although Google usually brings me to the book description on bol.com I rarely end up buying my book there in favor of the local book store. Either way, from glancing at my front page, it was pretty clear what bol.com thought of me.\n\n\n\nFigure 1: There are a few books for infants and a banner for baby food.\n\n\n\nI figured I’d reach out to their customer service. If these ads went away, that’d be one less risk of a trigger on our end. Eventually, I got a hold of two people, one via their website and one via their social media channel. Unfortunately, they told me that there was nothing that they could do. They told me that I could turn off their emails and notification, but they could not change the front page of their website or change the ads I received from them on YouTube.\nSidenote.  I tried contacting a human via their chatbot, which led to some incredibly inappropriate responses before it understood that I wanted to talk to a human.\nI should add that the folks I spoke to were understanding and kind, and they wholeheartedly agreed that it was a sorry state of affairs. But in the end, they genuinely could do nothing.\nExcerpt of the dialogue, in Dutch.\nThere are many “this is broken” kinds of thoughts that I experienced while this was happening. Fun fact: I have trained many of the data scientists at bol.com a few years ago. So I knew a few people who still worked there. For a moment, I contemplated reaching out to some of them on LinkedIn to discuss this. But how much impact would that make? If I were to reach out to a senior data scientist at Bol.com, would they even be able to go against their team’s quarterly OKRs to fix this? How much work would it even be? Would it require a complete overhaul? Would I need to do this for every website out there?\nIt’s stuff like this\nFor a few years now, I’ve been trying to understand under what circumstances machine learning systems fail in real life. This anecdote is a perfect example of a common theme; it’s rarely the machine learning technique that’s at fault whenever a system fails. Instead, it’s unintended side-effects combined with an inability to correct a system.\nI’d love, love, to give bol.com more information on what I want. But this information should be interpreted as constraints, not mere click-data. It’s not just about baby products either. To mention another example: I don’t own a driver’s license, so I’d love to be able to explain to my browser that car commercials are pointless.\nSidenote.  This is not a new idea.\nPart of me even thinks it’s a more straightforward “data science”-problem when you ask the user what they want instead of playing the guessing game with tensors. Instead of worrying about an optimal recommendation, maybe there’s something to be said to allow users to customize instead. But why would bol.com ever consider this? Allowing for customization might be something that users are interested in, but it may cause them to customize for products with lower margins.\nThis last bit thought got me thinking. Maybe, just maybe, there was a simple but incredibly awkward remedy to the problem.\nAn Awkward Remedy\nLet’s make an assumption by pretending that bol.com only optimizes towards product margin. I can’t imagine this is actually true, but for arguments sake, let’s say it is. If it really is true it makes my situation very simple. If I want the baby ads to go away, I need to find a product or category with a higher margin and convince bol.com that I am interested in that.\nI tried out a bunch of products, but the best solution seems to be … Chromebooks.\nI spent a few minutes searching for Chromebooks on bol.com. I clicked many links and opened many tabs in an attempt to steer the recommender. After a while the front page no longer had the baby ads, and I even confirmed that YouTube had stopped displaying the diaper ads.\n\n\n\nFigure 2: This is what ‘my’ front page looks like now.\n\n\n\nWhile I’m happy to report this trick seems to work, the fact that this had to be the solution leaves a bad feeling in my stomach. I reported back to customer service and requested they start advising this to people in a similar situation. The folks on the other end responded positively. My impression was that they genuinely didn’t consider this was an option, and I hope they will spread the word.\nReality Check\nI’ve zoomed in on bol.com here, but I should stress that there are many offenders in this space. It’s effortless to sign up for a free “baby box” from Albert Heijn, but it’s disappointingly tough to get them to stop sending you pictures of healthy babies that remind you of a stillborn child. Not just via email, mind you, I’m talking physical letter mail here.\nMost providers of baby boxes will gladly remove you from their system when you call them. You can tell that they deal with this request frequently and that there is a process in place. But in my experience they will also tell you there’s a 4-week waiting period for the changes to cause an effect. To make things worse, when you sign up for a baby box, your data typically gets shared with 3rd parties too. My phone call may have removed me from one database, but nobody dared to guarantee anything about what 3rd parties might do.\nBut here’s the thing: it’s not like my situation is rare. In 2020, 735 children (0.4% of all births) were stillborn in the Netherlands. Another 538 children died within the first 28 days after delivery. About 1/4 pregnancies in the Netherlands don’t make it past the 12th week due to a miscarriage.\nSource\nThis effects many thousands of people. And if any of these people want to stop seeing ads that reminds them of a traumatic experience … the best thing we can do is ask them to click on Chromebooks?!\nWhat about websites like Zalando? Should my wife be required to search the service for high-margin alternatives to maternity clothing in order not to be reminded of what happened? What about instagram? You’re able to click a button that reads “this ad is not relevant for me”. That’s not a bad feature to have but it’s incredibly painful to have to do repeatedly. The button may provide a signal, but it’s by no means an off-switch. It took weeks before the pregnancy ads went away.\nThis is Beyond Broken\nWhenever I walk in the park and see a newborn in a stroller, I’m confronted with a traumatic memory. I couldn’t be happier for the parents, but my heart does sink for a moment every time. The park is something that I am going to have to learn to live with. But am I really to accept that it’s required for me to get an email that reads “everything you need for your baby” with pictures after everything that happened? Just imagine you’d get such an email while you’re at work in the office just before an important meeting. Is that really something we don’t want to be able to turn off immediately, given how many people it affects?\nI hope my “hack the recommender by clicking Chromebooks”-trick might serve people in a similar situation but I hope we can all agree it’s a horrible remedy. The bigger hope that I have is that this piece helps folks recognize that it’s past time we start thinking about building an “off switch” for these systems.\nWithout it, many of our emails, letters, recommenders, searchbars and frontpages remain beyond broken because they are unfixable for the end-user.\n\nFood for Thought\nI also propose we consider this “off-switch” not just for pregnancy related products but also for other products that could cause trauma. A former alcoholic won’t enjoy that beer ad and somebody who is dealing with an eating disorder may not want to see slim-fit diet-ads either.\nAppendix\nIt deserves highlighting that everything that I’m describing here is a well-documented problem. I’m certainly not the first to talk about it. For more information, I might recommend checking:\nThis article by Gillian Brockell. It’s one of the first articles that I’ve been aware of that addresses the same problem. It gives advice on what to do as a facebook user.\nThis small guide on digital bereavement after a miscarriage online.\nThis Dutch resource with all sorts of meaningful and helpful information to aid with stillborn grief.\n\n\n\n",
    "preview": "posts/beyond-broken/minne.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 927,
    "preview_height": 205
  },
  {
    "path": "posts/labels/",
    "title": "Bad Labels",
    "description": "GridSearch is Not Enough: Part Six",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2021-09-02",
    "categories": [],
    "contents": "\n\nI write a lot of blogposts on why you need more than grid-search to properly judge a machine learning model. In this blogpost I want to demonstrate yet another reason; labels often seem to be wrong.\n\nWhat I’ll describe here is also available as a course on calmcode.io.\nBit of Background\nIt turns out that bad labels are a huge problem in many popular benchmark datasets. To get an impression of the scale of the issue, just go to labelerrors.com. It’s an impressive project that shows problems with many popular datasets; CIFAR, MNIST, Amazon Reviews, IMDB, Quickdraw and Newsgroups just to name a few. It’s part of a research paper that tries to quantify how big of a problem these bad labels are.\n\n\n\nFigure 1: The table from the paper gives a nice summary. It’s a huge problem.\n\n\n\nThe issue here isn’t just that we might have bad labels in our training set, the issue is that it appears in the validation set. If a machine learning model can become state of the art by squeezing another 0.5% out of a validation set one has to wonder. Are we really making a better model? Or are we creating a model that is better able to overfit on the bad labels?\nAnother Dataset\nThe results from the paper didn’t surprise me much, but it did get me wondering how easy it might be for me to find bad labels in a dataset myself. After a bit of searching I discovered the Google Emotions dataset. This dataset contains text from Reddit (so expect profanity) with emotion tags attached. There are 28 different tags and a single text can belong to more than one emotion\nThe dataset also has an paper about it which explains how the dataset came to be. It explains what steps have been taken to make the dataset robust.\nThere are 82 raters involved n labelling this dataset. Each example should have been at least 3 people checking it. The paper mentions that all the folks who rated were from India but spoke English natively.\nAn effort was made to remove subreddits that were not safe for work or that contained too much vulgar tokens (according to a predefined word-list).\nAn effort was made to balance different subreddits such that larger subreddits wouldn’t bias the dataset.\nAn effort was made to remove subreddits that didn’t offer a variety of emotions.\nAn effort was made to mask names of people as well as references to religions.\nAn effort was made to, in hindsight, confirm that there is sufficient interrated correlation.\nAll of this amounts to quite a lot of effort indeed. So how hard would it be to find bad examples here?\nQuick Trick\nHere’s a quick trick seems worthwhile. Let’s say that we train a model that is very general. That means high bias, low variance. You may have a lower capacity model this way, but it will be less prone to overfit on details.\nAfter training such a model, it’d be interesting to see where the model disagrees with the training data. These would be valid candidates to check, but it might result in list that’s a bit too long for comfort. So to save time you can can sort the data based on the predict_proba()-value. When the model gets it wrong, that’s interesting, but when it also associates a very low confidence to the correct class, that’s an example worth double checking.\nSo I figured I would try this trick on the Google emotions dataset to see what would happen. I tried predicting a few tags chosen at random and tried using this sorting trick to to see how easy it was to find bad labels. For each tag, I would apply my sorting to see if I could find bad labels in the top 20 results.\nHere’s some of the results:\nLabel = “love” \nWeird game lol\nLooks like it. I didn’t make it, I just found it.\nWow, you people…\nLabel = “not love” \nVery nice!! I love your art! What journal is this? I love the texture on the pages.\nlove love love this. so happy for the both of you.\nI LOVE IT, I would love if they will make season 2… I really enjoyed it\nI love this, my wife told me about something she read on reddit yesterday and I was like…. well just like ol [NAME] here!!!\nLabel = “curiosity” \nI actually enjoy doing this on my own. Am I weird?\nShe probably has a kid by now.\nSo much time saved. Not.\nDidn’t you just post this and people told you it was dumb and not meant for this sub?\nLabel = “not curiosity” \nI cant wait. I’m curious if it will give us any more insight into the incident other than what we already know.\nWhy do you guys hate [NAME]? I’m neutral leaning slightly positive on him. Just curious why the strong negative opinion?\nWhat does that even mean? How does one decide right or wrong with something so vague?\nWait, this is actually a really interesting point. That could/should play a factor if he‘s a legitimate candidate.\nIs it weed? I’m curious to ask if you know what weed smells like?\nLabel = “not excitement” \nI am inexplicably excited by [NAME]. I get so excited by how he curls passes\nOmg this is so amazing ! Keep up the awesome work and have a fantastic New Year !\nI just read your list and now I can’t wait, either!! Hurry up with the happy, relieved and peaceful onward and upward!! Congratulations😎\nI absolutely love that idea. I went on an anniversary trip with a couple once and it was amazing! We had so much fun.\nHappy New Year! Looks like you had a great time there! Cheers! Here’s to a great 2019 hopefully in both baseball and life!\nLabel = “not joy” \nHappy cake day! Have a great day and year, cheers.\nIt’s wonderful and gives me happy happy feels\nHappy to hear this exciting news. Congratulations on your fun-filled morning.\nIt’s good, good, good, good - good good good!\nMy son and I both enjoy taking pictures. It gives us pleasure. Part of the fun for us on vacation is taking pictures of new things.\nLabel = “not gratitude” \nThanks. Nice input as always.\nThanks. I didn’t quite get it from the original. Appreciate the time.\nThis made my hump day. Thank you good sir\nExcellent work thank you for this. This is why I love Reddit.\nYou’re amazing thank you so much!! :)\nI don’t know about you, but many of these examples seem wrong.\nFriggin’ Strange\nBefore pointing a finger, it’d be good to admit that interpreting emotion isn’t a straightforward task. At all. There’s context and all sorts of cultural interpretation to consider. It’s a tricky task to define well.\nThe paper also added a disclaimer to the paper to make people aware of potential flaws in the dataset. Here’s a part of it:\n\nWe are aware that the dataset contains biases and is not representative of global diversity. We are aware that the dataset contains potentially problematic content. Potential biases in the data include: Inherent biases in Reddit and user base biases, the offensive/vulgar word lists used for data filtering, inherent or unconscious bias in assessment of offensive identity labels, annotators were all native English speakers from India. All these likely affect labeling, precision, and recall for a trained model.\n\nAdding this disclaimer is fair. That said. It really feels just a bit too weird that it was that easy for me to find examples that really seem so clearly wrongly labeled. I didn’t run through the whole dataset, so I don’t have a number on the amount of bad labels but I’m certainly worried now. Given the kind of label errors, I can certainly imagine that my grid-search results are skewed.\nWhat does this mean?\nThe abstract of the paper certainly paints a clear picture of what this exercise means for state-of-the-art models:\n\nWe find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by 5%. Traditionally, ML practitioners choose which model to deploy based on test accuracy – our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets.\n\nSo what now?\nMore people should do check their labels more frequently. Anybody is free to try out any trick that they like, but if you’re looking for a simple place to start, check out the cleanlab project. It’s made by the same authors of the labelerrors-paper and is meant to help you find bad labels. I’ve used it a bunch of times and I can confirm that it’s able to return relevant examples to double-check.\nHere’s the standard snippet that you’d need:\nfrom cleanlab.pruning import get_noise_indices\n\n# Find label indices\nordered_label_errors = get_noise_indices(\n    s=numpy_array_of_noisy_labels,\n    psx=numpy_array_of_predicted_probabilities,\n    sorted_index_method='normalized_margin', # Orders label errors\n)\n\n# Use indices to subset dataframe\nexamples_df.iloc[ordered_label_errors]\nIt’s not a lot of effort and it feels like such an obvious thing to check going forward. The disclaimer on the Google Emotions paper checks a lot of boxes, but imagine that in the future they’d add “we checked out labels with cleanlab before releasing it”. For a dataset that’s meant to become a public benchmark, it’d sure be a step worth adding.\nFor everyone; maybe we should spend a less time tuning parameters and instead spend it trying to get a more meaningful dataset. If working at Rasa is teaching me anything, it’s that this would be time well spent.\nSpeaking of Rasa.\nWe’re hiring!!\nAppendix\nI read a few articles about bad labels which I summarised into TILs.\nTIL: Label Errors\nTIL: Confidence vs. Variability\nTIL: Plenty of Labels\n\n\n\n",
    "preview": "posts/labels/land.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1076,
    "preview_height": 346
  },
  {
    "path": "posts/mac-nuc-ssh/",
    "title": "My New Home Setup",
    "description": "Better Patterns for Development Work.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2021-08-15",
    "categories": [],
    "contents": "\nA while ago I switched my home setup for development and it’s been such a success that I decided writing a short blogpost about it.\n\nThe setup involves three machines.\nI own an Intel NUC. This machine runs popOS (a variant of ubuntu) and is meant to run heavyweight code related things. It typically runs docker, databases and machine learning scripts that take a while. It also serves as a data server where all of the larger datasets that I use are stored.\nI also bought the cheapest M1 mac mini that has 16GB of RAM. This machine runs all of my creative software. It works with my Wacom One drawing screen as well as all of my microphones and screen-recording software. This machine is also a user-interface of sorts for my NUC. I mainly run visual studio code via ssh, the code itself runs on my NUC but I can still use my big screen on my Mac as well as all of the snippet tools that I am used to.\nI also own a laptop from my employer. This is an Intel Mac (the one with the terrible keyboard). It is able to run all of the aforementioned software when I’m on the road but it is also able to SSH into my NUC. This laptop is also the only machine that runs social media and slack. The idea is that this laptop is my distraction machine that I can physically turn off by shutting down the lid.\nThings that work out very well\nThere’s so many benefits to this approach.\nI can run a heavy machine learning algorithm while recording. Since the machine that is running the recording software isn’t the same machine that is running the machine learning script I suddenly don’t need to worry about any lag when I’m making educational content.\nIn general, the NUC is a much better deal in terms of hardware. This is especially true for the disk, especially when you compare it with the apple hardware. It’s much cheaper to invest in a second lightweight linux machine than it is to upgrade the specs on an apple device.\nUpgrading an apple device is near impossible (boo apple!). But upgrading the NUC is a breeze. I got 32Gb of RAM but nothing prevents me from upgrading it to 64GB in the future.\nVisual Studio Code has really nailed the SSH feature. It’s a native experience, really. But the killer feature here is that all the things that currently don’t run on the M1 chip (tensorflow, docker) all run extremely well on an Intel machine running linux.\nI certainly need to use slack and I also need to check into social media from time to time. But the nice thing about limiting that to just my laptop is that I’m literally able to “shut the lid on distractions”. It’s proven to be a huge reducer of stress.\nI could have chosen not to go with the Intel NUC in favor of something that’s heavier. But, with 12 threads, the NUC is pretty darn good for the small form factor. It’s also pretty light on energy consumption since the hardware is more like a laptop than a server. I also think it’d be a bad idea to go for heavier hardware. If I had bought a big machine with GPUs then I’d have huge fans spinning on my desk and I’d be writing my software on a machine that doesn’t represent the average persons availability for compute.\nIf I wanted to, I could totally bring the M1 Mac and the NUC with me! Sure, there’d be some fumbling with cables involved but both totally fit inside of my backpack. It’d be very easy for me to move my entire setup for a few days if I wanted to work from my parents house for a week.\nTheoretically, you could also replace my intel NUC with a VM in the cloud. My setup however, doesn’t require the internet. It only needs the network in my home.\nShould any of my machines break, I can still be productive enough with the other machines in my house.\nFinal Remarks\nEspecially with the advent of home work, a lot of us lucky developers can rethink what our ideal setup might be like. I don’t like the idea of investing in big apple machines that aren’t upgrade-able and this NUC setup really strikes a balance. I imagine at some point in the future, when laptop manufacturers finally get their keyboards in order, this setup might also work for windows machines. The laptop can be become much more of a user interface, where an upgrade-able linux machine can run all development software you’ll need.\n\n\n\n",
    "preview": "posts/mac-nuc-ssh/setup.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 972,
    "preview_height": 352
  },
  {
    "path": "posts/predict-limits/",
    "title": "Predicting Limits",
    "description": "Multiple Outputs Make Sense.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2021-06-28",
    "categories": [],
    "contents": "\nScikit-Learn is a package that I have been using for 5+ years and to this day I still discover new components when I read the docs. In particular, I learned about a meta model called MultiOutputClassifier the other day. It’s a class that allows you to copy the pipeline to predict multiple outputs. Besides the classification variant there’s also a MultiOutputRegressor for regression.\n\nI’m a big fan of meta models. They allow you to add post-processing behavior to a scikit-learn pipeline. The MultiOutput-models will take a model as input and create a copy for multiple labels. That means that you can predict multiple labels from the same dataset \\(X\\).\nThis sounds like a useful trick but I had a bit of trouble finding an obvious usecase. I was immediately convinced that it’s a cool tool but I had a bit of trouble finding an example that clearly demonstrates the merit.\nThink about it. Usually if you’d like to predict a label you’d want to make a custom pipeline for it. So when would you want to predict multiple labels from a single dataset with the same underlying model? Why would you?\nIntervals\nLet’s consider a dataset from a noisy sensor.\n\nimport numpy as np\nimport matplotlib.pylab as plt \n\nx = np.linspace(0, 5, 50)\ny = x * 2 + np.sin(2*x)\n\nplt.plot(x, y, c='black')\nplt.errorbar(x, y, yerr=np.sin(x), fmt='k');\nLet’s say we have a dataset \\(x, y\\) that’s noisy. Depending on the value of \\(x\\) we have a unreliable value \\(y\\) that comes with an upper and lower limit. That means that a sample from the distribution above might look like:\n\nxs = np.linspace(0, 5, 50)\nys = xs * 2 + np.sin(2*xs)\nnoise = np.random.uniform(0, 2, len(xs))-1\nobs = ys + np.sin(xs) * noise\n\nplt.errorbar(x, obs, yerr=np.sin(x), fmt='.k');\nIn such a dataset we’d have three labels \\(y_{\\text{lower}}, y_{\\text{middle}}, y_{\\text{upper}}\\) that we want to predict for every \\(x\\).\nimport pandas as pd\n\ndf = (pd.DataFrame({'x': xs, \n                    'y': ys + np.sin(xs)*(np.random.uniform(0, 2, len(xs))-1)})\n        .assign(y_upper=lambda d: d['y'] + abs(np.sin(x)))\n        .assign(y_lower=lambda d: d['y'] - abs(np.sin(x))))\nAnd … this might be a use-case for a meta classifier! We’d want a model for each label and for consistency we can argue that we want the same model for all of these labels. It’s a simple implementation too.\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nclf = MultiOutputRegressor(KNeighborsRegressor())\nclf.fit(df[['x']], df[['y_lower', 'y', 'y_upper']])\npreds = clf.predict(df[['x']])\n\nplt.plot(df['x'], preds[:, 1])\nplt.fill_between(df['x'], preds[:, 0], preds[:, 2],\n                 color='gray', alpha=0.2)\nplt.errorbar(x, y, yerr=np.sin(x), fmt='k');\n\nI’m using a nearest neighbor model here but you can use any scikit-learn classifier.  For more info, see the tutorial on calmcode.\nHere’s what we predict with this model.\n\nIf we overlay the original chart, we can see that it’s a match too!\n\nConclusion\nGranted, this blogpost shows a very simple example. What I like about it though is that scikit-learn always seemed to have a bit of trouble when it comes to dealing with confidence intervals. But if you’ve got a dataset with uncertainty estimates then scikit-learn can learn them easily via the MultiOutputRegressor-meta model.\n\n\n\n",
    "preview": "posts/predict-limits/mod.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1442,
    "preview_height": 524
  },
  {
    "path": "posts/just-another-dangerous-situation/",
    "title": "Naive Bias[tm] and Fairness Tooling",
    "description": "Just Another Dangerous Situation",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2021-05-05",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: I want to talk about this phenomenon.\n\n\n\nIn the beginning of this year I noticed a press release from Boston Consulting Group. It seems they have an analytics group called BCG Gamma which released a new scikit-learn compatible tool called FACET. To quote the capitalized headline, it’s a tool that helps you “Understand Advanced Machine Learning Models So They Can Make Better and More Ethical Decisions”.\n\n\n\nFigure 2: Quite the Announcement\n\n\n\nI’ve done some open-source work in the area of algorithmic fairness so I was immediately curious. I went to the documentation page to have a look. By looking around I learned it was a package that offers a lot of visualisation tools, some of which were inspired by Shapley values.\n\n\n\nFigure 3: There were loads of visualisation tools.\n\n\n\nUnfortunately, between all of these interpretability charts there was a big red flag.\nload_boston\nThe main tutorial on the documentation page was using a dataset from scikit-learn know as the load_boston dataset. The goal of the dataset is to predict house prices and it is commonly used as an example on what machine learning can do. You can read more about the dataset on the API documentation and in the user guide.\nThis dataset is notorious because of one variable that is in that dataset.\n> print(load_boston()['DESCR'])\n  ...\n  - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\nI remember being shocked when I first realised that scikit-learn was hosting a dataset that uses skin color to predict a house price. Did nobody else notice? It’s what made me start learning more about algorithmic fairness and it led to this scikit-learn transformer and this popular talk. There’s a deeper story behind this parameter worth reading too.\nAt any rate, it’s clearly inappropriate. So I figured I’d do the right thing by notifying the maintainers on GitHub and asking them to replace the tutorial.\n\n\n\nFigure 4: Screenshot of the GitHub issue.\n\n\n\n\nTo anybody curious, yes, the dataset is up for removal from scikit-learn too. It’s a pretty big effort too because many tutorials need an update.\nI want to acknowledge that the group responded swiftly and they fixed their documentation within a day. That said, this is the perfect example of a growing concern on my end.\nOuch\nThe painful part isn’t just that this dataset was used on a documentation page. The issue is that this page will be seen as an authoritative source on how to properly “do the fairness thing”. It shows you how to use the tricks of the package, but it omits the need to understand the data before you even try to model it.\nAnd here’s the scary part: we’re not talking about a hobby project here. Facet is actively promoted by the Boston Consultancy Group, a strategy consultancy firm with a global reach. One can expect this tool to be promoted and used by a lot of their clients. Imagine the harm that can be done if people start considering their algorithms “fair” or “ethical” merely because they’re using a package.\nNaive Bias[tm]\nThe problem in the tutorial isn’t that the visualisation techniques can’t be applied in a meaningful way for fairness. It’s that the technique seems to have been distracting from the actual problem in the dataset. And it’s that what worries me.\n\n\n\nFigure 5: Let’s call this phenomenon Naive Bias[tm].\n\n\n\nThere’s a lot of noble work to be done in the realm of fairness tooling and it’s an area that certainly deserves plenty of attention. But I fear it’s attracting a wrong attitude. We can try to make the model more constrained, more monotonic, more bayesian, with more Shapley values or more causal … but that alone won’t “solve fairness” in any meaningful sense.\nIt’d be a true shame if people consider a python library as a checkbox so that they no longer need to think about the information that is hidden in their data.\nIt Pops Up a Lot\nWhat doesn’t help is that this isn’t the only documentation page with this issue. A few months after the FACET mishap I found myself on the causalnex repository. This is a project from QuantumBlack, which is the advanced analytics group part of McKinsey. Also here, there was a suggestion that causal models might be able to make the world more fair. Also here, we’re talking about a company with a global reach. Also here, the documentation page was using the load_boston dataset. Therefore, also here, I opened a GitHub issue.\n\n\n\nFigure 6: Screenshot of another GitHub issue.\n\n\n\nI want to give the causalnex group credit where credit is due because they too immediately recognized the issue and started replacing their example. This group even invited me to speak and make my concerns clear to the maintainers of the project. They’re even taking the effort of adding data-cards to all of their examples. But the concern remains; we may be in for a wave of fairness tools that, albeit with best intentions, will distract us from reality.\nIt’s not just the load_boston dataset that has me concerned though. There’s a whole zoo of datasets that deserve explicit acknowledgement of their bias. The launching demo of Scailable, a Dutch startup that wants to web-assebmle all the machine learning models, had a demo that used the adult dataset. This dataset allows you to train a model to predict the salary of an employee. Unfortunately, gender is one of the variables used to make the prediction.\nThe painful part here wasn’t just that members of the group initially though it was fine because they had a small disclaimer on the page. Rather, it was that the founding members of the group all teach data science in Dutch universities. So again we’re talking about a document that will be seen as an authoritative source on how to construct machine learning algorithms. Again, I felt compelled to write a GitHub issue.\n\n\n\nFigure 7: Screenshot of yet another GitHub issue.\n\n\n\nThey, too, have since updated their docs and they have also acknowledged they should’ve picked another demo. They also wrote a manifesto.\nHere and Now\nI fear we’re going to be in for a wave for fairness tools that are incentivised by marketing goals. They may be made with the best intentions but, as the examples show, are just tools that are at risk of distracting us from more meaningful tasks.\nThe fairlearn project does a much better job in setting expectations. On their landing page it’s pointed out that “fairness is sociotechnical”. Here’s an excerpt from their section:\n\nFairness of AI systems is about more than simply running lines of code. In each use case, both societal and technical aspects shape who might be harmed by AI systems and how. There are many complex sources of unfairness and a variety of societal and technical processes for mitigation, not just the mitigation algorithms in our library. Throughout this website, you can find resources on how to think about fairness as sociotechnical, and how to use Fairlearn’s metrics and algorithms while considering the AI system’s broader societal context.\n\nCompare this to: Understand Advanced Machine Learning Models So They Can Make Better and More Ethical Decisions\nWhere to go from here?\nIt’s been a few months now and during a morning walk I was contemplating how these documentation mishaps could have occurred. After mulling on it, the thing that strikes me with all these examples is that all the errors on the documentation pages are quite “human”.\nLet’s consider what it might be like for the (perhaps junior) developer who is writing the tutorial. In each example there’s a need for a demo, so somebody took a widely used dataset to explain how their tools work. The goal for documentation pages is to focus on how the tool needs to be used, so I totally see how the dataset won’t get the attention it needs. Both the load_boston and the adult dataset are very common examples on blogs and books too. So why would anybody not use them?\nThat’s why I don’t think it’s productive, or even fair, to start pointing fingers and passing a ton of blame to any particular individual. There is an unwritten norm in the community to use popular datasets in a demo because that way you don’t need to explain the dataset. Can you really blame anybody to not second-guess the norm? Keeping that in mind, the issue here is more like ignorance than malice. Very much like a “don’t hate the player, hate the game”-kind of a situation.\nThe world does need to move forward though, so how might we change the game?\nI guess the main thing to do is to just keep opening GitHub issues. So if you ever see a variant of load_boston that isn’t acknowledging the issues with the dataset, maybe just open a ticket. If you’re strict, but not hostile, odds are folks will listen. So far all maintainers responded professionally and changed their docs when they were made aware of the issue.\nI’m hoping folks would join me in this so that hopefully we can change the norm for machine learning demos. If we’re going to be demonstrating the merits of machine learning, we really need to stop using gender or skin color to predict a house price or a salary.\nKeeping Track\nI’ll be keeping track of requested changes below here.\nscikit-learn\ncausalnex\nfacet\nscailable docs\nPyTorchLightning\n\n\n\n",
    "preview": "posts/just-another-dangerous-situation/naive-bias.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1922,
    "preview_height": 558
  },
  {
    "path": "posts/for-loop-memo/",
    "title": "A Loop to Stop Writing.",
    "description": "Let's make life a whole log simpler.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2021-03-03",
    "categories": [],
    "contents": "\n\nWhen you’re working in a notebook you’ve probably written a for-loop like the one below.\nimport numpy as np \n\ndata = []\n\ndef birthday_experiment(class_size, n_sim):\n    \"\"\"Simulates the birthday paradox. Vectorized = Fast!\"\"\"\n    sims = np.random.randint(1, 365 + 1, (n_sim, class_size))\n    sort_sims = np.sort(sims, axis=1)\n    n_uniq = (sort_sims[:, 1:] != sort_sims[:, :-1]).sum(axis = 1) + 1\n    proba = np.mean(n_uniq != class_size)\n    return proba\n\n# This is the for-loop everybody keeps writing! \nfor size in [10, 15, 20, 25, 30]:\n    # At every turn in the loop we add a number to the list.\n    data.append(birthday_experiment(class_size=size, n_sim=10_000))\nWe’re doing a simulation here, but it might as well be a grid-search. We’re looping over settings in order to collect data in our list.\nPandas\nWe can expand this loop to get our data into a pandas dataframe.\nimport numpy as np \n\ndata = []\n\ndef birthday_experiment(class_size, n_sim):\n    \"\"\"Simulates the birthday paradox. Vectorized = Fast!\"\"\"\n    sims = np.random.randint(1, 365 + 1, (n_sim, class_size))\n    sort_sims = np.sort(sims, axis=1)\n    n_uniq = (sort_sims[:, 1:] != sort_sims[:, :-1]).sum(axis = 1) + 1\n    proba = np.mean(n_uniq != class_size)\n    return proba\n\nsizes = [10, 15, 20, 25, 30]\nfor size in sizes:\n    data.append(birthday_experiment(class_size=size, n_sim=10_000))\n\n# At the end we put everything in a DataFrame. Neeto! \npd.DataFrame({\"size\": sizes, \"result\": data})\nSo far, so good. But will this pattern work for larger grids?\nIt gets bad.\nLet’s see what happens when we add more elements we’d like to loop over.\nimport numpy as np \n\ndata = []\n\ndef birthday_experiment(class_size, n_sim):\n    \"\"\"Simulates the birthday paradox. Vectorized = Fast!\"\"\"\n    sims = np.random.randint(1, 365 + 1, (n_sim, class_size))\n    sort_sims = np.sort(sims, axis=1)\n    n_uniq = (sort_sims[:, 1:] != sort_sims[:, :-1]).sum(axis = 1) + 1\n    proba = np.mean(n_uniq != class_size)\n    return proba\n\n# We're now looping over a larger grid!\nfor size in [10, 15, 20, 25, 30]:\n    for n_sim in [1_000, 10_000, 100_000]:\n        data.append(birthday_experiment(class_size=size, n_sim=n_sim))\nWe now need to write two loops but this has a consequence. How can we possibly link up the size parameter with the n_sim parameter when we cast this into a dataframe? You could do something like this;\nimport numpy as np \n\ndata = []\n\ndef birthday_experiment(class_size, n_sim):\n    \"\"\"Simulates the birthday paradox. Vectorized = Fast!\"\"\"\n    sims = np.random.randint(1, 365 + 1, (n_sim, class_size))\n    sort_sims = np.sort(sims, axis=1)\n    n_uniq = (sort_sims[:, 1:] != sort_sims[:, :-1]).sum(axis = 1) + 1\n    proba = np.mean(n_uniq != class_size)\n    return proba\n\n# We're now looping over a larger grid!\nfor size in [10, 15, 20, 25, 30]:\n    for n_sim in [1_000, 10_000, 100_000]:\n        result = birthday_experiment(class_size=size, n_sim=n_sim)\n        row = [size, n_sim, result]\n        data.append(row)\n\n# More manual labor. Kind of error prone.\ndf = pd.DataFrame(data, columns=[\"size\", \"n_sim\", \"result\"])\nBut suddenly we’re spending a lot of effort in maintaining a for-loop.\nBeen here before?\nI’ve noticed that this for-loop keeps getting re-written in a lot of notebooks. You find it in simulations, but also in lots of grid-searches. It’s a source of complexity, especially when our nested loops increase in size. So I figured I’d write a small package that can make all this easier.\nDecorators\nLet’s make a three minor changes to the code.\nimport numpy as np \nfrom memo import memlist\n\ndata = []\n\n@memlist(data=data)\ndef birthday_experiment(class_size, n_sim):\n    \"\"\"Simulates the birthday paradox. Vectorized = Fast!\"\"\"\n    sims = np.random.randint(1, 365 + 1, (n_sim, class_size))\n    sort_sims = np.sort(sims, axis=1)\n    n_uniq = (sort_sims[:, 1:] != sort_sims[:, :-1]).sum(axis = 1) + 1\n    proba = np.mean(n_uniq != class_size)\n    return {\"est_proba\": proba}\n\nfor size in [5, 10, 20, 30]:\n    for n_sim in [1000, 1_000_000]:\n        birthday_experiment(class_size=size, n_sim=n_sim)\nWe’ve changed three things.\nWe’ve added a memlist decorator to our original function from the memo package. This will allow us to configure a place to relay out stats into. Note that the decorator receives an empty list as input. It’s this data list that will receive new data.\nWe’ve changed our function to output a dictionary. This way we can attach names to our output and we’re able to support functions that output more than one number.\nThe for-loops now only run the function and don’t handle any state any more.\nIf you were to run this code, the data variable would now contain the following information:\n[\n    {\"class_size\": 5, \"n_sim\": 1000, \"est_proba\": 0.024},\n    {\"class_size\": 5, \"n_sim\": 1000000, \"est_proba\": 0.027178},\n    {\"class_size\": 10, \"n_sim\": 1000, \"est_proba\": 0.104},\n    {\"class_size\": 10, \"n_sim\": 1000000, \"est_proba\": 0.117062},\n    {\"class_size\": 20, \"n_sim\": 1000, \"est_proba\": 0.415},\n    {\"class_size\": 20, \"n_sim\": 1000000, \"est_proba\": 0.411571},\n    {\"class_size\": 30, \"n_sim\": 1000, \"est_proba\": 0.703},\n    {\"class_size\": 30, \"n_sim\": 1000000, \"est_proba\": 0.706033},\n]\nWhat’s nice about a list of dictionaries is that this is pandas can parse this directly without the need for you to worry about column names.\npd.DataFrame(data)\nLet’s do more.\nThis pattern is nice, but we’re still dealing with for-loops. So let’s fix that and add some extra features.\nimport numpy as np \nfrom memo import memlist, memfile, grid, time_taken\n\ndata = []\n\n@memfile(filepath=\"results.jsonl\")\n@memlist(data=data)\n@time_taken()\ndef birthday_experiment(class_size, n_sim):\n    \"\"\"Simulates the birthday paradox. Vectorized = Fast!\"\"\"\n    sims = np.random.randint(1, 365 + 1, (n_sim, class_size))\n    sort_sims = np.sort(sims, axis=1)\n    n_uniq = (sort_sims[:, 1:] != sort_sims[:, :-1]).sum(axis = 1) + 1\n    proba = np.mean(n_uniq != class_size)\n    return {\"est_proba\": proba}\n\nsetting_grid = grid(class_size=[5, 10, 20, 30], n_sim=[1000, 1_000_000])\nfor settings in setting_grid:\n    birthday_experiment(**settings)\nPay attention to the following changes.\nWe’ve got two mem-decorators now. One decorator is passing the stats to a list while the other one appends the results to a file (\"results.json\" to be exact).\nWe’ve also added a decorator called time_taken which will make sure that we also log how long the function took to complete.\nWe’ve used a grid method to generate a grid of settings on our behalf. It represents a generate of settings that can directly be passed to our function. This way, we need one (and only one) for loop. Even if we are working on large grids. You can even configure it to show a neat little progress bar!\nIf you were to inspect the \"results.json\" file it would look like this:\n{\"class_size\": 5, \"n_sim\": 1000, \"est_proba\": 0.024, \"time_taken\": 0.0004899501800537109}\n{\"class_size\": 5, \"n_sim\": 1000000, \"est_proba\": 0.027178, \"time_taken\": 0.19407916069030762}\n{\"class_size\": 10, \"n_sim\": 1000, \"est_proba\": 0.104, \"time_taken\": 0.000598907470703125}\n{\"class_size\": 10, \"n_sim\": 1000000, \"est_proba\": 0.117062, \"time_taken\": 0.3751380443572998}\n{\"class_size\": 20, \"n_sim\": 1000, \"est_proba\": 0.415, \"time_taken\": 0.0009679794311523438}\n{\"class_size\": 20, \"n_sim\": 1000000, \"est_proba\": 0.411571, \"time_taken\": 0.7928380966186523}\n{\"class_size\": 30, \"n_sim\": 1000, \"est_proba\": 0.703, \"time_taken\": 0.0018239021301269531}\n{\"class_size\": 30, \"n_sim\": 1000000, \"est_proba\": 0.706033, \"time_taken\": 1.1375510692596436}\nWhen is this useful?\nThe goal for memo is to make it easier to stop worrying about that one for-loop that we all write. We should just collect stats instead. Note that you can use the decorators from this package to send information to files and lists, but also to callback functions or as a post-request payload to a central logging service.\nI’ve found it useful in many projects. The main example for me is running benchmarks with scikit-learn. I do a lot of NLP and a lot of my components are not serializable with a python pickle which means that I cannot use the standard GridSearch from scikit-learn. You can even combine it with ray to gather statistics from compute happening in parallel. It also plays very nicely with hiplot if you’re interested in visualising your statistics.\nIf this tools sounds useful, feel free to install it via:\npip install memo\nIf you’d like to understand more about the details, check out the github repo and the documentation page. There’s also a full tutorial on calmcode.io in case you’re interested.\n\n\n\n",
    "preview": "posts/for-loop-memo/logo.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1239,
    "preview_height": 393
  },
  {
    "path": "posts/maths-as-a-compiler/",
    "title": "Maths as a Compiler",
    "description": "Once again, rephrasing is a friend.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-12-26",
    "categories": [],
    "contents": "\n\nYou might have heard of a mathematician named Gauss. He’s known for a lot of contributions in mathematics but my favorite contribution has a story. As story goes there was a maths teacher who wanted to read the newspaper while he was supposed to be teaching. So instead of teaching the teacher gave his pupils the assignment to add the numbers 1 through 100.\n\\[ \ns = 1 + 2 + 3 + \\ldots + 98 + 99 + 100 = ?\n\\] Instead of blindly calculating away, which was what most of the other students did, the young Gauss looked at the problem and started wondering if there perhaps was a more clever way of looking at the problem. After skribbling down some ideas he noticed a pattern.\n\\[ \n\\begin{array}{c *{6}{c@{\\hspace{6pt} + \\hspace{6pt}}} c}\n  s & =  & 1   & 2   & 3   & \\ldots & 98 & 99 & 100   \\\\\n  s & =   & 100   & 99 & 98 & \\ldots & 3   & 2   & 1  \n\\end{array}\n\\]\nMaybe we can calculate \\(s\\) in another way.\n\\[ \n\\begin{array}{c *{6}{c@{\\hspace{6pt} + \\hspace{6pt}}} c}\n    & 1   & 2   & 3   & \\ldots & 98 & 99 & 100   \\\\\n  +  & 100   & 99 & 98 & \\ldots & 3   & 2   & 1 \\\\  \n    \\hline \n  = & 101 & 101 & 101 & \\ldots & 101 & 101 & 101\n\\end{array}\n\\]\nIt seems that if we sort cleverly the sum of these two identical series can also be expressed as a multiplication\n\\[ \n\\underbrace{\n\\begin{array}{c *{6}{c@{\\hspace{6pt} + \\hspace{6pt}}} c}\n   & 101 & 101 & 101 & \\ldots & 101 & 101 & 101 & \n\\end{array}\n}_{100 \\times 101 = 10100} \n\\]\nIn other words;\n\\[ \ns = \\frac{10100}{2} = 5050\n\\]\n\\[ \n\\begin{array}{c *{6}{c@{\\hspace{6pt} + \\hspace{6pt}}} c}\n    &  s \\\\\n  +  &  s \\\\\n    \\hline \n  = & 10100 \n\\end{array}\n\\]\nWhat’s very neat about this “rethinking” is that it can also be generalized. If we’re interested in taking the sum from \\(1 \\ldots n\\) then we can repeat the same trick.\n\\[\n\\begin{array}{c *{6}{c@{\\hspace{6pt} + \\hspace{6pt}}} c}\n    & 1   & 2   & 3   & \\ldots & n-2 & n-1 & n   \\\\\n  + & n   & n-1 & n-2 & \\ldots & 3   & 2   & 1   \\\\\n  \\hline \n  = & n+1 & n+1 & n+1 & \\ldots & n+1 & n+1 & n+1\n\\end{array}\n\\]\nThis will give us the general formula.\n\\[\n1 + \\ldots + n = \\sum_{x=1}^n x= \\frac{n\\times(n+1)}{2}\n\\]\nThe brilliance of this is in recognizing that there are multiple ways of calculating the thing we’re interested in. Instead of doing a whole lof of addition, we can apply a mere multiplication and still get the same result.\nThis is a beautiful point, because it gives another angle to mathematics. There’s a “computer-science-voice” inside of me that’s looking at this and wondering if maybe … maths can be applied much more like a compiler than a calculator?\nThink about it! This example shows that you might be able to rephrase an original problem into one that is much easier to calculate. We’re literally generating an alternative set of instructions. Just like the kid in the classroom, our program might also spend less time performing a multiplication compared to a long sequence of additions.\nIt’s a compiler that is potentially “better” too.\nBenchmarks\nYou can actually run this as an experiment.\nimport numba as nb\n\ndef calc_sum(n):\n    return sum(i for i in range(1, n + 1))\n\ndef calc_smart(n):\n    return n * (n+1) / 2\n\n@nb.njit\ndef calc_sum_compiled(n):\n    res = 0\n    for i in range(1, n + 1):\n        res += i\n    return res\n\n@nb.njit\ndef calc_smart_compiled(n):\n    return n * (n+1) / 2\nThe results of various %timeit calls are listed below.\n\ncalc_sum\ncalc_smart\ncalc_sum_comp\ncalc_smart_comp\nn=1_000\n46.5 µs ± 1.16 µs\n143 ns ± 5.18 ns\n149 ns ± 10.7 ns\n127 ns ± 6.25 ns\nn=10_000\n435 µs ± 3.86 µs\n135 ns ± 0.461 ns\n140 ns ± 6.97 ns\n116 ns ± 0.257 ns\nn=100_000\n4.46 ms ± 142 µs\n147 ns ± 6.61 ns\n132 ns ± 3.9 ns\n117 ns ± 1.17 ns\nEven if we use the JIT compiler from numba then we still see that we can get a huge speedup by using maths as a compiler. The fastest results are achieved by using both the compiler and the maths but the example is sufficient to show that you’re able to improve a compiler by using maths.\n\n\n\n",
    "preview": "posts/maths-as-a-compiler/intro.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1239,
    "preview_height": 393
  },
  {
    "path": "posts/oops-and-optimality/",
    "title": "Oops and Optimality",
    "description": "GridSearch is Not Enough: Part Five",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-09-11",
    "categories": [],
    "contents": "\nI write a lot of blogposts on why you need more than grid-search to properly judge a machine learning model. In this blogpost I want to demonstrate yet another reason; the optimisers curse.\nExibit A Exibit B\nExibit C Exibit D\nDice Analogy\nLet’s first discuss a game with dice. Let’s say that I’ve got a 100-sided die. I can assume the following;\nIf I roll once then the expected value for the number of eyes is 50.5.\nIf I roll two times and take the highest of the two then the expected value increases.\nIf I roll three times and take the highest of the three then it increases again.\nIf I roll four times and take the highest of the four then the it increases yet again.\netc\nHere’s the kicker to this situation; it keeps increasing! Every time we roll again we have another opportunity to beat the previous score.\nPretty Distribution\nThis effect doesn’t just hold for dice; it holds for probability distributions in general!\nThe plot below demonstrates this effect for the normal distribution.\n\n\n\nFigure 1: The maximum of n random values increases as n increases.\n\n\n\n\nFor the full derivation of the maths used here check out here and here.\nCode.\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pylab as plt\n\nplt.figure(figsize=(12, 4))\nx = np.linspace(-3, 5, 100)\nfor n in [1, 2, 4, 8, 16, 32]:\n    s = n * norm.pdf(x) * norm.cdf(x)**n\n    plt.plot(x, s/s.sum(), label=f\"n={n}\")\n\nplt.legend()\nplt.title(\"Try more often, the score goes up!\");\n\nIf we really go crazy we can see that the distribution really changes. It’s no longer normally distributed because it is so far skewed to the right.\n\n\n\nFigure 2: If n increases by a lot, it no longer represents the original distribution.\n\n\n\nCode.\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pylab as plt\n\nplt.figure(figsize=(12, 4))\nx = np.linspace(-3, 5, 200)\nfor n in [1, 32, 128, 256, 516]:\n    s = n * norm.pdf(x) * norm.cdf(x)**n\n    plt.plot(x, s/s.sum(), label=f\"n={n}\")\n\nplt.legend()\nplt.title(\"It's getting less and less normal.\");\n\nProblem\nWhen we get to “re-roll” before we grab the maximum die then we should expect to see a higher number. This holds for any process with randomness in it. This includes grid-search via cross-validation. Instead of rolling more dice, we can add more parameters to the grid. As we increase the size of the grid, we might start reporting “luck” if we’re not careful. W\nCross-validation is not perfectly deterministic. There’s noise from the underlying algorithm, numeric in-accuracies as well as the cross-validation itself.\n\nAnd I’m not even talking about using the random seed as a hyperparameter.\nSo let’s see if we can quantify this effect using an actual example.\nCreditcard\nLet’s grab the creditcard dataset and try to predict some fraud. This dataset is available directly via scikit-lego.\nfrom sklego.datasets import fetch_creditcard\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import make_scorer, accuracy_score\n\nX, y = fetch_creditcard(return_X_y=True)\n\ndef run_grid(settings=10, cv=5):\n    params = {\n        'C': np.linspace(.1, 10.0, settings)\n    }\n\n    grid = GridSearchCV(\n        estimator=LogisticRegression(class_weight=\"balanced\"), \n        cv=cv,\n        n_jobs=-1,\n        param_grid=LogisticRegression, \n        scoring={\"acc\": make_scorer(accuracy_score)},\n        refit=\"acc\"\n    )\n\n    return pd.DataFrame(grid.fit(X, y).cv_results_).sort_values(\"rank_test_acc\");\nThis code-block gives us a function run_grid that runs a grid-search. We are optimising a parameter C which represents regularisation strength and we can choose to consider many settings for that parameter or few. We also have a cv argument that allows us to specify how many cross-validations to perform.\nI’ve been running this function with a whole bunch of settings and it eventually was able to give me this data.\nsettings/cv\n2\n5\n5\n0.944102\n0.973533\n10\n0.94695\n0.973705\n20\n0.945184\n0.973810\n100\n0.956050\n0.974150\nThere’s a few things to point out here.\nFirst, calculating this takes a fair amount of time, even on this relatively small dataset. If we try out 100 parameters and cross validate five times then waiting becomes part of our job.\nSecond, if you want higher grid-search results it might help to increase the number of cross-validations that you make. I might argue that methodologically this not a “super bad” observation. It means that you might need to spend a lot of compute resources but it makes sense. If you only have 2 sets for cross validation then the model only learns from 1/2 of the dataset. If you have 5 sets then the model can learn from 4/5 of the dataset and should therefore generalise better.\nThirs, if you want higher grid-search results; it seems good to increase the size of the grid. But let’s think twice about what this means. Are we seeing this because of a better hyperparameter? Or is this the “more-dice” effect?\n\n\n\nFigure 3: In other words; can we really trust these sorts of statements?\n\n\n\nChecking\nLet’s zoom in on that one grid-search that is big and has 100 settings. We’ll first look at the top grid-search results.\nrank_test_acc\nparam_C\nmean_test_acc\n1\n0.8\n0.974151\n2\n7.8\n0.973705\n3\n5.1\n0.973589\n4\n9.8\n0.973512\n5\n0.1\n0.973473\nIs it just me, or does this look fishy? The best scoring param_C values here don’t seem to be related to mean_test_acc. In fact, they’re spread all across the spectrum. You’d expect the best scoring candidates to be similar and different to the rest. This begs the question; is there a relationship between the param_C values that we’re considering and the test score? Let’s plot it.\n\n\n\nFigure 4: Noise.\n\n\n\nI don’t know about you, but this looks like random data to me. The best score that we’re selecting, might just get picked due to luck, not because it is contributing to a better model.\nWhat is Happening?\nThe example here is a bit silly. This dataset is very imbalanced which is making it hard for the LogisticRegression to properly converge. The parameter C therefore isn’t contributing much. That means that by optimising for it, you’re more likely capturing noise than measuring the contribution that it makes to predictive power.\nBut the point isn’t the dataset. It’s the fact that GridSearchCV has no way to deal with this extra noise. It will just pick the best model according to the score that we provide it. The only way to understand that we’re optimising for noise here is to understand the data and the model in our system. You shouldn’t call fit(X, y).predict(X) and call it a day. You need more.\nThink about this if you’re running a big grid-search on a daily basis. If you blindly give your EnterpriseAutoML[tm]-engine a gigantic set of parameters to go optimise for, and you find a new model reported to be 0.15% better are you sure it’s really better? You’re probably not making the model better, instead you’re optimising for the noise of your cross-validation. This is dangerous territory because you might report overly optimistic numbers which will dissapoint everyone when the model is deployed.\nNot to mention all the compute power you’ve wasted.\nConclusion\nGrid-search is not a inheritly bad idea. In fact, there’s many merits to it! The main point here is to demonstrate that it is not enough. You can game it by artificially inflating the parameter grid.\nIn this blogpost we’ve shown a phenomenon called “the optimisers curse.” The odds of getting too optimistic during optimisation increases the more cases you consider. The problem isn’t a high statistic in a table, the problem is that that number doesn’t represent what will happen in reality.\nAppendix\nIf you want to learn more about the optimisers curse you can check the original paper.\nIf you’re interested in fiddling around with the notebook, you can find the scratchpad here.\n\n\n\n",
    "preview": "posts/oops-and-optimality/assumptions.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1239,
    "preview_height": 393
  },
  {
    "path": "posts/cool-commits/",
    "title": "Uncommon Contributions",
    "description": "Making impact without touching the core of a library.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-08-24",
    "categories": [],
    "contents": "\nThere are a lot of ways that you can contribute to open source. Frequent contributions include adding features to a library, fixing bugs, or providing examples to a documentation page. All of these contributions are valid, but there are other avenues that you may want to consider too. In this blog post, I’d like to list a few non-standard examples that might inspire.\nI’ll also add some images to highlight why some of these changes matter.\nInfo\n\nMy first proper PR for Rasa has little to do with the core library. It doesn’t help in making a chatbot at all. Instead, I upgraded this command:\nrasa --version\nBefore, this command would list the current version of Rasa. In the new version, it lists:\nThe version of python.\nThe path to your virtual environment.\nThe versions of related packages.\nThis is a pretty big feature if you consider the amount of work that goes into submitting a proper bug report. By adding this feature, it is much easier to supply all the relevant information. Just copy and paste the output of this call, and you’ll be writing a much better bug report. No need to fiddle around with commands like:\npip freeze | grep package-name\nThis feature has little to do with the core package code but still makes a lot of impact. The debugging process is made easier for both the user and the maintainers of the project.\nCron on Dependencies\n\nA user for scikit-lego, a package that I maintain, discovered that the reason the code wasn’t working was because scikit-learn introduced a minor, but breaking, change. To fix this the user added a cronjob with Github actions to the project.\nBefore the PR, we had to hear from users when a dependency introduced breaking changes. By adding a cronjob that would run our unit tests daily, the user removed a blind spot from the project. Every day we now run the unit tests using the latest versions of our dependencies. If the tests break, we can quickly pinpoint what package caused it and create a fix. You can imagine how this might lead to fewer issues for our users.\nThis feature, again, had little to do with the core package code.\nSpellcheck\n\nFor the scikit-lego project, we met a user who was interested in contributing but didn’t know where to start. The user hadn’t made many contributions yet and was looking for something easy. His main goal was to get more comfortable with git and to get going with a first open-source contribution. I mentioned that addressing spelling errors is certainly valid, so the user went ahead with this.\nThe results were somewhat epic. He ran a spellchecker, not just against our docs, but also on our source code! It turns out we had some issues in our docstrings as well. While exploring this theme we’ve also discovered flake8 compatible packages that do spellchecking on variable names.\nSpell-checking was something we hadn’t considered, and it was a much-appreciated code quality update.\nError Messages\n\nOne of the harder parts of writing a package for other people is getting people to understand how they should use the package. As a maintainer, you cannot imagine what it is like not to understand your own code. New users, on the other hand, can spot this instantly.\nThis is why a popular entry point for contribution is documentation. Documentation is often read when something goes wrong after the user sees a confusing error, so it makes sense for new users to contribute there. But you can go a step further! Instead of changing the docs, why not write a more meaningful error message?\nIn whatlies, we’ve recently allowed for optional dependencies. If you try to use a part of the library that requires a dependency that is not part of the base package, then you’ll get this error message.\nIn order to use ConveRTLanguage you'll need to install via;\n\n> pip install whatlies[tfhub]\n\nSee installation guide here: https://rasahq.github.io/whatlies/#installation\nThis feature has little to do with the core functionality of the library. Yet, it will do a lot for developer experience.\nFailing Unit Tests\n\nThere’s a lovely plugin for mkdocs called mkdocs-jupyter. It allows you to easily add jupyter notebooks to your documentation pages. When I was playing with it, I noticed that it wasn’t compatible with a new version of mkdocs. Instead of just submitting a bug to Github, I went the extra mile. I created a PR that contained a failing unit-test for this issue. This was great for the maintainer because it was easier to understand the issue and to fix it.\nThis feature, again, had little to do with the core package code.\nRenaming files.\n\nLet’s compare two pieces of code from a library that I maintain.\nExibit A\nfrom whatlies.transformer import Pca\npca_plot = emb.transform(Pca(2)).plot_interactive()\nExibit B\nfrom whatlies.transformer import pca\npca_plot = emb.transform(pca(2)).plot_interactive()\nThe Difference\nDid you see the difference? In the first example we’re importing Pca and in the second example pca. The difference is an upper case/lower case letter and this small typo caused a very unintuitive bug.\nThe bug is related to the file structure of the project.\nwhatlies\n├── __init__.py\n├── embedding.py\n├── embeddingset.py\n├── language\n│   ├── __init__.py\n│   ├── ...\n└── transformers\n    ├── __init__.py\n    ├── pca.py\n    └── ...\nThe Pca class is defined in the pca.py file in the transformers folder. We intended to expose the Pca class via the __init__.py file in the same folder. Unfortunately, by importing pca instead of Pca you’re getting the submodule instead of the intended class.\nA single character could cause a really confusing bug so we had to fix it. So we fixed it by changing the filename from pca.py to _pca.py.\nwhatlies\n├── __init__.py\n├── embedding.py\n├── embeddingset.py\n├── language\n│   ├── __init__.py\n│   ├── ...\n└── transformers\n    ├── __init__.py\n    ├── _pca.py\n    └── ...\nWe didn’t just do this for _pca.py but for all files where this error might occur.\nAgain, this change doesn’t require you to understand the core code of the library.\nConclusion\nMany of the coolest contributions might have nothing to do with the core library. All of the examples that I’ve listed above have been tremendous features for some of the package that I maintain but also for some of the larger code repositories out there. Feel free to remember this when you’re considering your first contribution.\n\n\n\n",
    "preview": "posts/cool-commits/cron.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 904,
    "preview_height": 436
  },
  {
    "path": "posts/mean-squared-terror/",
    "title": "Mean Squared Terror",
    "description": "GridSearch is Not Enough: Part Four",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-06-28",
    "categories": [],
    "contents": "\n\n2020-06-29 This blogpost was edited the day after it got posted. During my morning walk I realised that my code had a glaring bug in the statistics. I was reporting training set figures, not the validation set. I re-ran the system with the appropriate numbers and while the conclusions did not change, some of the figures/numbers did.\n\nWhen I started in data, many years ago, people were actively working on heuristics. These are rules that could look at data and then make appropriate decisions. In schematics, this was the norm;\n\n\n\nFigure 1: Those were the days.\n\n\n\nYou had to understand the data to come up with these rules and this was very time consuming. But then machine learning changed everything. Instead of writing rules that fit a problem, we provide labels and have a machine figure out the rules instead.\n\n\n\nFigure 2: The latest and greatest.\n\n\n\nThe latter approach has merit to it but it requires a new way of working. It also tends to be compute intensive because we’re interested in finding the best possible settings.\n\n\n\nFigure 3: All those settings.\n\n\n\nTo find the best settings for all of these models we’ve even got gridsearch via cross validation; an industry standard to make sure that we pick the best model.\n\n\n\nFigure 4: All this expensive compute better be worth it.\n\n\n\nDon’t get me wrong, there’s merit to the methodology. We want to demonstrate consistent results in our models. But we’re representing all the possible properties of a model as a single number. That’s a massive reduction of information, and frankly, an overly simplified way to think about models.\nDangerzone\nI’ve written before that “GridSearch is not Enough[tm]” but in this document I wanted to give an example of how the technique can also be a problem. GridSearch is insufficient at best, it can also be dangerous at worst.\nExibit A Exibit B Exibit C\nThe main reason is that I’ve seen people put blind faith into the methodology causing a blind spot for other issues with the model. I’ve also spoken to data scientists who think that machine learning algorithms are unbiased tools that find the “truth” in the data and that grid search is a methodological purity. This is a dangerous attitude so I figured I’d create a counter-example.\nExperiment\nSo let’s do a small experiment. The goal is to see the effect of machine learning on a biased dataset. Given a biased dataset, will grid search amplify the bias in the dataset or reduce it?\nI’ve draw a dataset using drawdata.xyz. Let’s pretend that this usecase represents predicting salaries based on skill level. The skill level is plotted on the x-axis and the salary/reward is plotted on the y-axis. There’s two groups of datapoints and one represents a marginalized group. This group is under represented, only appears with low salaries and it also has a lower salary than it’s non-marginalised peers. The current illustration is a blunt representation of what might happen in a real life dataset (the bias can be more subtle) but the question is; what pattern will be picked up after grid search?\n\n\n\nBelow we’ll have a look at the results of three different models. We’ll also list the mean squared error that was reported back to us from the gridsearch.\n\nYou can find the code for this work here.\nLinear Model with Dummy Variable\n\n\n\nFigure 5: Mean Squared Error, Train: 1166.64 Test: 1767.36\n\n\n\nNearest Neighbor Regression with Scaling\n\n\n\nFigure 6: Mean Squared Error, Train: 909.66 Test: 2337.65\n\n\n\nGrouped Linear Model\n\n\n\nFigure 7: Mean Squared Error, Train: 993.57 Test: 1343.09\n\n\n\nNotice how the model that has the lowest mean squared error is also the model that causes the most bias between groups for higher skill levels. This increase in bias cannot be blamed merely on the data. It’s the choice of the model that increases this bias which is the responsibility of the algorithm designer.\nNow, you could argue that this is a theoretical dataset. No professional would include something like gender into a model predicting salary. Right? We can repeat the same exercise with something that correlates with a sensitive attribute instead but you can expect similar result. You can get a smaller mean squared error when you can learn the bias in the dataset. No matter how subtle.\nThis is what I find scary about gridsearch. People have been learned to trust it even though it can pick a model you should not consider. Notice how the grid search would tell us that there should be a big payment gap between the two groups even if they display high levels of skill. In algorithmic terms some might call this behavior “optimal” while in real life, you would call this sexism, ageism or racism.\n\n\n\nFigure 8: Mean Squared Terror: over 9000.\n\n\n\nThe thing to remember here is that 100% unbiased datasets don’t really exist. This phenomenon of overfitting on a metric is something that gridsearch contributes to. Combine that with algorithm designers who don’t have domain knowledge in the domain that they’re applying your algorithm to and you might just get this.\n\n\n\nFigure 9: Mean Squared Terror: np.inf\n\n\n\nAppendix\nThis post was prompted by a discussion on twitter as well as a discussion on linkedin. It’s dissapointing to see data professionals be dismissive of how bias is amplified by algorithms. It’s not something the profession should be casual about.\n\n\n\n",
    "preview": "posts/mean-squared-terror/ivory.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1392,
    "preview_height": 847
  },
  {
    "path": "posts/caring-means-sharing/",
    "title": "Sharing is Caring",
    "description": "Put Cookie-Monster on a Diet.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-04-25",
    "categories": [],
    "contents": "\nThere’s an imbalance on the internet.\n\n\nimgs <- c(\"img1.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nFigure 1: What it should feel like to see a website.\n\n\n\nEver since GDPR there’s been an increase in websites asking me permission to use my behavioral data. I’m open to that idea and I think websites can become better by using data appropriately. But I often do not know how my data is being used. I’m usually supplied with a notification stating that my privacy is super important after which I am told that my data will be shared with 3rd parties.\n\n\nimgs <- c(\"img2.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nFigure 2: What it starts to feel like to see a website.\n\n\n\nSure, they’re being transparant in the sense that they are telling me about their plans. But let’s be honest here. I can’t pretend to know what is going to happen with that data. How is anyone? There’s a suggestion of control by asking me for permission but I’m still struggeling to understand what is happening when I click accept. Does the third party collect data independantly per website or is it merging everything from other platforms? What am I approving when I click “accept”?\n\n\nimgs <- c(\"img4.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nFigure 3: What it also starts to feel like to see a website.\n\n\n\nAnd that bothers me. The meaning of the word “approval” no longer implies trust on the internet. It implies “we’ve covered our legal asses”.\nThis is tricky territory.\nI’d really like to trust the website but it’s hard for me to trust some third party unless I can varify what they do with it. Then again, I also get that you’re doing yourself short if you’re not running analytics on your website. It can reduce churn if you track what clients are doing on your platform and installing google analytics is way easier than inventing in your own stack.\nI’m a techie and even I wouldn’t want to run analytics myself for any of my own projects. It’s just too much gruntwork to maintain my own analytics pipeline even if it is a simple one. Then again, I really like to know how many people read this blog. I’d also like to know how my side projects are doing.\nThis is where the slippery slope starts. A few years back I used google analytics for this blog just for the view counts. But google gave me way more. It’s an overload of information. There’s no way to tell google to track less. I enjoy seeing where the traffic to my site comes from but I really just don’t care about demographics. It’s not just the overload of “insights” that makes google’s analytics product a pain to use. It’s also that I’m using software that tries to estimate the age of the people who visit my website.\nI can’t imagine anybody giving permission for that.\nConstrained Solution\nSo here’s my take on how to fix this mess.\nI’m not going to ask for permission on behalf of third parties that you’re going to have to magically trust. Instead, I’m going to just open up my analytics dashboard. The analytics from this blog and all of my other projects are now viewable by anyone who is interested. This way you get to see what I track. The idea is that I care, and therefore I should share.\n\n\nimgs <- c(\"img3.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nFigure 4: That’s better.\n\n\n\n\nIt’s good to see cookiemonster on a diet.\nThere’s a few nice effects when you open up this way. Not only can you demonstrate that you genuinely care about privacy but by keeping the dashboard open you’re incentivised to only track data that you’re comfortable sharing to the open world. That’s a sane constraint to build into your analytics.\nPlausible\nIt turns out that there’s actually a service for this: plausible.io. They’re a privacy aware analytics company and you can view their data policy here. All the data that is collected is aggregated and anonymised. This service is not free and I like it that way. By getting paid by people like me they have the incentive to stick to their promises. The source code of the service is even available on github so people can inspect it.\nOne refreshing selling point of their service is that they really only give you a few simple metrics. It actually does less than their competition. This is nice because there’s just a basic summary. I read it and after two minutes I am up to date. I am not distracted or overloaded. It is a calm experience so that I may carry on with my life.\nConcluding\nI’m running a couple of blogs and websites. For each of them you can now view the open dashboards;\nthe koaning.io dashboard is here\nthe calmcode.io dashboard is here\nthe drawdata.xyz dashboard is here\nthe thismonthrocks dashboard is here\nthe dearme.email dashboard is here\nI’m looking forward to a new norm; sharing is caring.\nAppendix\nAnd yes, plausible has an open dashboard too.\n\n\n\n",
    "preview": "posts/caring-means-sharing/preview.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 583,
    "preview_height": 224
  },
  {
    "path": "posts/more-descent-less-gradient/",
    "title": "More Descent, Less Gradient",
    "description": "Your regression may only need one gradient step. Really.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-04-10",
    "categories": [],
    "contents": "\n\nsummary{\n  background-color: #fbfbfb;\n}\ndetails{\n  background-color: #fbfbfb;\n  padding: 25px;\n  padding-bottom: 10px;\n  padding-top: 10px;\n}\nI’ve been rethinking gradient descent over the weekend. It struck me that calculating the gradient is typically way more expensive than taking the step that follows it. I ran the numbers and found that about 80% of the training loop is spent calculating a gradient.\nThis led me to some fun hacking and I want to demonstrate the findings in this document. In particular I would like to highlight some ideas that had insightful results;\nYou can apply calculus to estimate the stepsize you need for the current iteration. If you do this in a principled way then my results suggests that for linear regression you may only need one gradient calculation.\nYou can also consider just stepping more before you re-calculate a gradient. Especially early on in SGD the direction won’t change too much so how about we just keep on stepping in a direction until the performance becomes worse? The idea is pretty blunt, but seems to beat SGD and it even gets close to Adam at times.\nThese results are done on artifical data, so the results deserve to be taken with a grain of salt, but the ideas are intertraining nonetheless.\nCalculus\nSuppose that I want to optimise a function, say \\(f(x)\\). You could calculate the gradient and take a step but after taking this step you’d need to calculate the gradient again.\n\n\n\nFigure 1: Gradient Descent 101.\n\n\n\nIt would work, but it may take a while. Especially if calculating the gradient is expensive (it usually is). So how about we do some extra work to do a calculated step instead.\n\n\n\nFigure 2: Can’t we do this?\n\n\n\nA calculated step might be more expensive to calculate, but this is offset by all the small steps we would otherwise do.\n\n\n\nFigure 3: We may be on to something here.\n\n\n\nThis is where calculus can help us. In particular, taylor series! Suppose that we have some value \\(x\\) and we’d like to estimate what \\(f(x + t)\\) is then we can approximate this by;\n\\[f\\left(x+t\\right) \\approx f\\left(x\\right)+f^{\\prime}\\left(x\\right) t+\\frac{1}{2} f^{\\prime \\prime}\\left(x\\right) t^{2}\\]\nIf I know the derivatives of the function \\(f\\) then I can approximate what \\(f(x+t)\\) might be. But we can go a step further. Suppose now that I am interested in finding a minimum for this function. Then we can rewrite the expression to represent iteration.\n\\[f\\left(x_{k}+t_k\\right) \\approx f\\left(x_{k}\\right)+f^{\\prime}\\left(x_{k}\\right) t_k+\\frac{1}{2} f^{\\prime \\prime}\\left(x_{k}\\right) t_k^{2}\\]\nHere \\(x_k\\) represents \\(x\\) at iteration time \\(k\\) and the next value \\(x_{k+1}\\) will be \\(x_{k} + t_k\\). The question now becomes; how can I choose \\(t_k\\) such that travel to the minimum as fast as possible? It turns out to be;\n\\[\nx_{k+1}=x_{k}+t_k=x_{k}-\\frac{f^{\\prime}\\left(x_{k}\\right)}{f^{\\prime \\prime}\\left(x_{k}\\right)}\n\\]\nThis formula can be used for functions with a single parameter, but with some linear algebra tricks we can also extend it to functions with many inputs using a hessian matrix.\nDerivation.\nIf the second derivative is positive, the quadratic approximation is a convex function of \\(t\\), and its minimum can be found by setting the derivative to zero. Note that; \\[ 0=\\frac{\\mathrm{d}}{\\mathrm{d} t_k}\\left(f\\left(x_{k}\\right)+f^{\\prime}\\left(x_{k}\\right) t_k+\\frac{1}{2} f^{\\prime \\prime}\\left(x_{k}\\right) t_k^{2}\\right)=f^{\\prime}\\left(x_{k}\\right)+f^{\\prime \\prime}\\left(x_{k}\\right)t_k\\]\nThus the minimum is achieved for \\(t_k=-\\frac{f^{\\prime}\\left(x_{k}\\right)}{f^{\\prime \\prime}\\left(x_{k}\\right)}\\)\nPutting everything together, Newton’s method performs the iteration;\n\\[\n  x_{k+1}=x_{k}+t=x_{k}-\\frac{f^{\\prime}\\left(x_{k}\\right)}{f^{\\prime \\prime}\\left(x_{k}\\right)}\n  \\] Now, this formula works for single parameter functions, but we can also express this result in linear algebra terms.\n\\[\n  x_{k+1}=x_{k}+t=x_{k}- [f^{\\prime \\prime}\\left(x_{k}\\right)]^{-1} f^{\\prime}\\left(x_{k}\\right)\n  \\]\nHere \\(x_k\\) is a vector, \\([f^{\\prime \\prime}\\left(x_{k}\\right)]^{-1}\\) is the Hessian matrix and \\(f^{\\prime}\\left(x_{k}\\right)\\) is the gradient vector.\nEnter Jax\nThis, to me, was a great excuse to play with jax. It has a couple of like-able features but a main one is that it is an autograd library that also features a hessian. You can just-in-time compile derivate functions and it will also run on GPU’s and TPU’s.\nThis is how you might implement a linear regression;\nimport jax.numpy as np\nfrom jax import grad, jit\n\ndef predict(params, inputs):\n    return inputs @ params\n\ndef mse(params, inputs, targets):\n    preds = predict(params, inputs)\n    return np.mean((preds - targets)**2)\n\ngrad_fun = jit(grad(mse))  # compiled gradient evaluation function\nThe grad_fun is now a compiled gradient function that has two input parameters left; inputs and targets and it will return the gradiet of the mse function. That means that I can use it in a learning loop. So here’s an implementation of linear regression;\nimport tqdm\nimport numpy as np\nimport matplotlib.pylab as plt\n\n# generate random regression data\nn, k = 1_000_000, 10\nboth = [np.ones((n, 1)), np.random.normal(0, 1, (n, k))]\nX = np.concatenate(both, axis=1)\ntrue_w = np.random.normal(0, 5, (k + 1,))\ny = X @ true_w\nnp.random.seed(42)\nW = np.random.normal(0, 1, (k + 1,))\n\nstepsize = 0.02\nn_step = 100\nhist_gd = np.zeros((n_step,))\nfor i in tqdm.tqdm(range(n_step)):\n    hist_gd[i] = mse(W, inputs=X, targets=y)\n    dW = grad_fun(W, inputs=X, targets=y)\n    W -= dW*stepsize\nThis is what the mean squared error looks like over the epochs.\n\n\n\nFigure 4: Looks converging.\n\n\n\nLet’s now do the same thing, but use the calculus trick.\nfrom jax import hessian\n\n# use same data, but reset the found weights \nnp.random.seed(42)\nW = np.random.normal(0, 1, (k + 1,))\n\nn_step = 100\nhist_hess = np.zeros((n_step,))\nfor i in tqdm.tqdm(range(n_step)):\n    hist_hess[i] = mse(W, inputs=X, targets=y)\n    inv_hessian = np.linalg.inv(hessian(mse)(W, X, y))\n    dW = inv_hessian @ grad_fun(W, inputs=X, targets=y)\n    W -= dW\nWant to see something cool? This is the new result.\nTry the notebook.\n\n\n\nFigure 5: You barely need the second epoch.\n\n\n\nThe shocking thing is that this graph always has the same shape, no matter the rows or columns. When I first ran this I could barely believe it. By using the hessian trick we predict how big of a step we need to make and it hits bullseye.\nThere’s reason for this bullseye but it is a bit mathematical.\nDerivation of Mathematical Bullseye.\nLet’s rewrite the loss for linear regression in matrix terms.\n\\[ L(\\beta) = (y - X \\beta)^T (y - X \\beta) \\] If we simply differentiate then the gradient vector is;\n\\[ \\nabla L (\\beta) = - X^T (y - X \\beta) \\] And the Hessian matrix is;\n\\[ \\nabla^2 L (\\beta) = X^T X \\] Let’s remind ourselves of newtons method.\n\\[\nx_{k+1}=x_{k}+t=x_{k}-\\frac{f^{\\prime}\\left(x_{k}\\right)}{f^{\\prime \\prime}\\left(x_{k}\\right)}\n\\]\nThat means that our stepsize (see earlier derivation) needs to be;\n\\[\n\\begin{equation}\n\\begin{split}\nt & = -[\\nabla^2 L (\\beta)]^{-1} \\nabla L (\\beta) \\\\\n  & = -(X^TX)^{-1}X^T(y - X \\beta) \\\\\n  & = -(X^TX)^{-1}X^Ty + (X^TX)^{-1}X^TX \\beta \\\\\n  & = \\beta -(X^TX)^{-1}X^Ty\n\\end{split}\n\\end{equation}\n\\] When we start our regression we start with \\(\\beta_k\\) and then the update rule becomes;\n\\[\n\\begin{equation}\n\\begin{split}\n\\beta_{k+1} & = \\beta_k - t\\\\\n  & = \\beta_k - \\beta_k + (X^TX)^{-1}X^Ty \\\\\n  & = (X^TX)^{-1}X^Ty\n\\end{split}\n\\end{equation}\n\\] And this is a bit of a coincidence, but \\((X^TX)^{-1}X^Ty\\) is the closed form solution for linear regression. This means that using newtons method for a single iteration on standard linear regression is equivalent to using the close form method.\nThis does not mean that this is the fastest way to perform linear regression. You can benchmark it yourself, scikit-learn is faster.\nWe should not expect something similar to happen with neural networks though.\nA linear regression is nice because it does not have a lot of parameters. In this example we have 10 parameters so we’d have a Hessian matrix of 100 elements. If we had a neural network with 2K numbers then we’d suddenly have a matrix with 4 million parameters.\nWe’re typically not interested in the best performance on a training set, we’re more interested in something that generalizes well. Even if this approach would work perfectly in a neural setting, we may not be that interested in it because of the overfitting risk.\nTowards Networks\nThis made me wonder, can we do something similar in spirit for neural networks? Well, maybe we should go for the other extreme. Instead of doing few steps, let’s do many!\nConsider what we usually do.\n\n\n\nFigure 6: This is base SGD.\n\n\n\nIf we briefly ignore the details of adam/momentum then the gradient descent idea does the two things calculating a gradient (thick dot) and moving a step (line). But what if we don’t stop stepping?\n\n\n\nFigure 7: First determine the direction. Then keep walking until results get worse. Only then do another gradient. Repeat.\n\n\n\nA dash here represents moving forward without re-evaluating the gradient. Once we notice that a step makes the score worse, we stop and check the gradient again.\nIt is well possible that the general direction that you’re moving in is a good one, do you really need to stop moving? Do we really need to calculate a gradient? Or can we just keep on stepping? Checking if the next step is making it worse is a forward pass, not a backward one. If a gradient calculation is about 80% of the compute power for training then this might be a neat idea.\nThere are two hyperparameters to this idea;\nmaybe we want to make sure that every n steps we check for another gradient\nmaybe we get impatient and we’re ok with increasing the stepsize after a few steps\nTo check the merits of this idea I figured it’d be fun to write my own optimiser for pytorch.\nImplementation of KeepStepping Optimizer\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\nclass KeepStepping(Optimizer):\n  \"\"\"\n  KeepStepping -  PyTorch Optimizer\n  \n  Inputs:\n      lr = learning rate, ie. the minimum stepsize \n      max_steps = the maximum number of steps that will be performed \n                  before calculating another gradient\n      scale_i = to what degree do we scale our impatience\n  \"\"\"\n  def __init__(self, params, lr=required, max_steps=20, scale_i=0):\n      if lr is not required and lr < 0.0:\n          raise ValueError(\"Invalid learning rate: {}\".format(lr))\n      defaults = dict(lr=lr)\n      super().__init__(params, defaults)\n      self.max_steps = max_steps\n      self.lr_orig = lr\n      self.scale_i = scale_i\n\n  def mini_step(self, i):\n      for group in self.param_groups:\n          for p in group['params']:\n              if p.grad is None:\n                  continue\n              d_p = p.grad.data\n              scale = - group['lr'] * self.scale_i * np.sqrt(i)\n              p.data.add_(-group['lr'] - scale, d_p)\n  \n  def step(self, closure):\n      \"\"\"Performs a single optimization step.\"\"\"\n      old_loss = closure()\n      i = 0\n      self.mini_step(i)\n      new_loss = closure()\n      while (new_loss < old_loss) & (i < self.max_steps):\n          self.mini_step(i)\n          old_loss = new_loss\n          new_loss = closure()\n          i += 1\n      return new_loss\nOne Step Further\nBefore testing this, I consdering taking the previous idea and combining it with the idea before. I am doing less gradients, sure, but I am still taking lots of steps. Can I instead perhaps calculate how big the stepsize should be? There might be something adaptive that we can do here?\nGiven a direction that we’re supposed to move in, you could consider that we’re back in a one dimensional domain again and that we merely need to find the right stepsize.\n\n\n\nFigure 8: Note that as far as the stepsize is concerned, we merely need to move in one direction. So it’s back to one dimensional-land.\n\n\n\nSo I made an implementation that takes this direction, numerical estimates of \\(f'(x_{\\text{direction}})\\) and \\(f''(x_{\\text{direction}})\\) and tries to adaptively estimate an appropriate stepsize.\nImplementation of KeepVaulting Optimizer\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\nclass KeepVaulting(Optimizer):\n    \"\"\"\n    KeepVaulting -  PyTorch Optimizer\n    \n    Inputs:\n        lr = learning rate, ie. the minimum stepsize \n        max_steps = the maximum number of steps that will be \n                    performed before calculating another gradient\n    \"\"\"\n    def __init__(self, params, lr=required, max_steps=20):\n        if lr is not required and lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        defaults = dict(lr=lr)\n        super().__init__(params, defaults)\n        self.max_steps = max_steps\n        self.lr_orig = lr\n\n    def mini_step(self, jumpsize=1):\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                p.data.add_(-(group['lr'] * float(jumpsize)), d_p)\n    \n    def step(self, closure):\n        \"\"\"Performs a single optimization step.\"\"\"\n        old_loss = closure()\n        i = 0\n        self.mini_step()\n        new_loss = closure()\n        losses = [old_loss.item(), new_loss.item()]\n        while (new_loss < old_loss) & (i < self.max_steps):\n            # we're using the secant method here to \n            # approximate the second order derivative \n            # http://acme.byu.edu/wp-content/uploads/2019/08/1dOptimization19.pdf\n            first_order_grad1 = (losses[-1] - losses[-2])/self.lr_orig\n            second_order_grad = (losses[-1] - first_order_grad1)/self.lr_orig\n            stepsize = -second_order_grad/first_order_grad1 * self.lr_orig\n            self.mini_step(stepsize)\n            old_loss = new_loss\n            new_loss = closure()\n            losses.append(new_loss.item())\n            i += 1\n        return new_loss\nResults\nA meaningful benchmark was hard to come up with so I just generated an artificial regression task with some deep layers. I don’t want to suggest that the following counts as “general performance” but they are interesting to think about. I’ll list a few results below.\nImplementation of generate_new_dataset\ndef generate_new_dataset(n_row, dim_in, dim_hidden, n_layers):\n    torch.manual_seed(0)\n    x = torch.randn(n_row, dim_in)\n    y = x.sum(axis=1).reshape(-1, 1)\n\n    model = torch.nn.Sequential(\n        torch.nn.Linear(dim_in, dim_hidden),\n        *[torch.nn.Linear(dim_hidden, dim_hidden) for _ in range(n_layers)],\n        torch.nn.Linear(dim_hidden, 1),\n    )\n    \n    loss_fn = torch.nn.MSELoss(reduction='mean')\n    \n    def loss_closure():\n        y_pred = model(x)\n        return loss_fn(y_pred, y)\n\n    return model, loss_fn, loss_closure, x, y\nImplementation of Data Collection\nresults = {}\nlearning_rate = 1e-3\noptimisers = {\n    'KS_50_0': lambda p: KeepStepping(p, lr=learning_rate, max_steps=50, scale_i=0),\n    'KS_50_2': lambda p: KeepStepping(p, lr=learning_rate, max_steps=50, scale_i=2),\n    'KS_10_0': lambda p: KeepStepping(p, lr=learning_rate, max_steps=10, scale_i=0),\n    'KS_10_2': lambda p: KeepStepping(p, lr=learning_rate, max_steps=10, scale_i=2),\n    'KV_10': lambda p: KeepVaulting(p, lr=learning_rate, max_steps=10),\n    'KV_50': lambda p: KeepVaulting(p, lr=learning_rate, max_steps=50),\n    'SGD': lambda p: torch.optim.SGD(p, lr=learning_rate), \n    'ADAM': lambda p: torch.optim.Adam(p, lr=learning_rate), \n}\n\nfor name, alg in optimisers.items():\n    model, loss_fn, loss_closure, x, y = generate_new_dataset()\n    n_steps = 1000 if not 'K' in name else 100\n    results[name] = np.zeros((n_steps, 2))\n\n    optimizer = alg(model.parameters())\n    \n    tic = time()\n    for t in tqdm.tqdm(range(n_steps)):\n        y_pred = model(x)\n        loss = loss_fn(y_pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step(loss_closure)\n        results[name][t, :] = [loss.item(), time() - tic]\n\nplt.figure(figsize=(16, 4))\nfor name, hist in results.items():\n    score, times = hist[:, 0], hist[:, 1]\n    plt.plot(times, score, label=name)\nplt.xlabel(\"time (s)\")\nplt.ylabel(\"mean squared error\")\nplt.legend();\n\n\n\nFigure 9: This run contained 10K datapoints with 20 columns as input while the network had 5 layers with 10 hidden units. The initial learning rate was 1e-3.\n\n\n\n\n\n\nFigure 10: This run contained 10K datapoints with 20 columns as input while the network had 5 layers with 10 hidden units. The initial learning rate was 1e-4. This run was the same as the previous one but has a much smaller initial stepsize. Everything looks a loot smoother, but also becomes a lot slower.\n\n\n\n\n\n\nFigure 11: This run contained 100K datapoints with 100 columns as input while the network had 5 layers with 20 hidden units. The initial learning rate was 1e-4. In this run we increased the number of columns.\n\n\n\n\n\n\nFigure 12: This run contained 100K datapoints with 10 columns as input while the network had 3 hidden layers with 10 units each. We also added Relu layers here. The initial learning rate was 1e-4.\n\n\n\n\n\n\nFigure 13: This run contained 100K datapoints with 100 columns as input while the network had zero hidden layers The initial learning rate was 1e-4.\n\n\n\nIt seems the impatient approach with a max step of 50 is beating Adam when it comes to convergence speed. The idea of vaulting is not performing as well as I had hoped but this may be due to numerical sensitivity. The idea seems to have merit to it but this work should not be seen as a proper benchmark.\nAlso … that last run is just a linear regression. The fastest way to optimise that is to use the hessian trick that we started with.\nReflection\nSo what does all of this mean?\nWell … it suggests that there may be a valid trade off between doing more work such that you need to do less gradient evaluations. Either you spend more time preparing a step such that you reduce the number of steps needed (like the hessian approach for the linear regression) or you just do a whole lot more of them without checking the gradient all the time.\nAppendix\nI’d be interested in hearing stories from folks who benchmark this, so feel free to try it out and let me know if it does (or does not) work on your dataset.\n\n\n\n",
    "preview": "posts/more-descent-less-gradient/idea-1.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 909,
    "preview_height": 365
  },
  {
    "path": "posts/priors-of-great-potential/",
    "title": "Priors of Great Potential",
    "description": "Common Sense reduced to Priors",
    "author": [
      {
        "name": "Matthijs Brouns",
        "url": "mbrouns.com"
      },
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-02-22",
    "categories": [],
    "contents": "\n\nsummary{\n  background-color: #fbfbfb;\n}\ndetails{\n  background-color: #fbfbfb;\n  padding: 25px;\n  padding-bottom: 10px;\n  padding-top: 10px;\n}\nThe goal of this document is to summarise a lesson we’ve had in the last year. We’ve done a lot of work on algorithmic bias (and open-sourced it). We reflected and concluded that constraints are an amazing idea that deserve to be used more often in machine learning.\nThis point drove us write the following formula on a whiteboard;\n\\[ \\text{model} = \\text{data} \\times \\text{constraints} \\]\nAfter writing it down, we noticed that we’ve seen this before but in a different notation.\n\\[ p(\\theta | D) = p(\\text{D} | \\theta) p(\\theta) \\]\nIt’s poetic: maybe … just maybe … priors can be interpreted as constraints that we wish to impose on models. It is knowledge that we have about how the model should work even if the data wants to push us in another direction.\nSo what we’d like to do in this blogpost is explore the idea of constraints a bit more. First by showcasing how our open source package deals with it but then by showing how a probibalistic approach might be able to use bayes rule to go an extra mile.\n\nThis document will contain the summaries, you can find the code here.\nDataset and Fairness\nWe will use a dataset from scikit-lego. It contains traffic arrests in Toronto and it is our job to predict if somebody is released after they are arrested. It has attributes for skin color, gender, age, employment, citizenship, past interactions and date. We consider date, employment and citizenship to be proxies that go into the model while we keep gender, skin color and age seperate as sensitive attributes that we want to remain fair on.\nHere’s a preview of the dataset.\n\nreleased\ncolour\nyear\nage\nsex\nemployed\ncitizen\nchecks\nYes\nFalse\n2002\nTrue\nFalse\nYes\nYes\n3\nNo\nTrue\n1999\nTrue\nFalse\nYes\nYes\n3\nYes\nFalse\n2000\nTrue\nFalse\nYes\nYes\n3\nNo\nTrue\n2000\nFalse\nFalse\nYes\nYes\n1\nYes\nTrue\n1999\nFalse\nTrue\nYes\nYes\n1\n\nThe dataset is interesting for many reasons. Not only is fairness a risk; there is also a balancing issue. The balancing issue can be dealt with by adding a class_weight parameter while the fairness can be dealt with in many ways (exibit A, exibit B). A favorable method (we think so) is to apply a hard constraint. Our implementation of EqualOpportunityClassifier does this running a logistic regression constrained by the distance to the decision boundary in two groups.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklego.linear_model import EqualOpportunityClassifier\n\nunfair_model = LogisticRegression(class_weight='balanced')\nfair_model = EqualOpportunityClassifier(\n    covariance_threshold=0.9, # strictness of threshold\n    positive_target='Yes',    # name of the preferable label\n    sensitive_cols=[0, 1, 2]  # columns in X that are considered sensitive\n)\n\nunfair_model.fit(X, y)\nfair_model.fit(X, y)\nDetails on the Methods.\nLogstic Regression works by optimising the log likelihood.\n\\[\n\\begin{array}{cl}{\\operatorname{minimize}} & -\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{i},\n        \\boldsymbol{\\theta}\\right) \\\\\\end{array}\n\\]\nBut what if we add constraints here? That’s what the EqualOpportunityClassifier does.\n\\[\n\\begin{array}{cl}{\\operatorname{minimize}} & -\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{i},\n        \\boldsymbol{\\theta}\\right) \\\\\n        {\\text { subject to }} & {\\frac{1}{POS} \\sum_{i=1}^{POS}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right) d\n        \\boldsymbol{\\theta}\\left(\\mathbf{x}_{i}\\right) \\leq \\mathbf{c}} \\\\\n        {} & {\\frac{1}{POS} \\sum_{i=1}^{POS}\\left(\\mathbf{z}_{i}-\\overline{\\mathbf{z}}\\right)\n        d_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{i}\\right) \\geq-\\mathbf{c}}\\end{array}\n\\]\nIt the log loss while constraining the correlation between the specified sensitive_cols and the distance to the decision boundary of the classifier for those examples that have a y_true of 1.\nSee documentation.\nResults\nThe main difference between the two approaches is that in the Logistic Regression scheme we drop the sensitive columns while the other approach actively corrects for them. The table below shows the cross-validated summary of the mean test performance of both models.\n\nmodel\neqo_color\neqo_age\neqo_sex\nprecision\nrecall\nLR\n0.6986\n0.7861\n0.8309\n0.9187\n0.6345\nEOC\n0.9740\n0.9929\n0.9892\n0.8353\n0.9893\n\nDetails on Equal Opportunity Score.\nOne way of measuring fairness could be to measure equal opportunity, which is abbreviated above as eqo. The idea is that we have a sensity attribute, say race, for which we don’t want unfairness with regards to the positive outcome \\(y = 1\\). Then equal opportunity is defined as follows;\n\\[ \\text{equality of opportunity} = \\min \\left(\\frac{P(\\hat{y}=1 | z=1, y=1)}{P(\\hat{y}=1 | z=0, y=1)}, \\frac{P(\\hat{y}=1 | z=0, y=1)}{P(\\hat{y}=1 | z=1, y=1)}\\right) \\] Extra details can be found here.\n\nYou can also confirm the difference between the two models by looking at their coefficients.\n\nmodel\nintercept\nemployed\ncitizen\nyear\nchecks\nLR\n-1.0655\n0.7913\n0.7537\n-0.0101\n-0.5951\nEOC\n0.5833\n0.7710\n0.6826\n-0.0196\n-0.5798\n\nThere are a few things to note at this stage;\nThe more fair model seems to shift the intercept drastically but the other columns (which are normalised) less so.\nThe more fair is arguably still useful despite having a lower precision. It compensates with fairness but also with recall.\nHaving a fairness constraint based on distance to the decision boundary is one description of fairness. There are many other measures of fairness and we’re only a particular one because it offers us a convex algorithm that can give us guarantees on the training data. It would be nice to have the opportunity to define more flexible definitions of fairness.\nThe current modelling approach allows us to make predictions but these predictions do not have uncertainty bounds.\nLinear models are fine, but sometimes we may want to apply constraints to hierarchical models.\nProbibalistic Programming\nThis brings us back to the formulae that we started with.\n\\[ \\text{model} = \\text{data} \\times \\text{constraints} \\]\nIn our case the constraints we want concern fairness.\n\\[ p(\\theta | D) \\propto \\underbrace{p(D | \\theta)}_{\\text{data}} \\underbrace{p(\\theta)}_{\\text{fairness?}} \\]\nSo can we come up with a prior for that?\nTo explore this idea we set out to reproduce our results from earlier in PyMC3. We started with an implementation of logistic regression but found that it did not match our earlier results. The results of the trace are listed below. We show the distribution of the weights as well as a distribution over the unfairness.\nPyMC3 Implementation.\nwith pm.Model() as unbalanced_model:\n  intercept = pm.Normal('intercept', 0, 1)\n  weights = pm.Normal('weights', 0, 1, shape=X.shape[1])\n\n  p = pm.math.sigmoid(intercept + pm.math.dot(X, weights))\n  \n  dist_colour = intercept + pm.math.dot(X_colour, weights)\n  dist_non_colour = intercept + pm.math.dot(X_non_colour, weights)\n  mu_diff = pm.Deterministic('mu_diff', dist_colour.mean() - dist_non_colour.mean())\n\n  pm.Bernoulli('released', p, observed=df['released'])\n\n  unbalanced_trace = pm.sample(tune=1000, draws=1000, chains=6)\n\n\n\n\nFigure 1: Standard Logistic Regression in PyMC3.\n\n\n\nYou might notice that the results of the traceplot do not agree with the coefficients that we’ve found earlier. This was because our original logistic regression had a balanced setting which is not included in our PyMC3 approach. Luckily for us PyMC3 has a feature to address this; pm.Potential.\npm.Potential\nThe idea behind the potential is that you add a prior on a combination of parameters instead of just having it on a single one. For example, this is how you’d usually set parameters;\nmu    = pm.Normal('mu', 0, 1)\nsigma = pm.HalfNormal('sigma', 0, 1)\nBy setting the sigma prior to be HalfNormal we prevent it from ever becoming negative. But what if we’d like to set another prior, namely that \\(\\mu \\approx \\sigma\\)? This is what pm.Potential can be used for.\npm.Potential('balance', pm.Normal.dist(0, 0.1).logp(mu - sigma))\nAdding a potential has an effect on the likelihood of a tracepoint.\n\n\n\nFigure 2: Example of a tracepoint that is both less (left) and more likely (right) given the potential.\n\n\n\nThis in turn will make the posterior look different.\n\n\n\nFigure 3: The effect that the potential might have.\n\n\n\nBack to Logistic Regression\nSo we made a second version of the logistic regression.\nPyMC3 Implementation.\nwith pm.Model() as balanced_model:\n    intercept = pm.Normal('intercept', 0, 1)\n    weights = pm.Normal('weights', 0, 1, shape=X.shape[1])\n\n    p = pm.math.sigmoid(intercept + pm.math.dot(X, weights))\n    \n    dist_colour = intercept + pm.math.dot(X_colour, weights)\n    dist_non_colour = intercept + pm.math.dot(X_non_colour, weights)\n    mu_diff = pm.Deterministic('mu_diff', dist_colour.mean() - dist_non_colour.mean())\n\n    pm.Potential('balance', sample_weights.values * pm.Bernoulli.dist(p).logp(df['released'].values))\n\n\n    balanced_trace = pm.sample(tune=1000, draws=1000, chains=6)\nThese results were in line with our previous result again.\nBut that pm.Potential can also be used for other things!\n\n\n\nFigure 4: From trace to posterior.\n\n\n\nSuppose we have our original trace that generates our posterior.\n\n\n\nFigure 5: Two belief systems …\n\n\n\nNow also suppose that we have a function that describes our potential.\n\n\n\nFigure 6: Two belief systems … merged!\n\n\n\nThen these two can be combined. Our prior can span beyond a single parameter! Our belief of how the model should behave before seeing data can influence the entire posterior. So we’ve literally come up with a prior that has a huge potential for fairness.\nX_colour, X_non_colour = split_groups(X, key=\"colour\")\n...\ndist_colour = intercept + pm.math.dot(X_colour, weights)\ndist_non_colour = intercept + pm.math.dot(X_non_colour, weights)\nmu_diff = pm.Deterministic('mu_diff', dist_colour.mean() - dist_non_colour.mean())\npm.Potential('dist', pm.Normal.dist(0, 0.01).logp(mu_diff))\nNote the 0.01 value on the bottom line. This value can be interpreted as strictness for fairness. The lower it is, the less wiggle room the sampler has to explore areas that are not fair.\nThe results can be seen below.\nPyMC3 Implementation.\nwith pm.Model() as dem_par_model:\n    intercept = pm.Normal('intercept', 0, 1)\n    weights = pm.Normal('weights', 0, 1, shape=X.shape[1])\n\n    p = pm.math.sigmoid(intercept + pm.math.dot(X, weights))\n\n    dist_colour = intercept + pm.math.dot(X_colour, weights)\n    dist_non_colour = intercept + pm.math.dot(X_non_colour, weights)\n    mu_diff = pm.Deterministic('mu_diff', dist_colour.mean() - dist_non_colour.mean())\n\n    pm.Potential('dist', pm.Normal.dist(0, 0.01).logp(mu_diff))\n    pm.Potential('balance', sample_weights.values * pm.Bernoulli.dist(p).logp(df['released'].values))\n\n    dem_par_trace = pm.sample(tune=1000, draws=1000, chains=6)\n\n\n\nFigure 7: Potential Fairness in PyMC3.\n\n\n\nStill a problem\nThere’s still an issue. We’ve gotten a flexible approach here. Compared to the scikit-learn pipeline we can have more flexible definitions of fairness and we can have more flexible models (hierarchical models, non-linear models) but at the moment our models does not guarantee fairness.\nBut then Matthijs came up with a neat little hack.\n\n\n\nFigure 8: Posterior Belief and Potential Direction\n\n\n\nWe use our potential to push samples in a direction. This push must be continous if we want gradients in our sampler method to be of any use to us. But after this push is done, we would we would like to make a hard cutoff on our fairness. So why don’t we just filter out the sampled points in our trace that we don’t like?\n\n\n\nFigure 9: After the data is pushed we do a hard filter.\n\n\n\nThis way, we still get a distribution over the models parameters out but this distribution is guaranteed to never assign any probability mass in regions where we deem the predictions to be ‘unfair’.\nPyMC3 Implementation.\ndef hard_constraint_model(df):\n    def predict(trace, df):\n        X = df[['year', 'employed', 'citizen', 'checks']].values\n        return expit((trace['intercept'][:, None] + trace['weights'] @ X.T).mean(axis=0))\n    \n    X = df[['year', 'employed', 'citizen', 'checks']].values\n    X_colour, X_non_colour = X[df['colour'] == 1], X[df['colour'] == 0] \n\n    \n    class_weights = len(df) / df['released'].value_counts()\n    sample_weights = df['released'].map(class_weights)\n    with pm.Model() as dem_par_model:\n        intercept = pm.Normal('intercept', 0, 1)\n        weights = pm.Normal('weights', 0, 1, shape=X.shape[1])\n\n        p = pm.math.sigmoid(intercept + pm.math.dot(X, weights))\n\n        dist_colour = intercept + pm.math.dot(X_colour, weights)\n        dist_non_colour = intercept + pm.math.dot(X_non_colour, weights)\n        mu_diff = pm.Deterministic('mu_diff', dist_colour.mean() - dist_non_colour.mean())\n\n        pm.Potential('dist', pm.Normal.dist(0, 0.01).logp(mu_diff))\n        pm.Potential('balance', sample_weights.values * pm.Bernoulli.dist(p).logp(df['released'].values))\n\n        dem_par_trace = pm.sample(tune=1000, draws=1000, chains=6)\n    return trace_filter(dem_par_trace, 0.16), predict\n\nNote that in the results, the fairness metric has a hard cutoff.\n\n\n\nFigure 10: Enforce Fairness in PyMC3.\n\n\n\nThe approach that we propose here is relatively generic. You can make hierarchical models and you have more flexiblity in your definition of fairness. You start with constraints which you need to translate into a potential after which you can apply a strict filter.\n\n\n\nFigure 11: The general recipe.\n\n\n\nWe’ve just come up with an approach where our potential represents fairness. Since we filter the trace afterwards we have an algorithm with properties we like. We don’t want to suggest this approach is perfect though, so here’s some valid points of critique;\nThis approach does not guarantee fairness. We’ve merely quantified an improvement on a single definition and only on our training set. This approach has merit to it but we should not presume that it will be enough. There’s technical details like drift but there’s also facets of fairness that may be hard to capture with a potential.\nThis approach will fail in settings where the model has so many hyperparameters that we can’t create a potential with domain knowledge. Neural networks for image detection may still be out of scope for this trick.\nSampler-based approaches don’t get stuck in local optima as often as gradient approaches but even NUTS can get stuck. We also can’t say for sure when the trace has enough samples to be reliable. The training procedure is also very expensive.\nFairness is a socio-technical problem. If reducing reality down to a technical problem got us in trouble in the first place, we shouldn’t hope a new definition of a technical problem will offer a complete solution. We also need to think about feedback mechanisms as well as qualitative understandings of how bias enters the system.\nConclusion\nDespite not guaranteeing fairness, we’re pretty exited about this way of thinking about models. The reason why is best described with an analogy in fashion and is summarized in this photo;\n\n\n\nFigure 12: This is what OneSizeForAll().fit() looks like. It never fits perfectly.\n\n\n\nWe think scikit-learn is an amazing tool. It sparked the familiar .fit()/.predict() interface that the ecosystem has grown accostomed to and it introduced a wonderful concept via its Pipeline-API. But all this greatness comes at a cost; people seem to be getting lazy.\nEvery problem get’s reduced to something that can be put into a .fit()/.predict() pipeline. The clearest examples of this can be found on the kaggle platform. Kaggle competitions are won by reducing a problem to a single metric, optimising it religiously and not worrying about the application. They’re not won by understanding the problem, modelling towards it or by wondering how the algorithm might have side-effects that you don’t want.\nIt is exactly on this axis that this approach gives us hope. Instead of calling model.fit() you get to enact tailor.model() because you’re forced to think in constraints. This means that we actually get to model again! We can add common sense as a friggin’ prior! How amazing is that!\nTo add a cherry on top; in our example we’re using fairness as a driving argument but the reason to be exited goes beyond that.\nMaybe you are predicting a timeseries but you want to prevent at all times that the prediction out of the model ever yields a negative value.\nMaybe you are building a recommender for netflix and you want to add a prior that punishes popular content such that the viewers don’t get stuck in a filter bubble and enjoy more diverse content.\nMaybe you are a researcher in healthcare and want to insure yourself against Simpsons paradox. One way of guarding yourself against this is by adding a prior that states that in all cases smoking is bad for your health. Even when an outlier suggests otherwise.\nThe act of thinking about constraints immediately makes you seriously consider the problem before modelling and that … that’s got a lot of potential.\nAppendix\nThis document is written by both myself and Matthijs Brouns and it is posted on two blogs. The code used here can be found in this github repository.\nIt’s also resulted in a conference talk at PyMCon.\n\n\n\n",
    "preview": "posts/priors-of-great-potential/drawing7.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 781,
    "preview_height": 378
  },
  {
    "path": "posts/introduction-to-inference/",
    "title": "Introduction to Inference",
    "description": "Reduce Common Sense to Calculus.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-02-15",
    "categories": [],
    "contents": "\n\nsummary{\n  background-color: #fbfbfb;\n}\ndetails{\n  background-color: #fbfbfb;\n  padding: 25px;\n  padding-bottom: 10px;\n  padding-top: 10px;\n}\n“The theory of probabilities is just common sense reduced to calculus.”\n– Pierre-Simon Laplace\nThis document contains an introduction to probibalistic inference meant as a comfortable first step for data people to get introduced to bayesian thinking. It will be a bit mathy, but nothing beyond kahn-level probability. It is a repost of an old document. I will assume you’ve done textbook exercises in probability theory before and that you’ve heard of bayes rule as well.\nDefinition\nBayes Rule is a lovely law in probability theory that is defined via a simple formula.\n\\[ P(E|D) = \\frac{P(E,D)}{P(D)} = \\frac{P(D|E)P(E)}{P(D)} \\]\nHere \\(E\\) and \\(D\\) are events. They can be anything. As a simplest example; suppose that I am rolling dice and that \\(D\\) is the number of dice that I’ve rolled and that \\(E\\) is the number of eyes that I see. Via simulation of dice I can generate plots of what \\(P(E|D)\\) might look like.\n\n\n\nFigure 1: When we roll more dice, the expect a different number of eyes.\n\n\n\nIn this plot you see the probability distribution for the number of eyes given the number of dice that are rolled. The number of dice are listed on the right hand side, the number of eyes are denoted on the x-axis and the y-axis shows how often this occured in a simulation. You should notice that when I roll 4 dice that it is not likely for me witness a total of 4 eyes but it is impossible to get 3. This is what we expect and this is what we could also calculate with probability theory.\nHere’s an amazing thing though; you could turn this plot on it’s head. If you only switch the numbers on the x-axis with the numbers on the right hand side then you suddenly have all distributions for \\(P(D|E)\\). We can use what we know about \\(P(E|D)\\) to gain knowledge about \\(P(D|E)\\). As we’ll see in the next formal example, this is a very nice property and it’s something that follows straight from probability theory.\n\n\n\nFigure 2: Same simulated data, but two different questions you can ask it. Here P(D|E)\n\n\n\n\n\n\nFigure 3: Same simulated data, but two different questions you can ask it. Here P(D|E)\n\n\n\nIt’s not just pretty colors. There’s many benefits of this way of thinking.\nTextbook Example\nLet’s discuss a textbook example, one that I remember from my exams.\n\nThere is an epidemic. A person has a probability 1/100 to have the disease. The authorities decide to test the population, but the test is not completely reliable: the test generally gives 1/110 people a positive result but given that you have the disease the probability of getting a positive result is 65/100.\n\nLet \\(D\\) denote the event of having the disease, let \\(T\\) denote event of a positive outcome of a test. If we are interested in finding \\(P(D|T)\\) then we can just go and apply Bayes rule:\n\\[P(D|T) = \\frac{P(T|D)P(D)}{P(T)} = \\frac{\\frac{65}{100} \\times \\frac{1}{100}}{\\frac{1}{110}} \\approx 0.715\\]\nThis is neat. We learned something about having the disease by knowing somthing on the performance of the test. Bayes rule gives us a precise method to generate more knowledge after having observed data. In this case the data that we observe is the fact that the test was positive. What else can we calculate?\nInfer Moar!\nSuppose that you do not have the disease, can the test come out positive? In maths: what is \\(P(T| \\neg D)\\)?\nWe can use basic probability theory again and split up the probability of getting a positive test result. It’s easier to think about this outcome by splitting it up into two situations; the situation that multiple tests are positive given that you have the disease and the situation that multiple tests are positive given that you don’t have the disease.\n\nIn maths \\(\\neg D\\) is read “not \\(D\\)”. So \\(P(T | \\neg D)\\) formally reads \"the probability when we have a positive test but a negative for the disease.\n\\[\n\\begin{align}\nP(T) & = P(T\\cap D) + P(T\\cap \\neg D) \\\\\n     & = P(T|D)P(D) + P(T|\\neg D)P(\\neg D)\n\\end{align}\n\\]\nWe know \\(P(T|D)\\) , \\(P(D)\\) and \\(P(T)\\). We can figure out that \\(P(¬D)=1−P(D)\\) too. So we only need to fill these numbers into the formula above to get;\n\\[ P(T|\\neg D) = \\frac{19}{7260} \\]\nInfer Even Moar!\nSofar, we’ve only considered the outcome of one test. But a person might seek a second opinion and go to another doctor. If we assume that these two doctors don’t influence eachother then again we can think about what probability theory has to say about this. So what are the probabilities from two or three tests being positive?\n\\[\n\\begin{align}\nP(TT) &= P(TT|D)P(D) + P(TT| \\neg D) P(\\neg D) &\\approx 0.004237374 \\\\\nP(TTT) &= P(TTT|D)P(D) + P(TTT| \\neg D) P(\\neg D) &\\approx 0.002746294\n\\end{align}\n\\]\nDerivation of the Maths.\nYou might be wondering how \\(P(TT|D)\\) and \\(P(TT | \\neg D)\\) are calculated. The main trick here is realising that \\(P(TT | D) = P(T | D) \\times P(T | D)\\).\nSo why does that make sense? If you take two tests, surely they are related, no?\nThis is true, but only if we don’t know if you have the disease. If we know that you have the disease then the doctors you will see might disagree but in general they should tell you 65% of the time that you have the disease. It is assumed that the doctors don’t influence eachother. This means that each test result that each doctor gives you must be independant.\n\\[\n  \\begin{align}\n  P(TT) &= P(TT|D)P(D) + P(TT| \\neg D) P(\\neg D) \\\\\n        &= P(T|D) P(T|D)P(D) + P(T| \\neg D) P(T| \\neg D) P(\\neg D) \\\\\n        &= P(T|D)^2 P(D) + P(T| \\neg D)^2 P(\\neg D) \n  \\end{align}\n  \\] This also holds for more tests.\n\\[\n  \\begin{align}\n  P(TTT) &= P(TTT|D)P(D) + P(TTT| \\neg D) P(\\neg D) \\\\\n        &= P(T|D)^3 P(D) + P(T| \\neg D)^3 P(\\neg D) \n  \\end{align}\n  \\] You might still be confused. Why \\(P(TT) \\neq P(T) \\times P(T)\\) while \\(P(TT | D) = P(T|D) \\times P(T|D)\\)?\nUnderstanding this properly takes a while (my experience). For now I hope you reliase that \\(P(TT|D)\\) can be interpreted as a query. It asks for the likelihood of seeing two positive tests, given that you have the disease. \\(P(TT)\\) is a strictly different query.\nThe next part of the post contains diagrams that might also help to understand this.\n\nNotice that these probabilities were not strictly given to us. These are probabilities that we’ve inferred from the information known to us by applying probability theory. The only true assumption that we’ve made is that the only thing that influeces a test result from a doctor is the presence of the disease (i.e. the doctors are not allowed to influence eachother beyond that).\nGraphical Views\nLet’s make a drawing of the causual graph next to our original formula.\n\\[ \n\\begin{align}\nP(D|T) & = \\frac{P(T|D)P(D)}{P(T)} = \\frac{\\frac{65}{100} \\times \\frac{1}{100}}{\\frac{1}{110}} \\\\\n       & \\approx 0.715 \n\\end{align}\n\\]\nCausal Diagram The graph says that the disease is what has an effect on the test result. Not the other way around. This makes sense. It’s not the test that makes you sick. The fact that you are sick makes the test result more likely to be positive. It is this information that the causal chart demonstrates. Nothing about probability values, just the causal relationships.\nTwo Tests\nLet’s move on to calculate something else that is interesting; what is the probability of having the disease after two doctors do two tests on you? We’ll write down the maths and draw the causal diagram again.\n\\[ \n\\begin{align}\nP(D|TT) & = \\frac{P(TT|D)P(D)}{P(T)}  \\\\\n       & \\approx 0.9971\n\\end{align}\n\\]\nCausal Diagram Note that we are now more certain! After seeing two positive test results it is harder to assume you don’t have the disease.\nHow to calculate \\(P(TT|D)\\).\nThe Rule-Of-Total-Probability[tm] states that;\n\\[\n\\begin{align}\nP(T) & = P(T\\cap D) + P(T\\cap \\neg D) \\\\\n     & = P(T|D)P(D) + P(T|\\neg D)P(\\neg D)\n\\end{align}\n\\] In words; there’s only two options if you have a positive test. It is positive and you’ve got the disease or it’s positive and you don’t. By splitting this probability into two seperate quantities to calculate we get a formula with 5 items. Out of these we know \\(P(T)\\), \\(P(D)\\) and \\(P(\\neg D)\\). What about \\(P(T|D)\\) and \\(P(T|\\neg D)\\)?\n\\(P(T|D)\\) should not be confused with \\(P(D|T)\\), but we can use bayes rule to use \\(P(D|T)\\) to calculate \\(P(T|D)\\).\n\\[ P(T|D) = \\frac{P(D , T)}{P(D)} = \\frac{P(D|T)P(T)}{P(D)}\\] That formula contains \\(P(D|T), P(T)\\) and \\(P(D)\\). These are all known. A similar conclusion can be made for the situation with \\(\\neg D\\)\n\\[ P(T| \\neg D) = \\frac{P(\\neg D , T)}{P(\\neg D)} = \\frac{P(\\neg D|T)P(T)}{P(\\neg D)}\\] Because \\(P(\\neg D) = 1 - P(D)\\) and \\(P(\\neg D | T) = 1 - P(D | T)\\) we again have everything we need.\nTo calculate \\(P(TT|D)\\) we merely need to multiply since the two tests are independant \\(P(TT|D) = P(T|D) \\times P(T|D)\\).\n\nThree Tests\nLet’s move on to calculate something else that is interesting; what is the probability of having the disease after two doctors do two tests on you? We’ll write down the maths and draw the causal diagram again.\n\\[ \n\\begin{align}\nP(D|TTT) & = \\frac{P(TTT|D)P(D)}{P(T)}  \\\\\n       & \\approx 0.9999\n\\end{align}\n\\]\nCausal Diagram This results makes sense. After seeing three doctors, it starts getting pretty likely that you actually have the disease. We also see that the added certainty of doing a third test after seeing two positives is low. We gain little information.\nPretty Amazing\nWith the help of Bayes’ Rule we’ve just inferred information about \\(P(D | TTT)\\) even though we initially only started with \\(P(T|D)\\), \\(P(T)\\) and \\(P(D)\\).\nPart of the trick here lies in the causal graph. It defines a model of the world. By assuming that the disease is the only thing that can influence a test outcome we can infer the rest. But it is exactly this makes inference so strong! By defining the causal direction between variables (common sense) we can calculate the rest (calculus).\nEven Moar?\nLet’s draw the diagram again but now differently. We’ll now put in a question-mark when we don’t know the variable. That way we can encode a query into a pretty picture. So let’s draw one for \\(P(T_3 | T_1, T_2)\\). In words this means “Given that you’ve done two tests that are both positive, how likely is it that the third one is also positive?”\nQuery Diagram The maths for this query is listed below.\n\\[P(T_3 | T_1 T_2 ) = \\frac{P(T_1, T_2, T_3)}{P(T_1, T_2)} = \\frac{1076657}{1661220} \\approx 0.6481\\]\nAlternative Derivation.\nThe above derivation works, but there is an alternative.\n\\[\n  \\begin{align}\nP(T_3 | T_1, T_2) & = P(T_3 | T_1, T_2, D) P(D | T_1, T_2) + P(T_3 | T_1, T_2, \\neg D) P(\\neg D | T_1, T_2) \\\\\n& = P(T_3 | D) P(D | T_1, T_2) + P(T_3 | \\neg D) P( \\neg D | T_1, T_2) \\\\\n&\\approx 0.6481\n\\end{align}\n\\] To get here it helps to realise that;\n\\[\n\\begin{align}\nP(D | T_1, T_2) & = \\frac{16731}{16780} \\\\ \nP(\\neg D | T_1, T_2) & = 1 - P(D | T_1, T_2) = \\frac{49}{16780}\n\\end{align}\n\\] I hope you can infer the benefits of the former method over the latter.\n\nYou can describe the sensation of inference through words as well. If two tests are positive, it is very likely that you have the disease. If it is very likely that you have the disease then it is also rather likely that the next test is positive. If you look at the graph, the inference that we do on it has a certain direction. We infer knowledge of \\(T_3\\) via \\(D\\).\n\nSpeaking of “sensation”. Notice that 0.6481 is almost the same as \\(P(T|D)\\). Makes sense right?\nThe graphical representation tells us that once we know \\(D\\) the events \\(T1...T3\\) are independant of eachother. That doesn’t mean that we cannot infer about \\(P(D|T_i)\\), it merely means that we have to keep track of different assumptions in our model. Probability theory resolves the rest.\nInference Diagram Conclusion\nPlease recognize how powerful this methodology is. Not only does this way of thinking give you immense modelling flexibilities but it even allows you to work with actual probabilities at all times. Many machine learning models try to approximate this thinking with more black box sort of methods but it loses some probibalistic interpretation in the process.\nImportant Detail\nIn this blogpost I’ve discussed a simple example that demonstrates the elegant parts of Bayes’ rule. But the model on display here might not be the best way of thinking about the world. The assumption that tests do not influence eachother is a big one. If you see a second doctor from the same hospital then odds are that this doctor will not have a completely independant opinion.\nTo accomodate for this you might need another way of looking at the world. A more hierarchical way of looking at it. So we might need to update our causal graph.\nMaybe a causal model like this;\n\n\n\nFigure 4: May be a better model. D is for disease, T is for a test/doctor, H is for hospital\n\n\n\nThis graph tells us qualitatively how variables are related but now we’ve gotta come up worth datasets that help us to quantify the effects between variables.\nNo free lunch here; modelling is still very hard.\nExperiment\nI have an alpha (ALPHA!) open source project that tries to make these calculations easy. It is unlikely that I’ll be able to offer any support for it anytime soon but it might be fun to play with.\nIt gives you an overview of both causaul graphs as well as plots of the queries you send it. Feed it a dataframe with discrete variables and you can ask it questions.\n\n\n\nFigure 5: Good lookin’ graphics!\n\n\n\nThey’re good DAGS and the project is called brent. Documentation found here.\nBlog Tech Revamp\nI was really exited to redo this blogpost because it allowed me to demonstrate a really cool html5 rick. Notice those blocks you can click that will expand? That’s all html, no javascript.\n<details>\n  <summary><b>Alternative Derivation.<\/b><\/summary>\n  Blah blah text and explain, maybe an image? \n<\/details>\nThis is amazing. I often wonder if I should include details in the blogpost and now I can leave it to the end-user to explore details if needed. This saves on screen real-estate and also makes the post a lot less intimidating to explore.\nThe medium for the content should invite people, not scare them away. Maths unfortunatly is a language that is great for precision of details but bad for recall of inclusion.\n\n\n\n",
    "preview": "posts/introduction-to-inference/landing.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1287,
    "preview_height": 427
  },
  {
    "path": "posts/theoretical-dependence/",
    "title": "Theoretical Dependence",
    "description": "Some assumptions are just plain weird.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-01-16",
    "categories": [],
    "contents": "\nThe simplest regression might look something like this;\n\\[ \\hat{y}_i \\approx \\text{intercept} + \\text{slope } x_i\\]\nYou might assume at this point in time that the slope and intercept are unrelated to eachother. In econometrics you’re even taught that this assumption is necessary. If you’re one I have to warn you for what you’re about to read. I’m about to show you why, by and large, this independance assumption is just plain weird.\nPosterior\nLet’s generate some fake data and let’s toss it in PyMC3.\nimport numpy as np\nimport matplotlib.pylab as plt\n\nn = 1000\nxs = np.random.uniform(0, 2, (n, ))\nys = 1.5 + 4.5 * xs + np.random.normal(0, 1, (n,))\nPyMC3\nLet’s now throw this data into pymc3.\nimport pymc3 as pm\n\nwith pm.Model() as model:\n    intercept = pm.Normal(\"intercept\", 0, 1)\n    slope = pm.Normal(\"slope\", 0, 1)\n    values = pm.Normal(\"y\", intercept + slope * xs, 1, observed=ys)\n    trace = pm.sample(2000, chains=1)\n\nplt.scatter(trace.get_values('intercept'), trace.get_values('slope'), alpha=0.2)\nplt.title(\"pymc3 results\");\nThat’s interesting; the intercept and slope variables aren’t independant at all. They are negatively related!\nScikit-Learn\nDon’t trust this result? Let’s sample subsets and throw it into scikit learn, let’s see what comes out of that.\nsize_subset = 500\nn_samples = 2000\nsamples = np.zeros((n_samples, 2))\n\nfor i in range(n_samples):\n    idx = np.random.choice(np.arange(n), size=size_subset, replace=False)\n    X = xs[idx].reshape(n_samples, 1)\n    Y = ys[idx]\n    sk_model = LinearRegression().fit(X, Y)\n    samples[i, 0] = sk_model.intercept_\n    samples[i, 1] = sk_model.coef_[0]\n    \nplt.scatter(samples[:, 0], samples[:, 1], alpha=0.2)\nplt.title(\"sklearn subsets result\");\nAgain, negative correlation.\nWhy?\nSo what is going on here? We generated the data with two independent parameters. Why are these posteriors suggesting that there is a relationship between the intercept and slope?\nThere are two arguments to make this intuitive.\nArgument One: Geometry\nConsider these two regression lines that go into a single point.As far as the little point is concerned both lines are equal. They both have the same fit. We’re able to exchange a little bit of the intercept with a little bit of the slope. Granted, this is for a single point, but also for a collection of points you can make an argument that you can exchange the intercept for the slope. This is why there must be a negative correlation.\nArgument Two: Causality\nConsider this causal graph. The \\(x_0\\) node and \\(x_1\\) node are independant, that is, unless \\(y\\) is given. That is because, once \\(y_i\\) is known, we’re back to a single point and then the geometry argument kicks in. But also because logically we could explain the point \\(y_i\\) in many ways; a lack of \\(x_0\\) can be explained by an increase of \\(x_1\\), vise versa or something in between. This encoded exactly in the graphical structure.\nConclusion\nIt actually took me a long time to come to grips with this. Upfront the linear regression does look like the addition of independant features. But since they all need to sum up to a number, it is only logical that they are related.\nAssuming properties of your model upfront is best done via a prior, not by an independence assumption.\nAppendix\nThe interesting thing about this phenomenon is that it is so pronounced in the simplest example. It is far less pronounced in large regressions with many features, like;\n\\[ y_i \\approx \\text{intercept} + \\beta_1 x_{1i} + ... + \\beta_f x_{fi} \\] Here’s some plots of the intercept value vs. the first estimated feature, \\(\\beta_1\\) given \\(f\\) features;\n\nYou can clearly see the covariance decrease as \\(f\\) increases. The code for this can be found here.\n\n\n\n",
    "preview": "posts/theoretical-dependence/logo.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 803,
    "preview_height": 406
  },
  {
    "path": "posts/cost-of-success/",
    "title": "The Cost of Success",
    "description": "VeryHighAccuracy[tm] as a Problem.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-01-06",
    "categories": [],
    "contents": "\nFraud is a common use-case for machine learning algorithms. It is also a very hard problem to tackle. Not only is it a cat and mouse game; it is also a low-signal vs. noise environment.\nDespite this it is not uncommon to hear about success stories, especially at tech conferences. What follows is an imaginary story based on talks I’ve had with folks at said tech conferences. It will illustrate something crazy that is bound to happen somewhere, someday.\nSucces\nLet’s imagine you work at a financial firm.\nYour team just spent a month running gridserarch. Squeezing every last drop of accuracy out of your cross-validations. But it was all well spent; your model went to production.\nAnd not just any model. One that is very accurate at detecting cases of fraud at the financial institution that you work for. This is not only confirmed by your cross validated experiments on the offline dataset but also in the field. It has proven itself to such an extend that the fraud teams are spending most of their time prosecuting. Gone are the days of searching/investigating for fraudsters.\n\nRaises are given to your team. Promotions are handed out too. White-papers get written. And to put the cherry on top; you get the opportunity to brag about it at tech conferences.\nCost\nOne might wonder though. Presumably there’s plenty of people at this financial firm who have been fighting fraud for many years (even decades). Is it really that plausible that merely tuning an algorithm on a dataset is enough to solve issue of fraud at this financial? It might sound a bit too good to be true. So let us instead consider a more plausible explanation of what just happened.\nThe typical reason why fraud use-cases are hard, even for machine learning algorithms, is not just because of the class imbalance. It is because you cannot trust the labels in the first place. The labels for ‘fraud’ are probably trustworthy, but the label indicating ‘no-fraud’ might very well be undetected fraud. It has probably never been checked by a human simply because there are so many cases to consider. It is also plausible that the fraud cases that did receive a label are the ones that were easy to detect. Your training data is bound to have more easy examples than hard ones.\nSo what happens when you present a fraud team with an ML algorithm that is very accurate? They will receive an algorithm that has a blind spot for very hard cases of fraud in favour of the easy ones.\n\nUnfortunately this bias will propogate indefinately because when this algorithm is applied it will send teams to easy cases making sure that any new training data will also focus on it. The fact that your algorithm is highly accurate will only reinforce it’s application which in turn will keep giving the appearance like you are fixing fraud.\nThe naughty, but plausible, situation is that it is now harder to find the harder types of fraud because your model has been reported to be highly accurate.\nIrony\nIt is because the model was reported to be highly accurate that people were less critical in the application of it. It’s not just in this theoretical fraud example, blind faith in machines has already caused deaths. The best example of this is the dozens of deaths caused by GPS in death valley. Instead of following the warning signs people blindly trusted their GPS devices to be more accurate.\nIt really is ironic. A 100% accurate model might cause blind faith, which will do horrible things for the application of the model while at the same time 100% accuracy is the goal we all try to achieve when we train our systems.\nIt not just fraud either. Can you imagine what it is like if doctors put blind trust in algorithms that detect diseases?\n\nSo maybe, when we think about the application of machine learning models, we should think about the feedback loop first. And maybe, just maybe, we need to serve highly accurate models with some entropy. Not so much entropy that people who rely on it won’t trust it but enough such that the people who rely on it don’t trust it blindly.\nBlind faith is the expensive cost to success.\nAppendix\nPart of the inspiration of this piece came from an episode of cautionary tales. It’s a podcast that I can recommend. Many of the warnings in that podcast can be directly applied to the field of machine learning.\n\n\n\n",
    "preview": "posts/cost-of-success/examples.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 848,
    "preview_height": 368
  },
  {
    "path": "posts/elephants/",
    "title": "The Elephants not in the Room",
    "description": "The unknowns might be more interesting.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2020-01-06",
    "categories": [],
    "contents": "\nI heard a cute anecdote that reminded me on the importance of solving the right problem. I am rephrasing the actual story in order to make a general point.\nImagine\nImagine that you’re a ranger and that it is your job to protect elephants from poaching. A large part of protecting the elephant herds is understanding the movement patterns them in a national park. Elephants are beautiful creatures, but they don’t like having sensors put on them. Instead it turned out to be possible to put up a huge array of microphones in the park. The idea has many benefits; you could detect sounds of many animals, including elephants, by recording over time and since the microphones are stationary you could even power them via solar.\n\nThere’s only one problem; the hundreds of these microphones will produce a lot of audio, too much to listen to. Thankfully, there is a Phd who is willing to write a thesis on the topic of detecting the audio of elephants. The Phd goes off, imports the libraries, let’s the tensors flow and after a week comes back with a highly accurate model; 100% accurate, cross-validated and bound to be a huge time-saver.\nThis result amazes the national forest’s investors and quickly becomes the prime focus of the year in terms of budget. All the rangers got an app that would tell them where they would need to go to find elephants.\nThen the field tests occur and it suddenly turns out that … the model actually isn’t 100% accurate in real life. About 20% of the time there are no elephants when the algorithm says that there should be. The rangers never got the audio clips, only the predictions, so they could never really varify what happened. All they observed was that the elephants weren’t there.\nWhat happened?\nIf you’re in the field of data science, an alarm bell should be ringing. It seems plausible that the dataset that was used during training came from a zoo that does not represent the elephants in the wild. This is a very fair thing to point out and is certainly something that deserves attention.\nBut maybe, the problem is that you shouldn’t be looking for elephant sounds in the first place. You should be listening for gunshots. Not only are they easier to detect, they are also the more pressing observation to react to. \nSlight Danger\nIn this example it would have helped if the rangers didn’t just get the app but also were active participants in the labelling of the data. Rangers might recognize that there’s a more important label missing from the algorithm.\nBut maybe the more human part of the problem here was that the high original accuracy was distracting the investors from more effective things that could be done. It makes sense to think “we can predict this accurately, let us focus and look for an application!” but this should never prevent you from considering the more low hanging fruit first.\nYou don’t want to be the person who is pointing to elephants that are not in the room.\n\n\n\n",
    "preview": "posts/elephants/img1.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1722,
    "preview_height": 514
  },
  {
    "path": "posts/roman-reasoning/",
    "title": "Roman Reasoning",
    "description": "We should evolve beyond it.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-12-07",
    "categories": [],
    "contents": "\nRomans are well known having for their numbers system but geez … did they have it hard.\n\nLook at the numbers for a while and you will recognize how hard these numbers are to work with. Seriously. You can’t think in fractions or in rational numbers, which is a serious constraint. But worst of all, there is no definition for zero.\n\nConsider the decimal system we’re used to today. What is the prime feature? It’s the idea of zero! The romans did not have a notation for it. Think about how limiting this is!\n\nBy introducing the notion (and notation!) of zero you suddenly get the opportunity rethink numbers a bit.\n\nOnce you have a decimal system of numbers, you realize that multiplying by ten is a common task because it is baked into the grammar of the notation. It takes a very consious mind to switch from roman numbers to numeral ones but once you’re there you can unconsiously explore numbers in a more interpretable manner. I can’t even imagine how a mind can conjure the idea of powers without having the decimal system to build on.\n\nAnd without powers, there wouldn’t be a square root either. Or logarithms.\nBack when I taught calculus, this bit of number theory was the first lecture. It’s plainly a great lesson of mathematics; notation actually matters and you really need to be aware of that. Notation is a user interface for thoughts and a way to serialise ideas. If you get this part wrong you will risk being stuck in translation when you’re trying to clearly conceptualize abstract thoughts. It will be so hard to explore and generalize problems if you cannot communicate them clearly.\nThe most powerful idea here is not to brag about how good our language of math is right now. Instead the idea is to keep wondering. When we are better at communicating ideas, will the ideas become better?\nThe answer is ‘very yes’.\nRecent Example\nI was playing around with a heuristic problem and I wrote a small evolutionary algorithm for it. It looked something like this (it’s pseudo-python code).\npopulation = [init_chromosome() for i in range(100)]\nfor gen in generations:\n    # first we calculate the scores per individual\n    # and we keep the best performing members of the population\n    scores = [calc_score(p) for p in population]\n    population = sorted(population, key=scores)[:50]\n    # from the survivors we make new offspring\n    for _ in range(50):\n        parent1, parent2 = pick_parents(population)\n        child = combine(parent1, parent2)\n        population.append(child)\n    # the new population will mutate\n    for i, p in enumerate(population):\n        population[i] = mutate(p)\nThe code (at least, to me) is pretty clear. I’ve abstracted lot’s of details into functions which makes the code clean. Still, it might help to add a diagram of what is happening.\n\nLooking at the code and looking at the diagram one cannot help but wonder; is the code really the clear communication device here? Or is the diagram better? It sort of feels like for-loops are kind of like the roman numbers. They represent a concept but they are not ideal methods of communicating. They merely facilitate the execution of ideas. I would prefer the code invites evolution of ideas instead.\nSo I’ll attempt a rephrase of the code Let’s see if we can rewrite it such that the code actually starts looking like the diagram. Here’s a first attempt.\npop = Population(creator=init_chromosome, members=100, \n                 score_func=calc_score)\n\nfor gen in generations: \n  pop = (pop\n         .survive(p=0.5)\n         .breed(parent_picker=pick_parents, combiner=combine)\n         .mutate(mutate_func=mutate))\nThere’s a lot less boilerplate and this brings in a lot of benefits.\nThere is less boilerplate so it is easier to maintain. The user can spend more time getting the pick_parents/init_chromosome functions right. Less clutter means more focus on the parts of the sytem that deserve the most attention on.\nWe are invited to play around more. It is much easier to swap a mutate function with an alternative. It’s also much easier to swap the order of steps around. Want mutate before you breed? Have at it! Less worry about the details of the for-loop means higher iteration speed. It’s meta, but evolution says that this is better.\nThe population object is a noun while the actions that are performed on it are verbs (survice, breed, mutate). Notice that a verb tells us what is happening while the function that goes in tells us how that action is being performed. It is much easier to reason about a sequence of steps when the description is close to actual human language.\nThere’s also a nice separation of concerns when it comes to performance. Since the loops are abstracted away we get the opportunity to parallise the for loop without bothering the domain expert with this task. You can allmost call this map-reproduce.\nWhen you start to notice these benefits you also play around with other ways in which this method can be used. For example, we might add keyword arguments to add more expressive behavior to the evolution steps.\npop = Population(creator=init_chromosome, members=100, \n                 score_func=calc_score)\n\nfor gen in generations: \n  pop = (pop\n         .survive(p=0.5)\n         .breed(parent_picker=pick_parents, combiner=combine)\n         .mutate(mutate_func=mutate, noise=10)\n         .survive(p=0.1)\n         .breed(parent_picker=pick_parents, combiner=combine)\n         .mutate(mutate_func=mutate, noise=1))\nThis is cool because we get to re-use parts. We can still use the mutate function but we’re now able to tell the algorithm to alternate between high noise and low noise.\n\nThere’s still a for-loop in there though. And we can remove it if we introduce yet another abstraction.\npop = Population(creator=init_chromosome, members=100, \n                 score_func=calc_score)\n\nevo = (Evolution()\n         .survive(p=0.5)\n         .breed(parent_picker=pick_parents, combiner=combine)\n         .mutate(mutate_func=mutate, noise=10)\n         .survive(p=0.1)\n         .breed(parent_picker=pick_parents, combiner=combine)\n         .mutate(mutate_func=mutate, noise=1))\n\npop.repeat(evo, n=generations)\nBy having a seperate object that represents a collection of steps we can more easily isolate the population and the actions that are being applied to it. Our evolutions can be declared much more expressively this way. Consider this one;\npop = Population(creator=init_chromosome, members=100, \n                 score_func=calc_score)\n\nevo1 = (Evolution()\n         .survive(p=0.5)\n         .breed(parent_picker=pick_parents, combiner=combine)\n         .mutate(mutate_func=mutate, noise=10))\nevo2 = (Evolution()\n         .survive(p=0.1)\n         .breed(parent_picker=pick_parents, combiner=combine)\n         .mutate(mutate_func=mutate, noise=1))\nevo3 = (Evolution().repeat(evo1, n=10).repeat(evo2, n=20))\n\npop.repeat(evo3, n=30)\nHere’s the associated diagram.\n\nWe’re nearing the moment when I prefer looking at the code for clarity.\nWhy this matters\nHere’s the code that you’d need to type if you did not have a Population or a Evolution as a vantage point and had to resort to for-loops instead.\npopulation = [init_chromosome() for i in range(100)]\n# start outer repeat step\nfor evo3 in range(30):\n  # start the evolution with the small noise param\n  for evo1 in range(10):\n    # first we calculate the scores per individual\n    # and we keep the best performing members of the population\n    scores = [calc_score(p) for p in population]\n    survivors = sorted(population, key=scores)[:50]\n    # from the survivors we make new offspring\n    for _ in range(50):\n        parent1, parent2 = pick_parents(population)\n        child = combine(parent1, parent2)\n        population.append(child)\n    # the new population will mutate\n    for i, p in enumerate(population):\n        population[i] = mutate(p, noise=10)\n  # start the evolution with the medium noise param\n  for evo2 in range(20):\n    # first we calculate the scores per individual\n    # and we keep the best performing members of the population\n    scores = [calc_score(p) for p in population]\n    survivors = sorted(population, key=scores)[:10]\n    # from the survivors we make new offspring\n    for _ in range(90):\n        parent1, parent2 = pick_parents(population)\n        child = combine(parent1, parent2)\n        population.append(child)\n    # the new population will mutate\n    for i, p in enumerate(population):\n        population[i] = mutate(p, noise=1)\nThis code smells bad and the comments aren’t helping. There’s repeating parts so there is a risk of a copy-paste bug in there. But that’s not the main issue.\nThe main issue is that this way of writing code will not inspire anyone to find the most appropriate algorithm. There’s too much boilerplate. Too many moving parts. My mind is stuck in endless translation. It’s unlikely to do any meaningful iterations from this standpoint. It’s like trying to understand the square root while all I have are roman numbers as a vehicle.\nFood for Reflection\nJust like you’ll never become a great driver when your mind is pre-occupied with understanding how the engine works you’ll never become a great algorithm designer if you don’t allow yourself to consider a new vantage point.\nWhen we are better at communicating ideas, will the ideas become better?\nThe answer is ‘very yes’. To be blunt; I’ve seen many codebases stuck in a ‘roman’ state and it has always proven to be a good idea to take the time with the team to come up with a better way of thinking. You can adapt a programming language such that it is more specific to your domain. You can even introduce the jargon of your domain in the classes and functions.\n\nI guess that’s why people sometimes make domain specific languages.\nSo maybe … as a rule of thumb; when a diagram explains the steps of your system way better than your code, think about how you can get the code to look more like the diagram. More times than not, it’s not impossible to get there and you will have more joy in your day-to-day as a result.\nAppendix\nThe point I wanted to make here was that it makes sense to take a step back when you are working on a project and to wonder if you are “stuck using roman numbers”. That said; it’s probably not a shocker for you to learn that I’ve been maintaining a library (together with Rogier van der Geer) that does exactly what I’ve described in this post.\nFeel free to play with it! It’s called evol and you can just pip install it. It’s an interesting project. We wrote it because the idea of exploring a grammar was very exiting. There exists a pipeline for scikit-learn but there wasn’t a good pipeline for heuristics. We spent many evenings (many whiteboard markers) trying to figure out what the ideal level of abstraction was. It’s not the most performant library but we like to think it is the most fun to write heuristics in.\nNote that when we wrote it we didn’t have an application for genetic algorithms what-so-ever. We merely imagined that this way of looking at heuristics was going to lead to better heuristics. Two years have passed by and despite our inactivity it has been downloaded 31K times.\n\n\n\n",
    "preview": "posts/roman-reasoning/roman4.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 777,
    "preview_height": 196
  },
  {
    "path": "posts/inconvenient-feedback/",
    "title": "What Overfitting Looks Like",
    "description": "GridSearch is not enough: Part Three.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-11-25",
    "categories": [],
    "contents": "\n\nd-article li{\n  margin-bottom: 0px;\n}\nI’ve written before on why grid-search is not enough (exibit A, exibit B). In this document, I’d like to run an experiment to demonstrate the strongest reason why I believe this is true.\nWe need to talk about, and learn to listen to, feedback.\nApplications over Models\nLet’s consider a recruitment application. We’ll pretend that it is for a typical job and that we are a recruitment agency that has experience in hiring for this position. This means that we have a dataset of characteristics of people who applied that includes an indicator if they ended up being a good fit for the job. The use-case is clear: we will make a classifier that will attempt to predict job performance based on the characteristics of the applicants.\n\n\n\nFigure 1: It starts out simple.\n\n\n\nAt this point, I could get a dataset, make a scikit-learn pipeline, and let the grids search. But instead, I would like to think one step further in the application layer. After all, we’re not going to be training this model once but many times. Over the lifetime of this application, we’ll gather new data, and we will retrain accordingly.\n\n\n\nFigure 2: But the model will retrain.\n\n\n\nFirst, the model is trained on the original dataset. Next, we will receive a new batch of (let’s say \\(c\\)) candidates. From this set, we will select the top (let’s say \\(n\\)) candidates who will be selected. It is from these selected candidates that we’ll get an actual label, and this will give us new data. This new data can be used to retrain the model again, such that the next round of candidates is predicted with even more accuracy.\n\n\n\nFigure 3: This model will update indefinately.\n\n\n\nThis cycle is going to repeat itself over and over.\nConcerns\nThe crux of all of this is that we can only see the actual label from the candidates who we give a job. The rest does not get the job, and we can never learn if they would have been good for the job.\nLet’s just list a few concerns here:\nif the starting dataset is biased then the predictions will be biased\nif the predictions are biased then the candidates who will be selected will be biased\nthese selected candidates are the only ones who feed new data back into the mechanism\nthis keeps the dataset where the algorithm can learn from biased\nYou might observe how the feedback mechanism is not, and cannot, be captured by the standard grid-search. Cross-validation does not protect against biased data.\n\n\n\nFigure 4: Which part of this system deserves more attention?\n\n\n\nIt might make sense to not focus on how we select a model (the red bits) but rather on how we collect data for it (the green bits). So I’ve set up an experiment to explore this.\nExperiment\nI’ve written up some code that simulates the system that’s described above. We start with a biased dataset and train a k-nearest-neighbor classifier for it. One trained, the model receives a set of 10 random candidates from the data we have not seen yet. Next, we select whichever candidates the algorithm deems the best performing. These will be the candidates that actually get a job offer, and these candidates will be able to give a label to us. The actual label of these candidates is logged, and the model is retrained. This is repeated over and over until we’ve exhausted the dataset.\nWe can then calculate lots of metrics. In general there are two datasets of interest. The first dataset contains all the candidates for which the system receives a label.\n\n\n\nFigure 5: These points have been seen by the algorithm\n\n\n\nThe second dataset contains all the candidates. For a subset of these candidates we receive a label so there is overlap with the previous dataset.\n\n\n\nFigure 6: These points judged by the algorithm\n\n\n\nThe goal of the experiment will be to see what the difference in performance is between these two datasets.\nObvious Dataset\nI’ll start with a dataset that has two clusters of candidates that have a positive label. In the plots below, you’ll see the entire (true) dataset, the subset that is taken as a biased start, the probability assignment by the algorithm and the greedily selected candidates from the models’ prediction.\nI’ve also listed the scores from two sources. One set is from all the data that our system actually observes. This set contains the biased dataset we started with and the labels of the selected candidates. The other set is from all the data, including the candidates that we have not seen.\n\n\n\nFigure 7: Plots for the easy dataset when selection 1/10 candidates\n\n\n\nSummary Table\nSource\nAccuracy\nPrecision\nRecall\nseen\n0.9423\n0.8928\n1.0000\nall\n0.7534\n0.4063\n0.2766\n\n\n\nFigure 8: Plots for the easy dataset when selection 2/10 candidates\n\n\n\nSummary Table\nSource\nAccuracy\nPrecision\nRecall\nseen\n0.9459\n0.9630\n0.8966\nall\n0.7721\n0.4615\n0.2553\nDiscussion\nNotice the striking difference between the entire dataset and the observed dataset! This example is a tad bit silly, though, because the dataset has two disconnected sets of groups. The odds of learning through this much bias is very slim. Will the same effect be observed when this isn’t the case? The next dataset will have a dataset where there’s one blob but still with a biased start.\n\n\n\nFigure 9: Plots for the easy dataset when selection 1/10 candidates\n\n\n\nSummary Table\nSource\nAccuracy\nPrecision\nRecall\nseen\n0.7821\n0.6969\n0.9583\nall\n0.7653\n0.5882\n0.3763\n\n\n\nFigure 10: Plots for the easy dataset when selection 2/10 candidates\n\n\n\nSummary Table\nSource\nAccuracy\nPrecision\nRecall\nseen\n0.8721\n0.8019\n0.9884\nall\n0.8023\n0.6975\n0.4462\nDiscussion\nTo be perfectly honest, the comparison I am making here is not 100% fair. The model is being scored on something different than what it is being applied to.\nBut maybe … this is exactly the point? In this example we run a grid-search and get an overly optimistic impression of how the model would perform in production. The gridsearch cannot guarantee robustness here because both the starting dataset and the feedback mechanism are biased. It also demonstrates how the algorithm might have great difficulty escaping a self fulfilling prophecy. The gridsearch is stuck in a local optima, giving back metrics suggesting to us it is doing very well.\nIt doesn’t really make much sense to use a gridsearch to squeeze another 1% performance out of the algorithm if this is well within the margin of application error. The feedback mechanism is an effect that is never captured by offline data. Therefore we cannot assume cross-validation to be a sufficient insurance policy against its bias. This is a no small issue. Just think of all the use-cases that are affected by this; recruitment, fraud, law enforcement, and just about everything in recommender-land.\nIt might be that the relationship between your offline data and your real-life phenomenon is a reliable one. I fear however that many usecases merely have a proxy that cannot be assumed to translate perfectly to your problem. It may still deserves to be looked at but, as the Dutch might say, “you can stare yourself blind at it”.\nTranslation\nWhen you apply grid search, you typically hope to squeeze another 1% of performance out of it. This makes sense but only if the problem is translated in such a way that the margin of translation error is smaller than 1%.\nLet’s think about it from a birds eye view, what do people in the data science industry really do?\n\n\n\nFigure 11: From PROBLEM to DATA PROBLEM back to PROBLEM\n\n\n\nTypically one starts with a problem in real life this is translated to a data problem. This way an algorithm can handle it and we might be able to automate something. Note that there’s a risk that some aspects of the problem are lost in translation there. The question then is; will the grid-search be able to compensate for this or is the grid-search maybe making the problem worse? You need to be careful here, if the translation doesn’t 100% match the real life problem … then improvement from the grid-search might very well be insignificant compared to the margin of error introducted by the translation.\nIt might be good to seriously wonder if we really want to spend so many resources performing the grid analysis? Unless the test sets in the gridsearch are perfectly representative of real life then the gridsearch will always give an optimistic view. Maybe … it’s more effective to focus on the translation part of the problem instead.\nAdapting the Feedback?\nSo what can we do with this knowledge? I have ideas but no solutions.\nWhen you design an algorithm, think hard about metrics and monitoring. What are all the things that can go wrong besides train/test validation and how can we keep an eye on that?\nThe experiments demonstrate that the amount of candidates we select has an influence on the actual performance. Especially when you are able to select more than one candidate, then you could consider spreading a bit. One candidate might be chosen with greed in mind while the other candidate is chosen with exploration in mind. The downside remains that it can take a long time before enough candidates have been explored to end up with a less biased machine learning algorithm.\nMy theory is that a lot of bias might be remedied by applying models that try to correct for fairness. This won’t be perfect but my life experience suggests that it is certainly worth a try.\nWe need to ensure that all possible sources of self-fulfilling prophecies are no longer in the data we receive as feedback. When users click a product more because it is recommended, then we cannot allow this to reinforce the recommender.\nWe could focus more on explainable models. This way, you’re no longer pushing a black box to production but rather an AB test that you understand. In reviewing results, this allows you to think more qualitatively about the results. This will help you understand the problem better which in turn will allow you to think more critically about potential translation issues.\nWe could consider that this problem is a bit like a bandit problem. Our models will be able to exploit a reward, but sometimes we might need to add some entropy to make sure that our system continues to explore. Entropy might be a good sanity check.\nWe should measure the difference between what we saw in our training phase and what we see in production. This difference is sometimes referred to as drift.\nEven in situations when we don’t always get \\(y\\), we do usually get \\(X\\). This suggest that we might be able to do some unsupervised tricks to quantify how many regions exist where we do not see any labels.\nWhen a user is subject to a decision that has been automated, they should have the right to know how said decision was made. This is a great boon too. This will allow them to give feedback to the systems designer. While quantitative feedback from the data can’t be trusted, the qualitative feedback from an end-user can offer clear scenarios for improvement.\nAll of these suggestions will depend a lot on the use-case. Common sense is still your best friend.\nFocus\nMost data science courses and books out there discuss the importance of doing (stochastic) grid search via cross-validation. This is undoubtedly a practice that has merit to it. Overfitting on a training dataset is a risk. Fair.\nBut if you’re going to apply the algorithm in the real world, then there are so many other concerns that you need to think about. Overfitting on a train set is a mere one concern. One! Putting the focus on doing just the grid-search is counterproductive because it gives a false sense of being “done” with the design of the algorithm while you’re just getting started.\nIt’s a general issue too. I’ve never really seen a dataset that isn’t biased in some way. My experiment here was even optimistic: I am assuming here the world does not change while a model is applied. In real life there’s also drift. Your training data will get stale and will reflect the new world less and less.\nConclusion\nGrid search is a good habbit but it is simply not enough. Please be aware that you may reduce the risk of overfitting on a training set, but you are not immune to everything else that might go wrong. Things like fairness, drift, artificial stupidity and feedback mechanisms deserve more attention then they are getting now.\nCode and Stuffs\nYou can find the notebook, python file, dataset1, dataset2 on this github gist.\n\n\n\n",
    "preview": "posts/inconvenient-feedback/overview3.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1190,
    "preview_height": 808
  },
  {
    "path": "posts/parallel-grid/",
    "title": "Parallel Grid",
    "description": "An Ode to Pipes, Seeds and Simplicity",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-11-16",
    "categories": [],
    "contents": "\nOne reason to use scikit-learn is to have access to a GridSearchCV object that can run tasks in parallel. But what about things that don’t fit the api? There’s keras/pytorch that needs custom code before it works nicely but there’s also tasks in fields like operations research/heuristics that won’t fit too swell.\nI was thinking about a nice way to abstract this and came to the conclusion that this is something you probably don’t want to solve with python. Instead you can solve this from the command line with parallel.\nUsecase\nThe reason I had this usecase was because I was wondering what the effect of a seed is when training a neural network. I don’t like the idea of having a seed as a hyperparameter but I don’t mind having a measure of risk. If the algorithm depends a lot on the seed then this would be a reason not to pick it.\nThe setup was simple: train a neural network that needs to learn to be the identity function. I decided to generate a dataset with \\(k\\) columns containing zeros and ones. The neural network would have \\(d\\) hidden layers that are fully connected and each layer has \\(k\\) nodes. Below is a drawing for \\(k=10\\) and \\(d=5\\).\n\nThe reason why these networks are interesting to train for is because they should theoretically be able to always achieve a loss function of zero. The network merely needs to learn the identity function so it only needs to propogate zeros and ones forward.\n\nStill, this is something that will have a random seed so I’ll need to run this many times. This got me wondering about what might be the most convenient approach.\nParellism\nFrom the terminal you can create a sequence appear.\n> seq 5\n1\n2\n3\n4\n5\nThis output can be piped into another command. In particular it can be piped to the parallel command. This command will run another command from the command line in parallel. Here’s the syntax.\nseq <numjobs> | parallel -j <threads> <command>\nFor example. Suppose that I have a job in a python file called main.py which will sample random grid parameters, run an experiment, and log the output to a file. Then this command will run that 1000 times while using 4 threads.\nseq 1000 | parallel -j 4 python main.py\nRandom Python Jobs\nThe main.py job can be described using randomness.\nres = build_train({'num_columns': random.choice([10, 11, 12]), \n                   'num_layers': random.choice([1, 2, 3, 4, 5]),\n                   'loss_func': 'binary_crossentropy', \n                   'tf_seed': random.randint(1, 4200),\n                   'epochs': 300, \n                   'rows': random.choice([2000, 3000, 4000])})\nwith open(f\"/tmp/gridlogs/{str(uuid.uuid4())[:14]}.jsonl\", \"w\") as f:\n    json.dump(res, f)\nWhen you run it with this:\n> seq 500 | parallel -j 6 python main.py\nThen you’ll run main.py 500 times while having running 6 jobs at a time. But you’re not just limited to random search.\nUsing Itertools\nYou could also make it deterministic. Take this jobfile;\n# capture.py\nimport sys\n\nif __name__ == \"__main__\":\n    print(f\"i am seeing this input: {sys.argv[1]}\")\nFrom python we can capture the that comes out of seq.\n> seq 5 | parallel -j 12 python capture.py\ni am seeing this input: 1\ni am seeing this input: 2\ni am seeing this input: 3\ni am seeing this input: 4\ni am seeing this input: 5\nThis also means that we can user an iterator to generate a grid and we can have the input determine which settings to select. Example below;\nimport sys\nimport time\nimport itertools as it \n\ndef mkgrid():\n    for param_a in range(8):\n        for param_b in range(8):\n            yield {\"param_a\": param_a, \"param_b\": param_b}\n\ngrid = mkgrid()\n\nif __name__ == \"__main__\":\n    settings = it.islice(grid, int(sys.argv[1]), int(sys.argv[1]) + 1)\n    time.sleep(1)\n    print(f\"these are the settings: {next(settings)}\")\nYou’ll need to tell seq to start at 0 and end at 63 but that’s about it. This job takes 8 seconds if you have enough cores:\n> seq 0 63 | parallel -j 8 python capture.py\nBenefits\nThis approach works pretty well on my mac. It comes with a few cores built in and it’s usually not doing anything during the evening. This approach generated a dataset with a million rows overnight. It might be enough for your jobs too without the need for a cloud vendor.\nAlso note that you can just log to a random file on a set path on disk. No locking of a single file between processes. There’s also no notebook and just a single main.py file which can still be unit tested.\nI also enjoy the feeling that I don’t need to worry about parallism from python. This is just something that the operating systems handles for me now from the command line. Win!\nResults\nThe code can be found in this gist and the main result is summerised in the next chart.\nThe chart shows the loss over the epochs during training. Each line is a seperate run. The grid shows the number of hidden layers (3, 4, 5) and the number of inputs (10, 11, 12). Notice that not few training procedures converge to zero loss even though this is certainly possible to achieve.\n\nWhen an algorithm depends so much on the seed, you start wondering if it’s a good idea to fully trust it. It should be said that this phenomenon mainly occurs for very deep neural networks not shallow ones and the example is a bit theoretical. Neural networks with just one hidden layers tend to converge immediately.\n\n\n\n",
    "preview": "posts/parallel-grid/network3.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1197,
    "preview_height": 623
  },
  {
    "path": "posts/goodheart-bad-metric/",
    "title": "Goodhart, Bad Metric",
    "description": "GridSearch is not Enough: Part Two.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-10-16",
    "categories": [],
    "contents": "\nThere are many ways to solve the wrong problem. A most iconic one is to fail to recognize Goodhart’s law;\n\nwhen a metric becomes a target to be optimized, it risks no longer being a useful metric\n\nThere are many anecdotes about this phenomenon on Wikipedia. The craziest one involves cobras during colonial India. The goal was to reduce the risk of getting poisoned by reducing the number of snakes.\n\nAs an incentive, the idea was to pay people to deliver dead cobras. This plan worked until people realized they could breed the snakes and make more money by doing so. Once the government realized this, they stopped paying for the cobras. This resulted in the corba breeders releasing the snakes into the wild. As a result, the program effectively increased the number of wild cobras. There’s a similar story about rats during French colonial rule in Vietnam.\nBut it’s not just the Wiki, the data-oriented enterprise is also getting a fair share of tales. I’ll list a few fables that cannot be confirmed or denied.\nFable 1: Bonus Time\nBack in the hey-day of data science, a large firm had an incentive program for their data scientists. If a scientist came up with an AB test that prooved statistically (1%) significant, then they would receive a hefty bonus. The goal was to promote data science and to incentivize great data scientists.\nThe irony was that this metric was easy to hack if you were just that. The clever ones immediately introduced 1000 random AB tests, and then the law of large numbers guaranteed their bonus for them. Once the bonus was “earned” the best thing to do is to leave the company (before anyone finds out what happened).\n\nYou’d be correct to assume that the bonus policy effectively caused the excellent data scientists (the ones that understood probability theory) to leave the company.\nFable 2: Suit Consultancy\nThere was an article in Gartner that discussed the importance of bounce rate when you are optimising your website for new users. This inspired a consultancy, we’ll call them SuitConsulting, to stress their client on the importance of this metric. They had meetings and made it clear to the analysts: they needed to find ways to increase the bounce rate.\n\nA junior analytical consultant from SuitConsulting came with an idea: how about we add a flashy banner before we let the user on the main page. The idea being that this new banner would set the mood for th new user. A new version of the page was made and the analyst started looking at the bounce rate; it had indeed improved! SuitConsulting made a presentation out of this, presented it to upper management, got paid and left with a new whitepaper demonstrating their expertise in web analytics.\n\nBy the time management noticed a drop in new users and sales, SuitConsulting was already gone. They hadn’t realised that by introducing the banner they were left with the heavy users allready familiar with the product. These users would not leave immediately but all the new users were turned off by requiring a new click.\nFable 3: Recommendations\nAt a video-streaming service, management was concerned that the recommender engine would not serve recommendations that people were interested in. Rightly so, the goal of the recommender here was to bring content to the users that would broaden their horizon. The streaming service wanted to be careful that they did not perform any click baiting.\nThis made the science team consider. Maybe a different cost-function would help. Maybe the algorithm should only get a rewardif the recommended video actually got watched for at least 75% of its total length. As the first version of this algorithm was pushed, the algorithm started scoring really well on this metric.\n\nA side effect of the algorithm was that the algorithm really started favoring videos that were easy to watch for at least 75% of its total length: 2-minute fragment videos. These videos were typically the videos that were designed to do click-baiting.\nPatterns\nThe goal of a metric should be limited to being a proxy since they are easily perverted and misinterpreted. They should not replace common sense or judgment.\nMetrics do make me wonder a bit about my own profession. When you do machine learning, even cross-validations, you typically optimize for a single parameter. Then I got reminded of an example that demonstrates why we should be worried.\nExample\nThe dataset below involves chickens. The use-case is to predict the growth per chicken such that you can determine the best diet and also predict the weight of the chicken. It’s a dataset that comes with the R language, but I’ve also made it available on scikit-lego. In the code below, you’ll see it being used in a grid-search.\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklego.datasets import load_chicken\n\ndf = load_chicken(give_pandas=True)\nX, y = df[['time', 'diet']], df['weight']\n\nmod = GridSearchCV(estimator=GradientBoostingRegressor(), \n                   iid=True,\n                   cv=10,\n                   n_jobs=-1,\n                   param_grid={'n_estimators': [10, 50, 100], \n                               'max_depth': [3, 5, 7]})\n\nmod.fit(X, y)\nThis model is cross-validated, and the predictions of the best model are shown below. We can see that there might be a preference for diets.\n\nThis model has been selected because it is the best at predicting the chickens’ weight. We could even extend the grid-search by looking for more types of models. The problem is that it does not matter since we’re dealing with a vanity metric. Let’s consider the chart of all the growing paths for all the chickens.\n\nThere’s a couple of paths that indicate that some chickens died prematurely. If we recognize that our model has no way of capturing this, then we should also acknowledge how the grid-search is a dangerous distraction. It distracts us from understanding our data. It may even be the case that the diet with the best mean growth is also the diet with the most premature deaths.\n\nConclusion\nThere’s danger in the act of machine learning.\nYou might make the argument that some of my examples merely demonstrate people putting faith in a faulty metric. There’s sense in that. Not all metrics are super harmful. That said, it is the act of optimizing religiously that is a natural effect if you introduce a metric. This is what so dangerous about them.\nGiving it to an algorithm will only make it worse. The algorithm will suggest that you need to focus on tuning instead of making you wonder if you’re solving the right problem.\nLet’s be frank. If we’re dealing with a vanity metric, then an algorithm sure is a great way to hide it. A grid search is not enough to protect you against this.\n\n\n\n",
    "preview": "posts/goodheart-bad-metric/logo.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1007,
    "preview_height": 442
  },
  {
    "path": "posts/little-victories-scale/",
    "title": "Little Victories Scale",
    "description": "Thoughts on automating automation itself.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-10-08",
    "categories": [],
    "contents": "\nThe most dependable method of improvement is reflection. If you can identify something that gives you a hassle, then you should give yourself the freedom to do something about it. If you’re a developer, a lot of this boils down to the question, “what can I automate this week?”. Usually, at least for me, this notion of automation results in something that I can do from the command line. It might be investing in a better habit, a new way of working, or something that “automates the boring stuff.”\n\nI figured it’d nice to demonstrate an example of something I did a while ago that made the whole “python and virtualenv” relationship a whole lot easier for me. It’s just an example but one that made my life a whole lot easier.\nBehold Thy Terminal\nI use a mac and a raspberry for my development. Both offer a terminal which you can fully customise. You can do this by editing a file called .bash_profile (or if you’re into zsh it’s .zshrc). These files exist in the home directory can you can usually see the contents by typing cat .bash_profile. This is a file that is run every time you open a terminal. You can add settings here, but also functions and links to apps you find useful. Here’s a snippet from my .zshrc.\nfunction mkvirtualenv() {\n  virtualenv -p $(which python3.6) venv\n  source venv/bin/activate\n}\n\nfunction pysrc() {\n  source venv/bin/activate\n}\n\nexport PIP_REQUIRE_VIRTUALENV=true\n\ngpip() {\n    PIP_REQUIRE_VIRTUALENV=\"\" pip \"$@\"\n}\nThere’s a few things that happen in this block:\nmkvirtualenv is now a function that I can call that creates a virtualenv in my local folder called venv, once this is created the virtualenv is also activated\npysrc is now a function that I can call that actives the virtualenv that is in the current folder\nthe PIP_REQUIRE_VIRTUALENV is an environemnt variable which is recognized by pip which ensures that I don’t accidentally pip install anything in my global environment (this was the biggest source of errors)\nshould I actually want to install anything in my global environment then I can use gpip install\nA nice thing about these settings is that these settings encourage me to adhere to the good habit of making virtualenv early and often. It also automatically prevents any accidentaly intalls into my global environment that might cause confusion in my local environment. I’ve noticed too often that tools like pytest or jupyter were looking for the globally scoped instances which caused all sorts of dependency confusion across environments.\nWith this change to my terminal the number of virtualenv errors that I experience is now zero.\nHere’s an asciinema that gives a demonstration.\nYour terminal might look a bit difference since I am using zsh but the commands should work in the same way. Note that a bash profile can be extended with other amazing things that are useful too.\nThis line of code ensures that I can quickly open and edit a file with sublime text in my local directory by typing subl filename.txt.\nln -s \"/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl\" /usr/local/bin/subl\nNot reflecting on what you do in your day to day is a dangerous thing; it slows down automation and enjoyment of work. Figuring these settings out took maybe an hour to get right but I’ve earned 100x the investment by no longer being haunted by virtualenv issues.\nConclusion\n\nThe interesting thing here is that people like to make a habit out of automation but fail to recognize the reflection that is needed beforehand. If you’re going to invest into any habit here, focus on the reflection part. It is the hardest to get right but you cannot automate automation without it.\nMore Developer Tricks\nThere are loads of habbits that you can introduce to your workflow that might make you more productive and more happy. There are three paid apps that I can highly recommend here too (if you’re using a mac):\nsnippetslab: it’s like your own private stackoverflow that you curate yourself, you make a small database of snippets that you often use and instead of finding the same stackoverflow questions over and over you can just copy and paste a best practice you’ve previously discovered. Imagine all the started templates and config files that you can put in there.\nalfred: it prevents me from using the trackpad/mouse by allowing me to write a shortcut for just about everything. It’s great.\ndeckset: I give a lot of presentations and being able to have them as a markdown file feels much better to me. My presentations live in git and I can very easily copy and paste parts. Also: good latex support.\n\n\n\n",
    "preview": "posts/little-victories-scale/img1.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1782,
    "preview_height": 1072
  },
  {
    "path": "posts/outliers-selection-vs-detection/",
    "title": "Outliers: Selection vs. Detection",
    "description": "Algorithms can detect outliers, but how do you select algorithms?",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-08-07",
    "categories": [],
    "contents": "\nA common method for detecting fraud is to look for outliers in data. It’s a fair approach: even if the detection doesn’t immediately imply fraud it can be a good candidate for further investigation. Still, how might we go about selecting hyper-parameters (or even the algorithm)? The hard part is that we have very little to go on. Just like clustering there’s no label. It is incredibly though to argue if a certain model is appropriate for a use-case.\n\nLuckily there’s a small trick that can help. How about we try to find outliers that simply correlate with fraudulent cases? It might be surprise to find out that scikit learn has support for this but it occurs via a slightly unusual pattern.\nSetup\nI will demonstrate an approach using this dataset from kaggle. It’s an unbalanced dataset meant for a fraud usecase.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n\ndf = pd.read_csv(\"creditcard.csv\").rename(str.lower, axis=1)\nX, y = df.drop(columns=[\"class\", \"time\", \"amount\"]), df['class']\nWith the dataset loaded I’ll run an IsolationForest. Note that I am not labelling, I am merely looking for outliers.\nfrom sklearn.ensemble import IsolationForest\nforest = IsolationForest(contamination=0.1, behaviour=\"new\").fit(X)\nWe can look at the algorithm results but we’re mostly interested in finding a good value for the contamination parameter. One thing you could do manually is to calculate, say, the precision of the predictions.\nfrom sklearn.metrics import precision_score, recall_score\nconverted = np.where(forest.predict(X) == 1, 0, 1)\nprecision_score(y, converted)\nNote that we’re using np.where here because an outlier detector in scikit learn will output either -1 or +1 while the fraud label will be 0 or 1.\nThe Problem\nWe could now go and write a for-loop to consider all the values but this is a lazy hack. It is much more preferable to cross validate the hyperparamter in a gridsearch. You might be wondering how to write a gridsearch that might facilitate this though. After all, we need to manually convert the models output to something the precision score can use and we need to figure out a way to allow our y and X values to also be cross validated. Also, generally, scikit learn has a pattern of using sklearn.metrics.make_scorer that accepts functions of signature score(y_true, y_pred). So how on earth are we going to get this to work?\nThe main trick is to recognise two things:\nScikit Learn will also accept a metric function with the signature score(model, X, y) and if you write a function this way you don’t need make_scorer.\nScikit Learn models that have a signature of .fix(X) can also accept .fit(X, y). In this case the y value is ignored by the model but can be used for any other part of the pipeline. This includes metrics.\nThese two facts combined give us a nice pattern:\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.model_selection import GridSearchCV\n\ndf_subset = df.sort_values('class', ascending=False)[:80000]\nX_subset = df_subset.drop(columns=['amount', 'class', 'time']).values\ny_subset = df_subset['class']\n\ndef outlier_precision(mod, X, y):\n    preds = mod.predict(X)\n    return precision_score(y, np.where(preds == 1, 0, 1))\n\ndef outlier_recall(mod, X, y):\n    preds = mod.predict(X)\n    return recall_score(y, np.where(preds == 1, 0, 1))\n\nforest = IsolationForest(contamination=0.1, behaviour=\"new\", max_features=0.2)\n\nmod = GridSearchCV(estimator=forest, \n                   cv=5,\n                   n_jobs=-1,\n                   scoring={\"precision\": outlier_precision,\n                            \"recall\": outlier_recall}, \n                   refit=\"precision\",\n                   param_grid={'contamination': np.linspace(0.0001, 0.02, 30)})\n\nmod.fit(X, y)\nThis gridsearch takes a while but afterwards we can visualise the following effect:\n\nThe pattern works! Note that in this dataset there are 492 positive cases out of 284807. It’s pretty interesting to see that we can get near 10% precision/recall with just an outlier detection algorithm considering that the base rate should be near 0.17%.\nDiscussion\nThis is a useful pattern but I would warn against using this as a “one size fits all” approach. A better approach detecting outliers is to have multiple detectors that each focus on a different part. The pattern that is explained here is a great way to generate such a candidate but it should not be the only thing that is detecting.\nThink about it. Suppose that you are able to come up with 100 valid but alternative methods of detecting if something is “out of the ordinary” then it becomes a lot easier allocate the expensive investigative resources. When suspicion overlaps the odds of finding something that deserves discovery increases.\nThink about your case:\nIs there unexpected behavior on an account on the short term/long term?\nactivity within a session\nthe number of sessions increases within a day\nthe number of chat messages being sent/received\nIs there unexpected behavior on a product on the short/long term?\nthe returns of a product suddenly spikes\nthe description of a product changes very frequently\nthe stock of a product suddenly flucuate immensely\nWhen lots of small systems detect something that deserves attention it becomes a effective proxy to determine on how to spend your investigative resources. If you’re interested in this field of thinking in the fraud space you might be interested in this pydata talk from Eddie Bell.\n\n\n\n",
    "preview": "posts/outliers-selection-vs-detection/goal.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1206,
    "preview_height": 480
  },
  {
    "path": "posts/foresight-for-predictions/",
    "title": "Foresight for Predictions",
    "description": "Predictions without Foresight can be Anticipated to become Dreadful.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-07-04",
    "categories": [],
    "contents": "\nForesight for Predictions\nUsually the application of a machine learning algorithm fits in these steps;\n\nMake a model predict\nUse this prediction to make a better decision\nThis better decision changes the world\nThe world gives more profit\nAn algoritmic system will be of very little use to anyone if you only have a prediction. You need to act on it if you want to experience an improvement of any sort.\nThis is where it gets interesting though because there is a causal effect in here.\n\nTwo things can happen.\nThing A: Something Realistic\nBecause the world changes your model shouldn’t be able to predict it as well anymore. This is partially because the world is bound to change on it’s own. More likely this change is caused by the predictions you’ve made.\nThink about it. The predictions came from a time where we did not have an algorithm aiding in decision making and the new decisions being made will contribute to inaccuracy. The effect of the new decisions is something that is new to the algorithm and you should update the model such that it can understand these effects.\nThing B: Something Unfortunate\nIt might be that your model starts performing better. This situation is very dangerous because it is most likely caused by a feedback loop causing a self fulfilling prophecy.\nThink about it. What other reason could there be that the performance of the model goes up over time? It shouldn’t. In the realm of recommenders this can be especially painful. By recommending certain content people will click it more which can cause the algorithm to wrongfully think that it should recommend it even more.\nConclusion\nBefore designing a system that predicts it might be a good idea to attempt some foresight. You will need to deal with effects like this some day and it is better to think about these effects early.\n\n\n\n",
    "preview": "posts/foresight-for-predictions/forsight3.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1038,
    "preview_height": 444
  },
  {
    "path": "posts/bingo-ball-pit/",
    "title": "Bingo Ball Pit",
    "description": "An Exercise in Systemic Counting",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-06-25",
    "categories": [],
    "contents": "\nI heard a story about an UnnamedCasino[tm] has been attempting to calculate the probability of winning in one of their proposed lotteries. They were so occopied with this problem that they hired UnnamedConsultancy[tm] to deal with the problem. Turns out that even these unnamed, albeit suit wearing, consultany folks have trouble with the maths, despite their suits, and resorted to simulation.\nSimulation certainly isn’t a bad idea (if only as a checking mechanism) but after some a discussion over beers I came to the conclusion that you might also be able to calculate the probability by systematic counting.\nThe exercise is a bit theoretical in nature but I hope it serves as a fun example for practitioners (or suit wearing consultants) who don’t mind reading an example of harder counting problems.\nThe Problem\n\nThe casino has a lottery like bingo. People get a sheet with 25 random numbers between 1-90 on it. Next, the lottery man will draw 25 balls at random. You win if the numbers on your sheet are pulled.\n\nIf nobody in the room has won then that’s fine, they will draw a new ball.\n\nThey will keep drawing balls until there actually is a winner.\n\nThe more balls are being pulled the larger the probability that multiple people win at the same time. The casino people (remember, these are the people in the casino that are risk averse) would like to know the odds of two or more people winning at the same time.\nThinking About It\nThis problem can get way out of hand if you aren’t structuring it. Let us first worry about the order of the balls and the number of possible combinations, we’ll worry about the number of people later.\n\nThere are \\(90! = 1 \\times 2 \\times ... \\times 89 \\times 90\\) different combinations possible for the first 90 balls.\nAfter 25 balls\nSuppose now that 25 balls have been pulled. It is possible that I have a fully marked sheet after the first 25 balls have been pulled. The first 25 balls can be in any order and the latter 65 balls cal also be in any order.\n\nThis image helps me come to this formula:\n\\[ \\mathbf{Prob}(\\text{hit after 25 balls}) =\n\\frac{\\mathbf{Count}(\\text{hits})\\mathbf{Count}(\\text{misses})}{\\mathbf{Count}(\\text{total})} = \\frac{25! \\times 65!}{90!} \\]\nIn this case the formula was pretty easy to derive. Let’s check if we can still do that when the number of balls increase.\nAfter 26 balls\nIf I don’t have a hit then it is possible that the next hit will be. This is only possible if out of the previous 25 balls one was a miss. This means we have to draw the picture a bit differently.\n\nSince the picture is different the formula also needs to be adapted.\n\\[ \\mathbf{Prob}(\\text{hit after 26 balls}) =\n\\frac{\\mathbf{Count}(\\text{hit 24 out of 25, #26 is hit, other 64 any order})}{\\mathbf{Count}(\\text{grab 26 from 90})} \\]\nBefore we worry about the details, let’s check if the same pattern emerges when we increase the number of balls.\nAfter 27 balls\nThe same argument as before can be made!\n\n\\[ \\mathbf{Prob}(\\text{hit after 27 balls}) =\n\\frac{\\mathbf{Count}(\\text{hit 24 out of 26, #27 is hit, other 63 any order})}{\\mathbf{Count}(\\text{grab 27 from 90})} \\]\nWith this drawing I feel like we might be able to turn this into something general.\nAfter \\(k \\geq 25\\) balls\n\nLet’s look for something general.\n\\[ \\mathbf{Prob}(\\text{hit after } k \\text{  balls}) =\n\\frac{\\mathbf{Count}(\\text{hit 24 out of } (k-1), k \\text{ is hit, other } (90 - k) \\text{ any order})}{\\mathbf{Count}(\\text{grab } k \\text{ from 90})} \\]\nTurns out that this formula can be translated into a variant of the hypergeometric distribution. But beware the slight twist! When we draw, say, 28 balls then I know that in the first 27 balls we did not get 25.\nIt’s allmost hypergeometric though. If I ignore the final ball \\(k\\) that causes the bingo then it is fully geometric. This suggests that if I just keep in mind that the probability of getting this ball is \\(\\frac{1}{90-k}\\). These two facts can be glued together to get the probability density that we’re interested in.\n\\[ \\mathbf{Prob}(\\text{hit after } k \\text{ balls}) = \\frac{{25 \\choose 24} {65 \\choose k - 24}}{{90 \\choose k}} \\times \\frac{1}{90 - k}\\]\nWe can also write this up in python to confirm that we have a distribution that sums to one.\nfrom scipy.special import comb \n\ndef prob(b=25):\n    return comb(25, 24) * comb(65, b-24) / comb(90, b)\n\nsum([prob(i)/(90-i) for i in range(25, 90)]) # equals 1 \nAgain, the trick here lies in understand that if I pull 50 balls that we must account for the fact that there were 24 hits in the 49 before. No more, no less. If I was not able to state this then our formula would be false.\nFinal Note\nIt might just be me, but maybe if you want to solve these sorts of problems it helps to make drawings instead of formulas. It is easier for other people to follow along but it is also easier to understand/explore the problem that you’re facing.\n\n\n\n",
    "preview": "posts/bingo-ball-pit/bingo1.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1402,
    "preview_height": 588
  },
  {
    "path": "posts/high-on-probability-low-on-certainty/",
    "title": "High on Probability, Low on Confidence",
    "description": "GridSearch is not Enough: Part One.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-05-22",
    "categories": [],
    "contents": "\nMachine Learning models don’t just classify, they also approximate probabilities. Notice that the most important word in the previous sentence is written in bold. Approximations of probabilities can be very useful but one needs to be very careful. They should not directly be interpreted as an actual degree of certainty.\nThe goal of this document is convince you of this and to hint on how to solve this issue.\nExample Dataset\nTo demonstrate this effect I will train a KNeighborsClassifier on a toy dataset that is generated by the scikit-learn api. The code for this is listed below as well as an image of the distribution of classes.\nX, y = make_blobs(n_samples=1000, center_box=(-2, 2), random_state=42)\n\nThis is a dataset that is relatively easy to train and fit.\nmod = KNeighborsClassifier(20, weights=\"distance\").fit(X, y)\nI’ve plotted the output of this model below. Areas where the algorithm has a probability value larger than 0.9 are highlighted.\n\nNotice that there is a lot of areas where the algorithm gives a very high probability but where the algorithm has never seen any data. This is, at least to me, feels very awkward. The algorithm suggests to give certainty to regions where it hasn’t seen any data.\nTo emphesize this crazyness, let’s zoom out a bit.\n\nThe area where the data actually resides in is smaller than the area where the algorithm suggests to be able to predict with certainty.\nGeneral Issue\nOne might think that this issue is caused by of our choice in algorithm. This is not true. To demonstrate this, consider the following results from other algorithms.\n\n\n\nThis issue, it seems, is general.\nGeneral Problem\nWe should remember that the output from .predict_proba() is an approximation of a probability. It might be a proxy for probability but this is different from being a direct link to certainty. This is because machine learning models are often designed for interpolation, not extrapolation.\nIt would be less of an issue if machine learning models weren’t designed to always give a result. There is usually no mechanism that assigns doubt to an outcome if a decision is made outside of a comfort zone.\nLucky for us, this last part of the problem can be fixed with a little bit of probability glue.\nThe Density Trick\nWhat if we used an outlier detection algorithm to detect if a point we’re trying to predict is outside of our comfort zone? One way of getting there is to use a Gaussian Mixture Model on our original dataset \\(X\\). We can measure what areas have likely values in \\(X\\) and with that we can say what area’s aren’t. This can be easily implemented thanks for scikit learn.\nX, y = make_blobs(n_samples=1000, center_box=(-2, 2), random_state=42)\nmod = KNeighborsClassifier(20, weights=\"distance\").fit(X, y)\nout = GaussianMixture(3).fit(X)\n\nboundary = np.quantile(out.score_samples(X), 0.01)\nsimdf = (pd.DataFrame({\"x1\": np.random.uniform(-6, 6, n), \n                       \"x2\": np.random.uniform(-6, 6, n)})\n         .assign(pred = lambda d: mod.predict(d.values).astype(np.object))\n         .assign(prob = lambda d: out.score_samples(d[['x1', 'x2']].values) > boundary))\nWhen we get new values then this learned boundary, or threshold, can be used to detect the region where we might be comfortable with interpolation. This result looks promising.\n\nWe seem to skip a few outliers but we can now design the system to only make a prediction when the point to predict is within the comfort zone. If it is outside of it, it can simply “not make a prediction”.\nConclusion\nPersonally, I’m charmed by the idea of designing a predictive system that can say “Nope! Not predicting this one!”. It sounds like a very sane way of dealing with uncertainty. Using an outlier algorithm as a cog in this system makes sense too. I personally like the idea of using a GMM for this because having a probability distribution as an output gives nice flexibility.\n\nI wouldn’t want to suggest that this idea is a free lunch though. Getting a GMM to fit your dataset well is still tricky business. Setting the right threshold isn’t much fun either.\nSources\nI’ve implemented this GMM outlier detection algorithm in scikit-lego. Feel free to use it.\nIf you want to play with the code used in this notebook, you can do so here.\n\n\n\n",
    "preview": "posts/high-on-probability-low-on-certainty/fallback.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1398,
    "preview_height": 502
  },
  {
    "path": "posts/some-geometric-algorithms/",
    "title": "Some Geometric Algorithms",
    "description": "And Something Obvious about Albert Heijn.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-05-19",
    "categories": [],
    "contents": "\nIn Amsterdam there’s so many Albert Heijn stores there’s a risk of tripping over them. This made me wonder: where do you need to live in Amsterdam if you want to live as far away from an Albert Heijn store as possible? To answer this question, I needed to write some geometric algorithms.\n\n\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addPolygons\",\"args\":[[[[{\"lng\":[4.88788604736328,4.94316101074219,4.97474670410156,4.96341705322266,5.01285552978516,4.95346069335938,4.91294860839844,4.89509582519531,4.89200592041016,4.88376617431641,4.87106323242188,4.84050750732422,4.83776092529297,4.81029510498047,4.82471466064453,4.83295440673828,4.84394073486328,4.84153747558594,4.85424041748047,4.85458374023438,4.84600067138672,4.81338500976562,4.79038238525391,4.76222991943359,4.76497650146484,4.76154327392578,4.78248596191406,4.81647491455078,4.78935241699219,4.78832244873047,4.76840972900391,4.76943969726562,4.84600067138672,4.88101959228516,4.88788604736328],\"lat\":[52.4248259616028,52.4059796392558,52.3823056287079,52.3510700404778,52.3032305389395,52.2780310720211,52.3284156725276,52.3338705625792,52.3219108859477,52.3208616372553,52.2769807831312,52.2776109594519,52.2727793786673,52.2771908429004,52.2906325973518,52.2948323093878,52.2921025418582,52.3175038742432,52.3170841359525,52.334709716727,52.3370173085391,52.3374368577509,52.3271567562141,52.3410028653502,52.3462464717862,52.348972901371,52.3640700823172,52.3714071268825,52.3737128034988,52.3831438635978,52.383562975075,52.3971819323429,52.3990673029333,52.4252446772889,52.4248259616028]}]]],null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2,\"smoothFactor\":1,\"noClip\":false},null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[52.2727793786673,52.4252446772889],\"lng\":[4.76154327392578,5.01285552978516]}},\"evals\":[],\"jsHooks\":[]}\n\nIt’s not exactly perfect, but this is my definition of Amsterdam.\nThe Problem\nSuppose that I have a list of all the latitude/longitude coordinates of every Albert Heijn store. Then I still need to figure out which ones are located in Amsterdam. I’ll also need a way to randomly sample random house adresses that are inside of a polygon and not outside of it. In essense, I need an algorithm that can detect if a dot is in the blue area below.\n\nIt is easy for a human to look at the drawing and to figure out that two points are inside of the polygon and one lies outside of it. For an algorithm this is less obvious because all I have are the ordered coordinates of the polygon. There’s an algorithm to solve this though and it relies on a few insights from geometry.\nyou can come up with a line (a “ray”) that goes through the shop of interest\nthis ray must at some point enter and leave the polygon if it is inside of it\na ray will enter or leave a polygon if the ray intersects with a line segment from the polygon\nif the polygon is not convex then this property still holds but you might enter/leave the polygon multiple times\n\nThis insight is helpful but we’ve now gotten a new problem: how can we find out if two line segments intersect given a set of coordinates? Turns out that there’s a geometric trick for this as well.\nSuppose that there are four points that span two lines \\(L_1 = (p_1, p_2)\\) and \\(L_2 = (p_3, p_4)\\). I could take three of the points, say \\(p_i, p_j, p_k\\), and check the direction in which the angle is turning.\n\nI could do this for a bunch of combinations of the points. Say, these;\n\nNote that we can demonstrate a non-intersect too.\n\nNotice how the directions \\(d_1\\) vs. \\(d_2\\) are opposites just like \\(d_3\\) vs. \\(d_4\\) when the lines intersect. Also notice how they aren’t when they don’t.\nCode\nWhen I code in my spare time I try to optimise for joy. It’s not that numpy isn’t joyful but in this particular case a custom class makes a whole lot of sense.\n\nclass Point:\n    def __init__(self, lat, lng):\n        self.lat = lat\n        self.lng = lng\n\n    def __add__(self, other):\n        return Point(self.lat + other.lat, self.lng + other.lng)\n\n    def __sub__(self, other):\n        return Point(self.lat - other.lat, self.lng - other.lng)\n\n    def __mul__(self, other):\n        \"\"\"crossproduct!\"\"\"\n        return self.lat * other.lng - self.lng * other.lat\n\n    def __repr__(self):\n        return f\"<Point({self.lat}, {self.lng} at 0x{id(self)}>\"\nThis custom class makes it easier for the python code to look like actual mathematics. Moreover, it is way more fun to program this way.\n\ndef direction(p0, p1, p2):\n    return (p1 - p0) * (p2 - p0) > 0\n\n\ndef intersect(p1, p2, p3, p4):\n    d1 = direction(p3, p4, p1)\n    d2 = direction(p3, p4, p2)\n    d3 = direction(p1, p2, p3)\n    d4 = direction(p1, p2, p4)\n    if (d1 != d2) and (d3 != d4):\n        return True\n    return False\nAlbert Heijn in Amsterdam\nWith this algorithm in place I was able to select all the shops that are in my definition of Amsterdam. As you can see, the centre of Amsterdam is quite blue.\n\n\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addMarkers\",\"args\":[[52.32894,52.356908,52.362485,52.366797,52.417387,52.353832,52.318288,52.361176,52.35788,52.344779,52.402418,52.372992,52.342071,52.339181,52.323447,52.397455,52.352965,52.357642,52.375238,52.390454,52.369956,52.359827,52.393268,52.377683,52.345864,52.34061,52.361042,52.378131,52.363005,52.355857,52.322636,52.349134,52.307481,52.363747,52.372834,52.303557,52.35254,52.374026,52.347798,52.366724,52.381849,52.385078,52.383744,52.371379,52.312041,52.338647,52.369431,52.362463,52.354842,52.336126,52.315777,52.385672,52.362529,52.380398,52.288869,52.380056,52.294355,52.370895,52.374104,52.351195,52.369981,52.378614,52.362763,52.378576,52.379214,52.343252,52.364266,52.357381,52.281185,52.374979,52.352315,52.346837,52.345935,52.367392,52.31199,52.389844,52.356821,52.349894,52.281784,52.335479,52.381848,52.354645,52.374208,52.359839,52.35072,52.316179,52.378594,52.374749,52.359139,52.35502,52.366551,52.348961,52.289291,52.364591,52.295163,52.375954,52.331297,52.378372,52.357184,52.392066,52.363112,52.344417,52.361964,52.346041,52.391633],[4.952942,4.928004,4.939544,4.893765,4.892918,4.869397,4.865719,4.899779,4.895567,4.856988,4.932468,4.890011,4.96287,4.873016,4.935757,4.939774,4.902344,4.833389,4.872966,4.915024,4.919778,4.891589,4.867029,4.801285,4.811772,4.871195,4.908373,4.90041,4.874737,4.910694,4.972053,4.839996,4.845837,4.83779,4.89944,4.859719,4.855586,4.883685,4.904778,4.870157,4.855705,4.903564,4.875805,4.85796,4.947501,4.77753,4.902561,4.913075,4.919952,4.87149,4.954152,4.88353,4.925156,4.82372,4.839462,4.899697,4.957171,4.8703,4.89364,4.891634,4.881528,4.845948,4.891887,4.900253,4.898133,4.90662,4.855049,4.940643,4.851192,4.861059,4.798552,4.918422,4.894948,4.901857,4.87443,4.837129,4.912217,4.939671,4.826487,4.859922,4.888516,4.890167,4.937899,4.861943,4.853852,4.978609,4.884162,4.893847,4.802557,4.826243,4.924116,4.877658,4.869137,4.883225,4.95765,4.904786,4.876907,4.896219,4.879663,4.955082,4.931771,4.891211,4.864373,4.863383,4.87755],null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},null,null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[52.281185,52.417387],\"lng\":[4.77753,4.978609]}},\"evals\":[],\"jsHooks\":[]}\nI wanted to know where you needed to live in Amsterdam if you wanted to be as far away from one of these shops as possible. The results of my genetic algorithm are shown below.\n\n\n\nThe result is really uninspiring. If you want to live far away your best bet is to live either: in a public park, in an industry park or in the river ’ij.\n\n\n",
    "preview": "posts/some-geometric-algorithms/some-geometric-algorithms_files/polygon-0.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 540,
    "preview_height": 329
  },
  {
    "path": "posts/the-future-is-past/",
    "title": "The Future of Data Science is Past",
    "description": "And it's not just because that's whats *actually* being predicted.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-05-14",
    "categories": [],
    "contents": "\n\nAfter some reflection with friends and colleagues I can’t help but observe an odd fact:\nthe Future of Data Science is Past\nThis isn’t because data scientists try to predict the past (which granted, is pretty funny and ironic). It is also because hype and other circumstances have turned data science into a field of inflated expectations. It feels more like a certain type of dream got pushed (even sold) that didn’t solve all the problems that people were hoping for.\nAfter I started hearing that a lot of data projects fail (here’s a nice example on reddit) I started thinking about what was going on. Five years ago the future was so bright that some of us may have been blinded by it. In short, the hype around algorithms forgot to mention that:\nalgorithms are merely cogs in a system\nalgorithms don’t understand problems\nalgorithms aren’t creative solutions\nalgorithms live in a mess\nIn this document I’d like to address these points one by one so that I have something to refer to. I realise that I sound a bit cynical but I’ll end the blog with some hope.\nAlgorithms are Merely Cogs in a System\nI was once put in a team to design a recommendation service for the Dutch BBC. One of the first things we did was make a drawing of what the service should look like. It looked something like this.\n\nThere were different parts of this design:\nThe UI layer\nThe AB splitting mechanism - we need this is we ever want to compare two algorithms.\nThe database that caches recommendations - we need this because at scale we cannot calculate these in real time\nThe scheduler that triggers a cluster to make calculations - we want to incorporate new data on a daily/hourly basis\nAn algorithm that calculates new recommendations - this is the data science code that takes log files and produces recommendations\nThe fallback/checker mechanism that ensures we only recommend things we’re allowed to recommend.\nOut of all these parts the final part (#6) is the most important in this system. It is the part that guarantees that we always have something to recommend even if any part before breaks. It is also the part that ensures that we always adhere to some very important business rules. Business rules like;\nmake sure that we don’t violate a license (some content may only shown during certain parts of the day)\nmake sure that we don’t show mature content before 22:00\nmake sure we don’t recommend violent shows after a kids show\nIt is unrealistic that a self learning algorithm is able to pick up these rules as hard constraints so it makes sense to handle this elsewhere in the system. With that in mind; notice how the algorithm is only a small cog in the entire system. The jupyter notebook that contains the recommender algorithm is not the investment that needs to be made by the team but rather it is all the stuff around the algorithm that requires practically all the work. Note that step 5 also requires communication with a big data cluster which also demonstrates that a recommender is more of an engineering investment than an algorithmic one. You don’t want a pure scientist team here, you need solid engineers and developers.\nThis is besides the fact that a lot of good algorithms can be created with simple heuristics that don’t require a maths degree. One could:\nrecommend content that is often watched together\nrecommend content that is often watched together unless it is too popular\nrecommend the next episode\nThese are all fair options to at least benchmark. I would also like to point out that the latter idea is a great idea even it isn’t technically machine learning. It should compete with any deep learning algorithm, any day.\nAlgorithms don’t Understand Problems\nIt takes a team a lot of time to realise what problem it is actually solving. Getting this right is hard. It’s even harder when the majority of the team consists of the same people. Worse; these people are trained in machine learning and prefer to keep themselves to the algorithmic part of their work.\n\nThing can go wrong in this situation. The team might not be great at translating the actual problem into an algorithm that represents the business. At the same time, the business might not be able to adapt to a recommendation that comes out of the algorithm. Even worse, it is also possible that the algorithmic solution brings in new problems. If you’re unlucky these new problems can get translated into new algorithmic problems. Before you know it you’re stuck in a loop.\n\nPeople with an algorithmic background usually tackle a problem in isolation of the system that it is in. A better approach is not to focus on the isolated part but to focus on the communication between two parts of the system. If this is not part of the problem design from the get-go then you’re bound to waste resources until you realise it has to.\nLet’s remember the recommender usecase. It is a bad idea to expect the algorithm to magically understand that it cannot recommend mature content before 22:00. When you realise this you know that you need a checking system between the algorithm and the serving layer. It is good to realise this early in the project and not when a large chunk of the code needs to get rewritten.\nOne potential cure against this “isolation” way of thinking is to not have a team with data people. Instead you want to have teams that are interdisciplinary and maybe have a data person in it. A team with many backgrounds causes more discussion and a more holistic view of things that “might go wrong”. It makes sense to have a UI specialist, a backend engineer, an algorithm person and a content expert in a team that is making a recommender.\n\nIt is really hard to think outside the box if your in it. By having multiple disciplines you ensure that there is always somebody who can cause a pivot. There’s always somebody who thinks outside of the box that you’re in.\nAlgorithms aren’t Creative Solutions\nCreative solutions to problems are usually not found by hyper-parameter search. The goal should not be to be the most efficient algorithm but the most effective. Do data teams ever consider that maybe the solution to the problem isn’t a machine learning model? Why would the algorithm ever be a good starting point to solving a problem?\nTo drive this point I’m reminded of a story in the eighties about a condom brand. The business was doing well but it was having some trouble planning production. It turned out to be very hard to predict demand. Both the factory and the warehouse are expensive so people were wondering what to do about it.\n\nThen a strategic consultancy firm came in and suggested that this was indeed a StatisticsProblem[tm]. The consultancy firm advised getting another third party involved to try to predict the demand curve. A small army of econometricians was brought in to tackle the business case.\n\nAfter two months (and many billed hours) the econometricians reported that no progress was made. The timeseries turned out to not show any predictable patterns that could help with the planning of either the factory or the warehouse. The original consultants and econometricians left (pockets filled) while they reminded their client of the NDA they all signed.\nAfter a few weeks an alternative way of thinking made it’s way to management and it came from a factory worker. The problem turned out to be formulated incorrectly. The factory did not need to be turned off if the company had products to produce. It merely needed to look for a product that it could produce when demand for condoms was low. If it could find a product that correlates negatively with the original product then the factory might produce all year round.\n\nSupposedly, this is how latex pacifiers got invented.\n\nThe issue with this fable is that it is very hard to verify it’s truth. Still, one might wonder what would happen if we allow ourselves to solve problems without algorithms for a change.\nAlgorithms Live in a Mess\nFrom the perspective of a data scientist life is pretty easy. One gets to play in an isolated environment of historical data. One get’s to do this from a jupyter environment that somebody else set up. One even gets to use very complex algorithms by calling model.fit() followed by model.predict(). Lot’s of charts are made, metrics are compared and maybe a model is pushed to production.\n\nThis is in stark contrast with businesses. Businesses are not confronted with problems that are in isolation. Businesses instead have dynamic situations that consist of complex systems of changing problems that interact with each other. To repeat what is said here; I call such situations messes. Problems are abstractions extracted from messes by analysis; they are to messes ’as atoms are to tables and chairs. We experience messes, tables, and chairs; not problems and atoms.\nThis is especially true when we consider that the world is a moving target. Optimality is usually of short duration and this is especially true if the consequences of the algorithm aren’t too well understood (who knows what happens when we call .fit() or .predict() given enough columns) or if the algorithm can’t be changed easily.\nIt get’s worse. The type of model deployed in our field implies a particular paradigm of problem solving. It consists of two parts: predicting the future and making a decision based on this. The effectiveness here depends on how well we can predict the future as well as our ability to act on it. The fact that you can predict something (churn, the weather) does not mean that you can act on it. The fact that you can act on it does not mean that it is predictable.\nIf we can make good decisions based on predictions then we have another problem: we will cause a shift in the world. This might be great for the short run but it will render our future predictions futile. If we don’t see a backlash in predictive power then our algorithm might cause a way worse problem: self fulfilling prophecies.\n\nThis algorithmic attitude towards problem solving often leaves business users furiated. Algorithms don’t try to understand the underlying system when we call fit().predict(). Objectivity is hard to guarantee unless we understand the system very well. If we understand the system and the objective very well, why do we even bother modelling with black boxes? If we don’t understand the objective, how can we be optimal?\nThe Real Issues\nWe need to be careful. Stakeholders who are not in our filter bubble will get angry about the misalignment between the hype and the deliveries of our data science field.\nLet’s be honest.\nWas the real issue in 2016 the fact that we weren’t using ApacheSpark[tm]?\nWas the real issue in 2018 the fact that we’re not using DeepLearning[tm]?\nIs the real issue in 2019 going to be the fact that we’re not using AutoML?\nOr is the problem that we don’t understand the problem we’re trying to solve?\nDo you talk with the end-users? Does this person care about the latest tech or the MSE metrics?\nDo you regularly drink coffee with the working folks down the factory floor? Do you understand what their actual problem is?\nDo you understand that a business is more like a mess than a convenient isolated notebook environment?\nIf we want this field to survive a winter (or an economic downturn) it might help if the field gets better at recognising that we need to worry less about the algorithm and more about it’s application in a system. To do this we need more than mere “data scientists”. We need engineers, managers, user interface designers, web programmers, domain experts and people who can come up with ways to improve a business by thinking creatively.\nThe future of our applied field is not in our new algorithms. It is in remembering all the old things we used to do with algorithms before there was hype.\nConclusion\nI recently read an old paper and it caused me to write this document. It was written in an era when data science didn’t exist because operation research did. This field was in a similar hype: amazing things could be done with computers and every process in the world was on the brink of optimisation. Yet the paper was a bit gloomy. The title:\nthe future of operational research is past\nImagine my surprise when I read the paper, written in 1979 by Russel L. Ackhoff, as if it was written about my field today. All arguments I’ve written here are modern translations from the article which can be found here.\nIf the article doesn’t convince you enough, you might enjoy this delight of a video instead. I would’ve liked to have known Mr. Ackhoff, I like to think we would’ve gotten along quite well.\n\n\n",
    "preview": "posts/the-future-is-past/the-future-is-past_files/rockstart.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 863,
    "preview_height": 494
  },
  {
    "path": "posts/optimisation-not-gradients/",
    "title": "Optimisation, Not Gradients",
    "description": "A small point to point out a difference.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2019-04-20",
    "categories": [],
    "contents": "\nA lot of optimisation is done with gradient systems. In this blogpost I’d just like to point out a very simple example to demonstrate that you need to be careful with calling this “optimisation”. Especially when you have a system with a constaint. I’ll pick an example from wikipedia.\nNote that you can play with the notebook I used for this here.\nThe Problem\n\\[ \n\\begin{align}\n\\text{max } f(x,y) &= x^2 y \\\\\n\\text{subject to } g(x,y) & = x^2 + y^2 - 3 = 0\n\\end{align}\n\\]\nThis system is a system that has a constraint which makes it somewhat hard to optimise. If we were to draw a picture of the general problem, we might notice that only a certain set of points is of interest to us.\nWe might be able to use a little bit of mathematics to help us out. We can write our original problem into another one;\n\\[ L(x, y, \\lambda) = f(x,y) - \\lambda g(x,y) \\]\nOne interpretation of this new function is that the parameter \\(\\lambda\\) can be seen as a punishment for not having a feasible allocation. Note that even if \\(\\lambda\\) is big, if \\(g(x,y) = 0\\) then it will not cause any form of punishment. This might remind you of a regulariser. Let’s go and differentiate \\(L\\) with regards to \\(x, y, \\lambda\\).\n\\[ \n\\begin{align}\n\\frac{\\delta L}{\\delta x} &= \\Delta_x f(x,y) - \\Delta_x \\lambda g(x,y) = 2xy - \\lambda 2x \\\\\n\\frac{\\delta L}{\\delta y} &= \\Delta_y f(x,y) - \\Delta_y \\lambda g(x,y) = x^2 - \\lambda 2 y\\\\\n\\frac{\\delta L}{\\delta \\lambda} &= g(x,y) = x^2 - y^2 - 3 \n\\end{align}\n\\] All three of these expressions need to be equal to zero. In the case of \\(\\frac{\\delta L}{\\delta \\lambda}\\) that’s great because this will allow us to guarantee that our problem is indeed feasible! So what one might consider doing is to rewrite this into an expression such that a gradient method will minimise this.\n\\[q(x, y, \\lambda) = \\sqrt{\\Big( \\frac{\\delta L}{\\delta x}\\Big)^2 + \\Big( \\frac{\\delta L}{\\delta y}\\Big)^2 + \\Big( \\frac{\\delta L}{\\delta \\lambda}\\Big)^2}\\]\nAutoGrad\nIt’d be great if we didn’t need to do all of this via maths. As lucky would have it; python has great support for autodifferentiation so we’ll use that to look for a solution. The code is shown below.\nfrom autograd import grad\nfrom autograd import elementwise_grad as egrad\nimport autograd.numpy as np\nimport matplotlib.pylab as plt\n\ndef f(weights):\n  x, y, l = weights[0], weights[1], weights[2]\n  return y * x**2\n\ndef g(weights):\n  x, y, l = weights[0], weights[1], weights[2]\n  return l*(x**2 + y**2 - 3)\n\ndef q(weights):\n  dx = grad(f)(weights)[0] + grad(g)(weights)[0]\n  dy = grad(f)(weights)[1] + grad(g)(weights)[1]\n  dl = grad(f)(weights)[2] + grad(g)(weights)[2]\n  return np.sqrt(dx**2 + dy**2 + dl**2)\n\nn = 100\nwts = np.array(np.random.normal(0, 1, (3, )))\nfor i in range(n):\n  wts -= egrad(q)(wts) * 0.01\nThis script was ran and logged and produced the following plot:\n\nWhen we ignore the small numerical inaccuracy we can confirm that our solution seems feasible enough since \\(q(x, y, \\lambda) \\approx 0\\). That said, this solution feels a bit strange.Taking the found values \\(f(x^*,y^*) \\approx f(0.000, 1.739)\\) suggests that the best value found is \\(\\approx 0\\). Are we sure we’re in the right spot?\n\nOne hint that we’re maybe in the wrong is that \\(\\frac{\\delta^2 L}{\\delta x^2} > 0\\) which suggests that we’re not in a maximum.\nThe Issue\nWe’ve used our tool AutoGrad the right way but there’s another issue: the gradient might get stuck in a place that is not optimal. There are more than 1 point that satisy the three derivates shown earlier. To demonstrate this, let us use sympy instead.\nimport sympy as sp \n\nx, y, l = sp.symbols(\"x, y, l\")\n\nf = y*x**2\ng = x**2 + y**2 - 3 \nlagrange = f - l * g\n\nsp.solve([sp.diff(lagrange, x), \n          sp.diff(lagrange, y), \n          sp.diff(lagrange, l)], [x, y, l])\nThis yields the following set of solutions:\n\\[\\left[\\left(0, -\\sqrt{3}, 0\\right ), \\left ( 0, \\sqrt{3}, 0\\right ), \\left ( - \\sqrt{2}, -1, -1\\right ), \\left ( - \\sqrt{2}, 1, 1\\right ), \\left ( \\sqrt{2}, -1, -1\\right ), \\left ( \\sqrt{2}, 1, 1\\right )\\right ]\\]\nNote that one of these solutions found with sympy yields \\((0, \\sqrt{3}, 0)\\) which corresponds \\(\\approx [-0.0001, 1.739, -0.0001]\\). We can confirm that our gradient solver found a solution that was feasible but it did not find one that is optimal.\nA solutions out of our gradient solver can be a saddle point, local minimum or local maximum but the gradient solver has no way of figuring out which one of these is the global optimum (which in this problem is \\(x,y = \\pm (-\\sqrt{2}, 1)\\)).\nWeather or not we get close to the correct optima depends on the starting values of the variables too. To demonstrate this I’ve attempted to run the above script multiple times with random starting values to see if a pattern emerges.\n\n\nIt seems that the starting value for \\(x,y\\) have a big effect.\n\n\nIt seems that the starting value for \\(\\lambda\\) also has a big effect.\nSome Learnings\nThis example helps point out some downsides of the gradient approach:\nGradient descent can get stuck in a local (suboptimal) position. The more local optima that exist the trickier it gets to guarantee any form of optimality.\nGradient descent is not really meant to handle constraints. We’ve used a bit of a hack in order to incorporate the constraint into our criterion but it feels more like we’re regularising for it than that we’re using it as a strict constraint.\nBy adding a constraint to our search we might very well be increaseing the number of local optima that the gradient search might encounter.\nIn short: gradient descent is a general tactic but when we add a constraint we’re in trouble. It also seems like variants of gradient descent like Adam will also suffer from these downsides.\nWhy Care?\nYou might wonder why to care. Many machine learning algorithms don’t have a constraint. Imagine we have a function \\(f(x|w) \\approx y\\) where \\(f\\) is a machine learning algorithm. A lot of these algorithms involve minimising a system like below;\n\\[ \n\\text{min } q(w) = \\sum_i \\text{loss}(f(x_i|w) - y_i)\n\\]\nThis common system does not have a constraint. But would it not be much more interesting to optimise another type of system?\n\\[ \n\\begin{align}\n\\text{min } q(w) & = \\sum_i \\text{loss}(f(x_i|w) - y_i) \\\\\n\\text{subject to } h(w) & = \\text{max}\\{\\text{loss}(f(x_i|w) - y_i) \\} = \\sigma\n\\end{align}\n\\] This would be fundamentally different than regularising the model to prevent a form of overfitting. Instead of revalue-ing an allocation of \\(w\\) we’d like to restrict an algorithm from ever doing something we don’t want it to do. The goal is not to optimise for two things at the same time but instead we would impose a hard constraint on the ML system.\nAnother idea for an interesting constraint:\n\\[ \n\\begin{align}\n\\text{min } q(w) & = \\sum_i \\text{loss}(f(x_i|w) - y_i) \\\\\n\\text{subject to } & \\\\\nh_1(w) & = \\text{loss}(f(x_1|w) - y_1) \\leq \\epsilon_1 \\\\ \n& \\vdots \\\\ \nh_k(w) & = \\text{loss}(f(x_k|w) - y_k) \\leq \\epsilon_k\n\\end{align}\n\\] The idea here is that one determines some constraints \\(1 ... k\\) on the loss of some subset of points in the original dataset. The idea here being that you might say “these points must be predicted within a certain accuracy while the other points matter less”.\n\\[ \n\\begin{align}\n\\text{min } q(w) & = \\sum_i \\text{loss}(f(x_i|w) - y_i) \\\\\n\\text{subject to } & \\\\\nh_1(w) & =\\frac{\\delta f(x|w)}{\\delta x_{k}} \\geq 0  \\\\\nh_2(w) & =\\frac{\\delta f(x|w)}{\\delta x_{m}} \\leq 0 \n\\end{align}\n\\] Here we’re trying to tell the model that for some feature \\(k\\) we demand a monotonic increasing relationship with the output of the model and for some feature \\(m\\) we demand a monotonic decreasing relationship with the output. For some features a modeller might be able to declare upfront that certain features should have a relationship with the final model and being able to constrain a model to keep this in mind might make the model a lot more robust to certain types of overfitting.\nDreams\nThis flexibility of modelling with constraints might do wonders for interpretability and feasibility of models in production. The big downer is that currently we do not have general tools to guarantee optimality under constraints in general machine learning algorithms.\nThat is, this is my current understanding. If I’m missing out on something: hit me up on twitter.\n\\(\\text{hack} \\neq \\text{fix}\\)\nAs a note I figured it might be nice to mention a hack I tried to improve the performance of the gradient algorithm.\nThe second derivatives of \\(L\\) with regards to \\(x,y\\) also need to be negative if we want \\(L\\) to be a maximum. Keeping this in mind we might add more information to our function \\(q\\).\n\\[q(x, y, \\lambda) = \\sqrt{\\Big( \\frac{\\delta L}{\\delta x}\\Big)^2 + \\Big( \\frac{\\delta L}{\\delta y}\\Big)^2 + \\Big( \\frac{\\delta L}{\\delta \\lambda}\\Big)^2 + \\text{relu} \\Big(\\frac{\\delta L}{(\\delta x)^2}\\Big)^2 + \\text{relu} \\Big(\\frac{\\delta L}{(\\delta y)^2}\\Big)^2}\\]\nThe reason why I’m adding a \\(\\text{relu}()\\) function here is because if the second derivative is indeed negative the error out of \\(\\text{relu}()\\) is zero. I’m squaring the \\(\\text{relu}()\\) effect such that the error made is in proportion to the rest. This approach also has a big downside though: the relu has large regions where the gradient is zero. So we might approximate with a softplus instead.\n\\[q(x, y, \\lambda) = \\sqrt{\\Big( \\frac{\\delta L}{\\delta x}\\Big)^2 + \\Big( \\frac{\\delta L}{\\delta y}\\Big)^2 + \\Big( \\frac{\\delta L}{\\delta \\lambda}\\Big)^2 + \\text{softplus} \\Big(\\frac{\\delta L}{(\\delta x)^2}\\Big)^2 + \\text{softplus} \\Big(\\frac{\\delta L}{(\\delta y)^2}\\Big)^2}\\]\nI was wondering if by adding this, one might improve the odds of finding the optimal value. The results speak for themselves:\nwith softplus 310/1000 solutions hit the proper value\nwithout 300/1000 solutions hit the proper value\nLuck might be a better tactic.\nNote that you can play with the notebook I used for this here.\n\n\n\n",
    "preview": "https://upload.wikimedia.org/wikipedia/commons/b/bf/LagrangeMultipliers2D.svg",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {}
  },
  {
    "path": "posts/sensors-are-servers/",
    "title": "Sensors are Servers",
    "description": "The only servers in my stack are the sensors themselves.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2018-12-28",
    "categories": [],
    "contents": "\nI bought an old house and I want to start fixing things.\nthere’s a humidity problem\nthere’s a temperature leak or two\nthere’s an opporunity to measure and to improve, or overengineer\nIt helps that I’m a bit of a nerd and this is a great excuse to learn about electronics. Before I knew it I had a raspberry in every room of my house and I started collecting my own data.\nHardware\nFor this problem, I chose to go with the rapsberry pi stack with sensors from pimoroni. I cannot recommend the pimoroni people enough if you’re interested in working with python. They offer a garden hat which is a solverless solution to attach sensors to a raspberry.\n\n\n\nThe main sensor that I am using is the BME 680. Take care not to confuse it with the BMP 280; the latter is much cheaper but does not offer a humidity reading. There are many other sensors that play nice on this platform that are worth a mention though:\na distance sensor, very useful for checking open doors\na light sensor can check for the sun or indoor lighting\na thermal camera even though the resolution is a bit too low for me\neven a display that you can stick on the garden\nSoftware\nThe sensors themselves come with packages that are installable from pypi which is great! The main problem though has to do with updating. I want the raspberries to post data somewhere (BigQuery) and if the endpoint changes I’d prefer that I do not need to manually update via USB.\n\nAfter thinking about what I wanted the stack to look like I considered that the raspberries themselves act as if they are servers so I could go ahead and use ansible for it. It is an old tool, I remember we’ve used it to provision hadoop clusters 5 years ago, but it has all the features that I needed.\nThe only manual thing that you need to do is put an identity file on each raspberry and allow ssh traffic on each device. From there you can do a lot of things programmatically if you configure the right files.\nSSH Config\n# ~/.ssh/config\nHost rpi-zero-room1\n    HostName 123.456.789.1\n    User pi\n    IdentityFile ~/.ssh/home-rpi-keyfile\nHost rpi-zero-room2\n    HostName 123.456.789.2\n    User pi\n    IdentityFile ~/.ssh/home-rpi-keyfile\n..\nWith this file configured you gain two benefits:\nYou can easily ssh into each raspberry from your laptop\nYou can rely on ansible being able to recognize any hosts in that file\nAnsible Files\nIn my project folder I have the following file structure:\n> tree -L 2\n.\n├── README.md\n├── ansible\n│   ├── hosts\n│   ├── provision-cronjobs-rasp-electro.yml\n│   ├── provision-cronjobs-rasp-zero.yml\n│   ├── provision-files-rasp3.yml\n│   └── provision-tools-rasp3.yml\nThe hosts file should look like:\n[raspberries]\nrpi-zero-room1 ansible_user=pi\nrpi-zero-room2 ansible_user=pi\nrpi-zero-room3 ansible_user=pi\n..\nrpi-zero-room9 ansible_user=pi\n\n[different-sensors]\nrpi-zero-other ansible_user=pi\nNote that the names of the servers need to correspond with those in the config file and from here you can split up functionality per server. I have certain raspberries that have completely different sensors and you can split this from the hosts file.\nThe remaining .yml files contain instructions for the raspberry which are picked up and are run in parallel.\nExample: Installation\nHere’s an example .yml file that installs some required software.\n- hosts: raspberries, different-sensors\n  become: yes\n  become_user: root\n  tasks:\n    - name: make sure that we have most recent apt\n      command: apt-get update\n\n    - name: install all the apt get stuff, incl fail2ban\n      apt: name={{item}} state=installed\n      with_items:\n        - fail2ban\n        - postfix\n        - build-essential\n\n    - name: pip install requirements globally\n      pip: name={{item}} state=present\n      with_items:\n        - google-cloud-bigquery\n        - ipython\n\nThe command might differ depending on the OS. Note that pypi is available via ansible!\nExample: Cronjobs\nHere’s an example .yml file that sets a cronjob.\n- hosts: raspberries\n  become: yes\n  become_user: root\n  tasks: \n    - name: remove old humidity/temperature cronjob\n      cron:\n        name=\"humiditemp\"\n        state=absent\n        user=pi\n    - name: add new humidity/temperature cronjob\n      cron:\n        name=\"humiditemp\"\n        minute=\"*\"\n        user=pi\n        job=\"sudo /usr/bin/python /loggers/cron-scripts/measure.py\"\n\nNote that you need to remove any old cronjobs before adding new ones. Also note this won’t run for all hosts.\nUpdates\nWith ansible set up, updates are now super easy. Whenever the code is updated, I merely need to run:\nansible -i hosts all -m ping\nansible-playbook -i hosts provision-tools-rasp3.yml\nansible-playbook -i hosts provision-files-rasp3.yml\nansible-playbook -i hosts provision-cronjobs-rasp3.yml\nThis will update any raspberry connected to wifi.\nSensor in the Garden\nI was considering installing a raspberry outside with solar panels and everything.\nI’ll still do this someday, but hardware takes more time than software and you cannot copy/paste in real life. So instead of measuring the weather outside of my house, I figured scraping the weather APIs would be a much better idea.\n\nIt was. Adding to my stack was super easy. Google now has a cloud-cron service which allows you to have a serverless function called every minute.\nCosts\nPer day I log about.\n\\[ 3 \\text{ sensors} \\times 7 \\text{ devices} \\times 1440 \\text{ readings/day} \\approx 30 \\text{K rows/day} \\]\nI’ve been running this for about 3 months.\n\\[ 30 \\text{K rows/day} \\times 90 \\text{ days} \\approx 2.7 \\text{M rows}\\]\nThis totals to about 150MB of data, with a dumb schema. That’s 600MB per year.\nPrice of Storage\n\\[ $0.020 \\text{GB}^{-1} \\times 0.6 \\text{GB} \\times 12 \\text{ months} \\approx $0.12 \\text{ year}^{-1}\\]\n\nSure, this amount will increase, but it takes a while before I start paying more than $1.\nPrice of Compute\n\\[ \\frac{1,000,000 \\text{ MB}}{600 \\text{ MB}} \\approx 1667 \\text{ years before I start paying}\\]\n\nThere is a free tier on google’s side, which suggests I don’t pay for the first TB each month.\nEnd Product\nBigQuery is really easy to access from R thanks to bigrquery. Setting up a dashboard locally took about half an hour and I can make all the charts that I desire from ggplot.\n\nThere’s a few things that I found interesting to read from these sensors:\nthe humidity will spike in the rooms next to the bathrooms after somebody has had a shower\nthe temperature does not fluctuate (in winter) in the room where the thermostat is\nthe humidity increases when there’s just been rain, which suggests a leak\nthe temperature increases when there are no clouds (this makes sense, I’ve got giant windows pointing south)\nIt’s fun to measure one’s house like this and it helps to motivate some investments too.\nFinal Tips\nIf you’re going to do this yourself; the heat from the raspberry influences the temperature/humidity sensor. So does sunlight!\nEither do some hardware work and move the sensors away from the raspberry if you want to get accurate readings or consider physics.\n\\[ \\text{actual temperature} = f(\\text{sensor temp}, \\text{cpu temp}) \\]\nEither way, consider that sensors are always biased. I haven’t fully figured out a good way around it just yet. The hardware can also get expensive if you want to have one such sensor per room in your house.\nI also think it is worth mentioning that most investments I’ve made to the house don’t really require the measurements. Adding a curtain in front of a leaky door makes for less heatloss and you do not need automated measuring tools in order to do this right away. Still, electronics can be a fun hobby.\nAcknowledgments\nThe real heros of this story are raspberry/pimoroni when it comes to hardware and python/linux when it comes to software. I couldn’t imagine just doing this 10 years ago and we’ve come a long way.\n\n\n\n",
    "preview": "posts/sensors-are-servers/sensors-are-servers_files/stack.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 2217,
    "preview_height": 1139
  },
  {
    "path": "posts/optimal-wine-strategies/",
    "title": "Wine Cellar Strategies",
    "description": "Pretending there's an optimal way to drink it.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2018-12-22",
    "categories": [],
    "contents": "\nI was glancing over one of my old college books; Optimisation from Brinkhuis, page 448. It had a fun problem that got me thinking.\nStory of a Wine Cellar\nThere’s a long summer party happening at a convex shaped castle. The main attraction of the party is a wine cellar that contains many bottles of wine, \\(n\\) in fact. Every night the attendees of the party can drink a bottle of wine, causing a party vibe. The attendees wonder, should all the wine be drunk on a single night or should the wine be spread out?\nLet’s assume that the partyvibe can be quantified. The total party vibe is a product of the number of bottles of wine drunk per evening.\nExample\nSuppose that we have 20 bottles of wine. Then these allocations would have the following values:\n\\[\n\\begin{equation} \\label{eq1}\n\\begin{split}\n\\text{score}(\\{10, 10\\}) & = 10 \\times 10 = 100 \\\\\\\\\n\\text{score}(\\{4, 6, 10\\}) & = 10 \\times 6 \\times 4 = 240 \\\\\\\\\n\\text{score}(\\{10, 5, 5\\}) & = 10 \\times 5 \\times 5 = 250 \n\\end{split}\n\\end{equation}\n\\]\n\nNote; a core part of the problem is that you can choose to spread out the wine drinking. But is this optimal?\nThe problem\nWhat is the optimal allocation? How many bottles should be opened on every evening? Assume that we only have \\(n\\) bottles to start with.\nPython Solution\nThis problem, turns out, can be solved with a bit of recursion via dynamic programming. The main thing to recognize is the recursive relationship. In maths; I have a function \\(s(n,b)\\) that I want to optimise where \\(n\\) is the number of bottles that are currently in the cellar and \\(b\\) is the number of bottles that is being taken for this night. On considering the problem, maths look like this:\n\\[ \\arg \\max_b s(n, b) = b \\times \\arg \\max_{b'} s(n-b, b')\\]\n\nThe maths looks more intimidating than it needs to be, what I’m trying to point out is that the problem is recursive.\nWriting this in python turns out to be relatively simple, but it took me 10 minutes on paper to get it right.\nfrom functools import reduce\n\ndef argmax_slow(bottles):\n    if bottles == 0:\n        return [1]\n    if bottles <= 3:\n        return [bottles]\n    else:\n        max_score, max_args = 0, []\n        for b in range(1, bottles + 1):\n            b_args = argmax(bottles - b)\n            b_value = reduce(lambda x,y: x * y, [b] + b_args)\n            if b_value > max_score: \n                max_score, max_args = b_value, [b] + b_args\n        return max_args\nSince the function is recursive, one would be wise to add a cache in order to memoize intermediate results.\nfrom functools import lru_cache\n\n@lru_cache(maxsize=512)\ndef argmax(bottles):\n    if bottles == 0:\n        return [1]\n    if bottles <= 3:\n        return [bottles]\n    else:\n        max_score, max_args = 0, []\n        for b in range(1, bottles + 1):\n            b_args = argmax(bottles - b)\n            b_value = reduce(lambda x,y: x * y, [b] + b_args)\n            if b_value > max_score: \n                max_score, max_args = b_value, [b] + b_args\n        return max_args\nThe timing from jupyter lab shows the speedup is definately measureable.\n> %time _ = argmax(512)\nCPU times: user 4 µs, sys: 1 µs, total: 5 µs\nWall time: 6.2 µs\n> %time _ = argmax_slow(512)\nCPU times: user 14.8 ms, sys: 1.07 ms, total: 15.9 ms\nWall time: 15 ms\nMath Solution\nThe interesting thing about this problem really shows itself when you see a solution.\n> argmax_slow(20)\n[2, 3, 3, 3, 3, 3, 3]\nMaybe we didn’t need any programming here. Let’s consider some parts of the problem:\nYou never want to open just one bottle. Instead of opening lots of bottles in one evening, it seems a better tactic to open bottles on many evenings. Why so many threes though?\nIt might not seem straightforward, but it’s because \\(2^3 < 3^2\\). Spending two nights with three bottles is “more better” in this example than spending three nights with two.\nFinal Remark\nAs a final remark, I really would like to point out that associating alcohol to “more fun” is an obvious path to ruin although the strategy of spreading the drinks such that you never consume more than three a night seems like sound advice.\n\n\n\n",
    "preview": "posts/optimal-wine-strategies/optimal-wine-strategies_files/dynamic.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 643
  },
  {
    "path": "posts/vi-drives-me-nuts/",
    "title": "VI Drives me NUTS",
    "description": "Feel free to be a bit weary.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2018-11-09",
    "categories": [],
    "contents": "\nVariational Inference is “hip” and I can’t say that I am a huge fan. I decided to give it a try and immediately it hit my head. In this document I hope to quickly demonstrate a potential failure scenario.\nThe Model\nHere is the code for the model. It is a model about increased weights of chickens who are given different diets.\n\ndf = pd.read_csv(\"http://koaning.io/theme/data/chickweight.csv\", \n                 skiprows=1,\n                 names=[\"r\", \"weight\", \"time\", \"chick\", \"diet\"])\ntime_input = 10\n\nwith pm.Model() as mod: \n    intercept = pm.Normal(\"intercept\", 0, 2)\n    time_effect = pm.Normal(\"time_weight_effect\", 0, 2, shape=(4,))\n    diet = pm.Categorical(\"diet\", p=[0.25, 0.25, 0.25, 0.25], shape=(4,),\n                          observed=dummy_rows)\n    sigma = pm.HalfNormal(\"sigma\", 2)\n    sigma_time_effect = pm.HalfNormal(\"time_sigma_effect\", 2, shape=(4,))\n    weight = pm.Normal(\"weight\", \n                       mu=intercept + time_effect.dot(diet.T)*df.time, \n                       sd=sigma + sigma_time_effect.dot(diet.T)*df.time, \n                       observed=df.weight)\n    trace = pm.sample(5000, chains=1)\nNext I’ll show how the traceplots are different if we compare different inference methods.\nNUTS sampling results\nI took 5500 samples with NUTS. It took about 7 seconds and this is the output:\n\n\nIt looks pretty clean. Nice convergence on all metrics.\nMetropolis sampling results\nI took 20000 samples with Metropolis. It took about 14 seconds and this is the output:\n\n\nNote that the burn in isn’t the only issue: the parameters that I end up with are way off.\nVI results\nI used the fullrank_advi setting. Here’s a traceplot from the samples I took from the approximated posteriour.\n\n\nAgain, the fitted estimates are nowhere near the NUTS samples.\nFix?\nThe interesting thing is that if I change the model slightly, VI suddenly has no issues (this was pointed out to me by a collegue, Mathijs).\n\nn_diets = df.diet.nunique()\n\nwith pm.Model() as model:\n    mu_intercept = pm.Normal('mu_intercept', mu=40, sd=5)\n    mu_slope = pm.HalfNormal('mu_slope', 10, shape=(n_diets,))\n    mu = mu_intercept + mu_slope[df.diet-1] * df.time\n    sigma_intercept = pm.HalfNormal('sigma_intercept', sd=2)\n    sigma_slope = pm.HalfNormal('sigma_slope', sd=2, shape=n_diets)\n    sigma = sigma_intercept + sigma_slope[df.diet-1] * df.time\n    weight = pm.Normal('weight', mu=mu, sd=sigma, observed=df.weight)\n    approx = pm.fit(20000, random_seed=42, method=\"fullrank_advi\")\nThe main difference is that I am no longer using pm.Categorical.\nWith that out of the way suddenly the estimates look a whole lot better.\n\n\nI can’t accurately pinpoint what exactly is causing this massive shift but I might imagine that anything using a gradient would have trouble with something discrete in a system.\nConclusion\nBe careful when using variational inference. It might be faster but it is only faster because it approximates. I’m not the only person why is a bit skeptical of variational inference.\nThe alternative, NUTS sampling still amazes me, even though it isn’t perfect.\n\n\n",
    "preview": "posts/vi-drives-me-nuts/nuts.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 604,
    "preview_height": 196
  },
  {
    "path": "posts/rephrasing-the-billboard/",
    "title": "Rephrasing the Billboard",
    "description": "A probability problem involving 40,000 sent letters.",
    "author": [
      {
        "name": "Vincent D. Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2018-10-04",
    "categories": [],
    "contents": "\nA podcast got me doing maths again.\n\nI was listening to this episode from the 99% invisible podcast. It contains a fascinating story about a radio contest where you could’ve won a house. Turns out that in the 80ies there was a severe economic crisis and this prompted many people to sign up to the contest. Many people signed up and only three men were picked to take part in the actual competition: who-ever could live on a billboard the longest would win a house. By the end of the podcast you’ll learned that this contest lasted for 261 days and got way out of hand.\nIt’s a great story, but a specific part of the story caught my attention.\nOut of all the submissions that were being sent in, only three people would be selected. This got people thinking: one of the contestants actually submitted 47000 entries in order to increase the odds of getting selected. The tactic makes sense if you’re desperate and the podcast explains that the 1st ten letters that were opened by the selection committee were from a single participant.\nThe radio station ended up picking this contestant nonetheless but I wondered: what if the station had a policy of deny-ing anybody that sends more than one letter? The radio station wouldn’t go through all the letters (for obvious reasons, the podcast reports 500,000 letters being sent) but suppose that we pick three random letters for the contestants then we would expect each candidate to only occur once. With this in mind, you might want to limit the number of letters that you send.\nEnter Maths\nLet’s do the maths here. We have a few free variables in the system that we’ve described.\n\\(c\\): the number of contestants who will be picked\n\\(s\\): the number of letters we send in\n\\(a\\): the number of other letters sent by other people\nOne Contestant\nLet’s start simple by first calculating the probability of getting selected if you’re the only person sending something in.\n\\[ p_{c=1}(\\text{picked exactly once}) = \\frac{s}{a + s} \\]\nIf only one candidate is picked then we want to have \\(s\\) be as large as possible.\nTwo Contestants\nLet’s now consider two contestants. The idea is that either you get selected first and then somebody who isn’t you needs to get selected or the other way around.\n\\[\\begin{equation} \\label{eq1}\n\\begin{split}\np_{c=2}(\\text{picked exactly once}) & = \\frac{s}{a + s} \\times \\frac{a}{a + s -1} + \\frac{a}{a + s} \\times \\frac{s}{a + s -1} \\\\\\\\\n & = \\frac{s \\times a}{(a+s)\\times(a+s-1)} + \\frac{s \\times a}{(a+s)\\times(a+s-1)} \\\\\\\\\n & = 2 \\times \\frac{s \\times a}{(a+s)\\times(a+s-1)}\n\\end{split}\n\\end{equation}\\]\nIt seems like we need to be a bit more careful now. If \\(s >> a\\) then we need to check that what is at the denominator doesn’t grow.\nThree Contestants\nWhen we do the maths for three contestants then we see a pattern occur.\n\\[\\begin{equation} \\label{eq2}\n\\begin{split}\np_{c=3}(\\text{picked exactly once}) & = \\frac{s}{a + s} \\times \\frac{a}{a + s -1} \\times \\frac{a-1}{a + s - 2} \\\\\\\\\n& + \\frac{a}{a + s} \\times \\frac{s}{a + s -1} \\times \\frac{a-1}{a + s - 2} \\\\\\\\\n& + \\frac{a}{a + s} \\times \\frac{a-1}{a + s -1} \\times \\frac{s}{a + s - 2} \\\\\\\\\n & = 3 \\times \\frac{s \\times a \\times (a-1)}{(a+s) \\times (a+s-1) \\times (a+s-2)}\n\\end{split}\n\\end{equation}\\]\nPlotting\nMaths is nice and all but true intuition comes from plotting the numbers.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n%matplotlib inline \n\ns = np.arange(1, 500000)\na = 500000\nc = 3\n\nprob_zero = (a*(a-1)*(a-2))/((a+s)*(a+s-1)*(a+s-2))\nprob_one = 3*(s*a*(a-1))/((a+s)*(a+s-1)*(a+s-2))\nprob_two = 3*(s*(s-1)*a)/((a+s)*(a+s-1)*(a+s-2))\nprob_three = (s*(s-1)*(s-2))/((a+s)*(a+s-1)*(a+s-2))\ndf = pd.DataFrame({'zero': prob_zero, \n                   'one': prob_one, \n                   'two': prob_two, \n                   'three': prob_three})\ndf.index = s\ndf.plot(title=\"probability of num letters opened after sending\", \n        figsize=(16, 8));\n\nTurns out that if you really want to win at this game, you’ll need to make sure that about half of the amount of other letters is the amount you need to send (this translates to 1/3 of the total letters to need to come in from you). But can we find this number exactly?\nSolving\nLet’s look at our equation from before.\n\\[\\begin{equation} \\label{eq3}\n\\begin{split}\np_{c=3}(\\text{picked exactly once}) & = 3 \\times \\frac{s \\times a \\times (a-1)}{(a+s) \\times (a+s-1) \\times (a+s-2)}\n\\end{split}\n\\end{equation}\\]\nIf we want to optimise that equation for \\(a\\) we’ll find that it is actually very tricky. We have a fraction with a polynomial in there and even when we have something like sympy it seems that it would not yield a pretty solution.\nRealising this makes the path towards the actual solution a whole lot easier. It gives us an opporunity to reformulate the problem such that everything becomes a lot more simple. Let’s change the variables, but only slightly.\nSuppose now that we have a system with these variables.\n\\(c\\): the number of contestants\n\\(t\\): the total number of applications\n\\(s\\): the number of letters we send in\nNote that the \\(t\\) parameter is the only one that is really different. With these parameters to use, let us check what our equation might look like.\n\\[\\begin{equation} \\label{eq4}\n\\begin{split}\np_{c=3}(\\text{picked exactly once}) & = \\frac{s}{t} \\times \\frac{t-s}{t -1} \\times \\frac{t-s-1}{t - 2} \\\\\\\\\n& + \\frac{t-s}{t} \\times \\frac{s}{t -1} \\times \\frac{t-s-1}{t - 2} \\\\\\\\\n& + \\frac{t-s}{t} \\times \\frac{t-s-1}{t -1} \\times \\frac{s}{t - 2} \\\\\\\\\n & = 3 \\times \\frac{s \\times (t-s) \\times (t-s-1)}{t \\times (t-1) \\times (t-2)}\n\\end{split}\n\\end{equation}\\]\nBy just rephrasing this, everything became a whole lot simpler. Since the variable \\(t\\) is something that is given we merely have created a division by a constant. You still won’t need to do any maths if you don’t feel like it though because sympy is here to help.\nimport sympy as sp \ns, t = sp.symbols(\"s, t\")\nsp.solve(sp.diff(3*s*(t-s)*(t-s-1)/(t*(t-1)*(t-2)), s), s, 0)\nThe positive solution of that expression yields:\n\\[\n\\frac{2 t}{3} + \\frac{\\sqrt{t^{2} - t + 1}}{3} - \\frac{1}{3}\n\\]\nNote that in sympy you can also solve numerically via;\nt = 500000\nsp.nsolve(sp.diff(3*s*(t-s)*(t-s-1)/(t*(t-1)*(t-2)), s), s, 0)\nWhen \\(t=500000\\) then \\(s^* = 166666.667\\). This is a similar conclusion to what we saw before.\nConclusion\nWhat is nice about this problem is that the maths can either be very easy or very hard depending on how you formulate the problem. If you take the initial formulation then you’ll find that even sympy will have a huge problem with it.\nThe exercise got a whole lot simpler when we rephrased the problem to make our life easier. It might make you wonder if a similar phenomenon occurs in machine learning too.\n\n\n\n",
    "preview": "posts/rephrasing-the-billboard/rephrasing-the-billboard_files/billboard.jpg",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {}
  },
  {
    "path": "posts/gaussian-auto-embeddings/",
    "title": "Gaussian Auto Embeddings",
    "description": "Sort of came up with an alternative to VI here.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2018-08-01",
    "categories": [],
    "contents": "\nBelow is a picture of a simple autoencoder.\n\n\nIt’s not a bad drawing.\nIf we train this autoencoder appropriately then we can assume that we gain a latent representation of our original data. This can be done via convolution layers, dense layers or hyped layers. It doesn’t really matter as long as we’re confident that the encoder can decode whatever it encodes.\nIt is worthwhile to note that this encoder is trained in a completely unsupervised manner. It often can be the case, like in mnist, that clusters of similar items appear. This gave me an interesting thought: we can also train a model on this latent representation. This way we can see the encoder as a mere preprocessing step.\n\nI started thinking some more. I could do an extra step and assume that the model I’d want to train is a gaussian mixture model. I’d train a single gaussian mixture model per class. If I do that then there’s a few things that I gain:\nI can use the GMM models as a classifier. When a new datapoint comes in then I can push it through the encoder and check the resulting likelihood for every GMM class. I could pick the GMM with the highest likelihood or report on normalised likelihoods as a probability vector.\nI can also suggest that a new datapoint does not belong to any class. I can put a threshold on the likelihoods of each GMM which could lead to a ‘none’ prediction. This is nice.\nI can also sample from each GMM with ease (because, it’s gaussian). The sample that I get in latent space can be thrown in the decoder and this might be a nice method to sample data (like GANs and VAEs).\nI can interpret the GMM as a manifold of sorts. The GMM can describe a somewhat complicated shape in latent space that belongs to a certain class.\nBecause the GMMs are all gaussian, I gain some articulate properties from the model. I can calculate the entropy per GMM and I can calulate the KL divergence between them. I can quantify an estimate of how similar or dissimilar classes are.\nWith all this in mind. It feels more appropriate to draw this model like this:\n\nThis could be a somewhat general, albeit very articulate model. This is nice most neural approaches aren’t too great in the explanatory department.\nBut does it work?\nLet’s see if we can get a view into this. I’ve got two notebooks that can back me up; one with mnist as a dataset and one with fashion mnist as a dataset. I’m well aware that this won’t be a general proof, but it seems to be a nice place to start.\nAutoencoder\nThe data that goes into the encoder, also seems to come out.\n\n\nimgs <- c(\"gaussian-auto-embeddings_files/mnist-input-output.png\", \n          \"gaussian-auto-embeddings_files/fminst-input-output.png\")\nknitr::include_graphics(imgs)\n\n\n\n\n\nThe white pixels go in, the bright colors go out. It works!\nSampling Data\nI’ve trained simple autoencoders as well as gaussian mixture models per class. This is what you see what I sample from a gaussian for a given class and then pass that along the decoder.\n\n\nimgs <- c(\"gaussian-auto-embeddings_files/mnist-samples.png\", \n          \"gaussian-auto-embeddings_files/fmnist-samples.png\")\nknitr::include_graphics(imgs)\n\n\n\n\n\nThe numbers look like numbers, the fashion looks like fashion!\nIt looks allright, not perfect, but allright. There’s very little tuning that I did and I didn’t train for more than 60 epochs. Definately the autoencoder might appreciate a bit more network design, but overall it seems to work.\nYou can also train a single GMM on all classes instead of training a GMM per class. If you sample from this single GMM then it looks sensible, but worse.\n\n\nimgs <- c(\"gaussian-auto-embeddings_files/minst-allgmm.png\", \n          \"gaussian-auto-embeddings_files/fminst-allgmm.png\")\nknitr::include_graphics(imgs)\n\n\n\n\n\nWe loose some context, but it still feels like we’re surfing on a manifold.\nManifold Evidence\nSuppose that I do not sample from the GMM but that I sample uniformly the entire latent space.\n\n\nimgs <- c(\"gaussian-auto-embeddings_files/autoencoder4.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nWhen I do that, the decoder outputs gibberish. What you mainly see is that some upsampling layers randomly start to activate and propogate forward.\n\n\nimgs <- c(\"gaussian-auto-embeddings_files/random-samples.png\",\n          \"gaussian-auto-embeddings_files/random-samples-2.png\")\nknitr::include_graphics(imgs)\n\n\n\n\n\nThe left side shows output from the mnist decoder, the right side shows output from the fashion mnist decoder.\nThis shows that my approach is a bit different than perhaps how some VAEs might do it. Since my method does not impose and form of variational shape during the embedding step the points in latent space might enjoy a bit more freedom to place themselves in the space however they like. This means that the latent space will have some useless areas and it is the hope that the GMM’s that I train will never assign any likelihood there.\nPredicting Data\nYou can sample actual labels and put them into a GMM to check the likelihood that comes out.\n\n\nimgs <- c(\"gaussian-auto-embeddings_files/mnist-class.png\",\n          \"gaussian-auto-embeddings_files/fminst-class.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nThe accuracy on the test/train sets are describe in the table below.\nmnist  train 0.9816\nmnist  test  0.9766\nfmnist train 0.8776\nfmnist test  0.8538\nNot the best, but not the worst either.\nTransitioning Classes\nWhen I sample one point from one GMM class and another point from another then I can try to see what it looks like to morph an item from one class to another. What do I mean by this? I mean that we won’t merely transition between images like below.\n\n\nimgs <- c(\"gaussian-auto-embeddings_files/mnist-transition-bad.png\",\n          \"gaussian-auto-embeddings_files/fmnist-transition-bad.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nInstead what I do is sample two classes in latent state and interpolate in latent state before passing it on to the decoder.\n\n\nimgs <- c(\"gaussian-auto-embeddings_files/minst-morph.png\",\n          \"gaussian-auto-embeddings_files/fmnist-transition-good.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nThese look allright, but they could be better, you could for example sample two classes that are hard to morph.\n\n\nimgs <- c(\"gaussian-auto-embeddings_files/minst-mehmorph.png\",\n          \"gaussian-auto-embeddings_files/fminst-mehmorph.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nMaybe it is hard to traverse the manifold between a shoe and a shirt.\nArticulating Styles\nI’ve trained the GMMs with 6 means. It seems that every mean is able to pick up on a certain cluster within a class. One interpretation is style. There are different styles of writing a number just like there are multiple styles of drawing a piece of clothing.\n\n\nimgs <- c(\"gaussian-auto-embeddings_files/minst-styles.png\",\n          \"gaussian-auto-embeddings_files/fminst-styles.png\")\nknitr::include_graphics(imgs)\n\n\n\n\n\nSix styles are shown because the GMM has this setting.\nWhat doesn’t work well\nThere’s some tricky bits with this approach.\nIt’s arguably a benefit that the latent state is generated from an unsupervised approach because there is less label bias to overfit on. But we do get a two stage approach and it might be possible to come up with better embeddings if we could create a loss functions to combine the GMM output as well as the autoencoder.\nI am using GMMs because they are general, but they are not perfect. There may very well be embeddings that won’t fit very well.\nIf the autoencoder is trained poorly then we shouldn’t expect much from the gaussians in the middle either. We’re still stuck with training an autoencoder that can be very complex in layers and hyperparameters.\nHowever flexible, the GMMs also need hyperparameter tuning, which can be expensive to do well.\nThe bigger the latent state, the easier it will be for the autoencoder. The smaller the latent state, the easier it will be for the GMM. This is a nuisance.\nConclusion\nI should try this problem on a harder problem, but sofar I’m pretty happy with the results. There’s some things I can do to make the transitions between the classes better but thats something for another day.\nIt is very liberating to use python tools like lego piecies, which is exactly what I’ve been doing here. Put a encoder here, place a gaussian there … the experimentation is liberating. It’s never been easier to do this.\nBut the reason I was able to get here was because I didn’t feel like implementing a popular algorithm. Rather, I wanted to have an attempt at doing some thinking on my own. I see a lot of people skip this mental step in favor of a doing something hip.\nMaybe, just maybe, a lot of folks are doing themselves short by doing this. I may have stumbled apon something useful for a project here and I wouldn’t have gotten here if I would just blindly follow the orders of an academic article.\nMaybe folks should do this more often.\nPlay\nThere are two notebooks that contain all code. One with mnist data and fashion mnist data. Feel free to play with them and let me know if I’ve made horrible errors.\n\n\n\n",
    "preview": "posts/gaussian-auto-embeddings/gaussian-auto-embeddings_files/autoencoder3.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1274,
    "preview_height": 600
  },
  {
    "path": "posts/amdahls-law/",
    "title": "Amdahl's Law",
    "description": "The more CPU's you add, the worse it gets.",
    "author": [
      {
        "name": "Vincent D. Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2018-06-14",
    "categories": [],
    "contents": "\nI was watching this video and the part of Amdahl’s Law got me thinking. It felt counterintuitive but it explains many benchmarks I’ve seen of multicore programs. In this blogpost I’ll very briefly explain what it is and why it’s implications are important (not to mention; very unfortunate) when you are attempting parallel programming.\nThe Maths\nNot all algorithms can be written in parallel form. Algorithms often need some amount of time to sync between parts. One might be able to design a parallel map step, but you’ll have idle CPUs during the reduce step. It turns out that even if your program cannot use all CPU’s for a small amount of time that this has huge effects on resource utility.\nThe maths turns out to be relatively easy to derive. If you consider that \\(p\\) percent of the time we’re only effectively using 1 core and \\((1-p)\\) percent of the time \\(k\\) cores can be used effectively then the speedup is defined by the following function;\n\\[ f(p, k) = \\frac{\\text{speedup 1 core}}{\\text{speedup  } k \\text{  cores}} = \\frac{1}{\\frac{p}{1} + \\frac{(1-p)}{k}} \\]\nThe Intuition\nThe maths doesn’t make everything intuitive though so to make it more intuitive though, let’s chart this and play with the numbers.\n\na probability of   of needing to sync [blue line] has an effectiveness of about /16, about /32 and about /64.\n\na probability of   of needing to sync [red line] has an effectiveness of about /16, about /32 and about /64.\nThe first time I saw this it seemed counter intuitive; but the results are fairly drastic. Even if you have a 95% parallel program then you can still expect to have very poor resource utilisation. Cloud CPUs tend to scale linearly so it can be very hard to escape this problem by throwing more money at it. The effectiveness decreases asymptotically.\nThe Lesson\nYou’ll probably have noticed the phenomenon before. When programming multicore programs the effectiveness of more CPUs becomes marginal as you add more and more of them. Instead of hoping for a speedup, it might be better to consinder costs instead. If you’re willing to wait, it is much more effective to pay for a smaller machine.\n\n\nfunction setUpTangle () {\n\n    var element = document.getElementById(\"example\");\n\n    var tangle = new Tangle(element, {\n        initialize: function () {\n            this.prob1 = 10;\n            this.prob2 = 20;\n        },\n        update: function () {\n            this.p1 = this.prob1\n            this.p2 = this.prob2\n            this.speedup16_blue = Math.round(1/(this.p1/1000 + (1-this.p1/1000)/16));\n            this.speedup16_red  = Math.round(1/(this.p2/1000 + (1-this.p2/1000)/16));\n            this.speedup32_blue = Math.round(1/(this.p1/1000 + (1-this.p1/1000)/32));\n            this.speedup32_red  = Math.round(1/(this.p2/1000 + (1-this.p2/1000)/32));\n            this.speedup64_blue = Math.round(1/(this.p1/1000 + (1-this.p1/1000)/64));\n            this.speedup64_red  = Math.round(1/(this.p2/1000 + (1-this.p2/1000)/64));\n            update_chart(this.prob1, this.prob2);\n        }\n    });\n}\n\nTangle.formats.percent = function (value) {\n    return \"\" + value/1000;\n};\n\nvar xvals = d3.range(64+1),\n    data = _.zip(xvals, xvals, xvals).map(function(d){ return {x: d[0], y: d[1], z: d[2]}});\n\ndata.forEach(function(d,i,l){\n    d.y = d3.max([0, 100*(d.x-3)]);\n    d.z = d3.max([0, 200*(d.x-7)]);\n})\n\nvar svg = d3.select(\"svg\"),\n    margin = {top: 20, right: 100, bottom: 30, left: 50},\n    width = document.getElementById(\"chart\").clientWidth - margin.left - margin.right,\n    height = document.getElementById(\"chart\").clientHeight - margin.top - margin.bottom,\n    g = svg.append(\"g\").attr(\"transform\", \"translate(\" + margin.left + \",\" + margin.top + \")\");\n\nvar x = d3.scaleLinear().rangeRound([0, width]);\nvar y = d3.scaleLinear().rangeRound([height, 0]);\n\nvar early_line = d3.line()\n    .x(function(d) { return x(d.x); })\n    .y(function(d) { return y(d.y); });\n\nvar later_line = d3.line()\n    .x(function(d) { return x(d.x); })\n    .y(function(d) { return y(d.z); });\n\nx.domain(d3.extent(data, function(d) { return d.x }));\n\nvar max_y = 64;\n\ny.domain([0, max_y]);\n\nvar axis = d3.axisBottom(x);\naxis.ticks(xvals.length/8);\n\ng.append(\"g\")\n  .attr(\"transform\", \"translate(0,\" + height + \")\")\n  .call(axis)\n  .append(\"text\")\n  .attr(\"fill\", \"#000\")\n  .attr(\"y\", -10)\n  .attr(\"x\", width - 20)\n  .attr(\"dy\", \"0.71em\")\n  .attr(\"text-anchor\", \"end\")\n  .text(\"cores given to the program\");\n\nvar y_axis_group = g.append(\"g\")\n  .call(d3.axisLeft(y))\n  .append(\"text\")\n  .attr(\"fill\", \"#000\")\n  .attr(\"transform\", \"rotate(-90)\")\n  .attr(\"y\", 6)\n  .attr(\"dy\", \"0.71em\")\n  .attr(\"text-anchor\", \"end\")\n  .text(\"speedup of program\");\n\ng.append(\"path\")\n  .attr(\"class\", \"early_line\")\n  .datum(data)\n  .attr(\"fill\", \"none\")\n  .attr(\"stroke\", \"steelblue\")\n  .attr(\"stroke-linejoin\", \"round\")\n  .attr(\"stroke-linecap\", \"round\")\n  .attr(\"stroke-width\", 2.5)\n  .attr(\"d\", early_line);\n\ng.append(\"path\")\n  .attr(\"class\", \"later_line\")\n  .datum(data)\n  .attr(\"fill\", \"none\")\n  .attr(\"stroke\", \"crimson\")\n  .attr(\"stroke-linejoin\", \"round\")\n  .attr(\"stroke-linecap\", \"round\")\n  .attr(\"stroke-width\", 2.5)\n  .attr(\"d\", later_line);\n\nvar update_chart = function(prob1, prob2){\n    data.forEach(function(d,i,l){\n        d.y = 1/(prob1/1000 + (1-prob1/1000)/d.x);\n        d.z = 1/(prob2/1000 + (1-prob2/1000)/d.x);\n    })\n\n    svg.select(\"path.early_line\").attr(\"d\", early_line(data));\n    svg.select(\"path.later_line\").attr(\"d\", later_line(data));\n}\n\n\n\n",
    "preview": "posts/amdahls-law/amdahls-law_files/chart.png",
    "last_modified": "2022-07-27T22:10:59+02:00",
    "input_file": {},
    "preview_width": 2048,
    "preview_height": 910
  },
  {
    "path": "posts/feed-forward-posteriors/",
    "title": "Feed Forward Posteriors",
    "description": "Combine the Neural with the Normal.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2017-11-01",
    "categories": [],
    "contents": "\nIn this document I will demonstrate density mixture models. The goal for me was to familiarize myself with tensorflow a bit more but it grew to a document that compares models too. The model was inspired from this book by Christopher Bishop who also wrote a paper about it in 1994. I’ll mention some code in the post but if you feel like playing around with the (rather messy) notebook you can find it here.\nDensity Mixture Networks\nA density mixture network is a neural network where an input \\(\\mathbf{x}\\) is mapped to a posterior distribution \\(p(\\mathbf{y} | \\mathbf{x})\\). By enforcing this we gain the benefit that we can have some uncertainty in our prediction (assign a lot of doubt for one prediction and a lot of certainty for another one). It will even give us the opportunity to suggest that more than one prediction is likely (say, this person is either very tall or very small but not medium). The main trick that facilitates this is the final hidden layer which can be split up into three different parts:\na \\(\\boldsymbol{\\mu}\\) layer with nodes \\(\\{\\mu_1 ... \\mu_k\\}\\) which will denote the mean of a normal distribution\na \\(\\boldsymbol{\\sigma}\\) layer with nodes \\(\\{\\sigma_1 ... \\sigma_k\\}\\) which will denote the deviation of a normal distribution\na \\(\\boldsymbol{\\pi}\\) layer with nodes \\(\\{\\pi_1 ... \\pi_k\\}\\) which will denote the weight of the associated normal distribution in the resulting prediction\nThe idea is that the final prediction will be a probability distribution that is given by the trained output nodes via; \\[ p(\\mathbf{y} | \\mathbf{x}) = \\sum_{i=1}^k \\pi_i \\times N(\\mu_i, \\sigma_i) \\]\nGraphically, and more intuitively, the network will look something like:\n\nAll the non-coloured nodes will have tahn activation functions but to ensure that the result is actually a probability distribution we will enforce this neural network to:\nensure that \\(\\sum_i \\pi_i = 1\\) at all times, we can use a softmax activation to enforce that\nensure that every \\(\\sigma_i > 0\\), we can use an exponential activation to enforce that\nNote that the architecture we have is rather special. Because we assign nodes to probibalistic meaning we enforce that the neural network gets some bayesian properties.\nImplementation\nThe implementation is relatively straightforward in tensorflow. To keep things simple, note that I am using the slim portion of contrib.\nimport tensorflow as tf \n\n# number of nodes in hidden layer\nN_HIDDEN = [25, 10]\n# number of mixtures\nK_MIX = 10\n\nx_ph = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_ph = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\nnn = tf.contrib.slim.fully_connected(x_ph, N_HIDDEN[0], activation_fn=tf.nn.tanh)\nfor nodes in N_HIDDEN[1:]:\n    nn = tf.contrib.slim.fully_connected(nn, nodes, activation_fn=tf.nn.tanh)\n\nmu_nodes = tf.contrib.slim.fully_connected(nn, K_MIX, activation_fn=None)\nsigma_nodes = tf.contrib.slim.fully_connected(nn, K_MIX, activation_fn=tf.exp)\npi_nodes = tf.contrib.slim.fully_connected(nn, K_MIX, activation_fn=tf.nn.softmax)\n\nnorm = (y_ph - mu_nodes)/sigma_nodes\npdf = tf.exp(-tf.square(norm))/2/sigma_nodes\nlikelihood = tf.reduce_sum(pdf*pi_nodes, axis=1)\nlog_lik = tf.reduce_sum(tf.log(likelihood))\n\noptimizer = tf.train.RMSPropOptimizer(0.01).minimize(-log_lik)\ninit = tf.global_variables_initializer()\nExperiments\nWith the implementation ready, I figured it would be nice to generate a few odd datasets to see how the architecture would hold. Below you will see five charts for four datasets.\nThe first chart shows the original dataset which was sampled via numpy.\nThe second chart shows the original dataset with samples from the predicted distribution. We first sample \\(x\\) to get the associated values at the \\(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}\\) and \\(\\boldsymbol{\\pi}\\) nodes in the network. To get a sample from \\(p(y|x)\\) we first decide which normal distribution \\(N(\\mu_i, \\sigma_i)\\) to sample from using the weights \\(\\boldsymbol{\\pi}\\).\nThe third plot shows the value for the different \\(\\mu_i\\) nodes given certain values of input \\(x\\).\nThe fourth plot shows the value for the different \\(\\pi_i\\) nodes given certain values of input \\(x\\). Note that for every \\(x\\) we hold that \\(\\sum_i \\pi_i = 1\\).\nThe fifth plot shows the value for the different \\(\\sigma_i\\) nodes given certain values of input \\(x\\).\n\n\n\n\nCriticism\nWhen looking at these charts we seem to be doing a few things right:\nOur predicted distribution is able to recognize when a prediction needs to be multi-peaked\nOur predicted distribution is able to understand area’s with more or less noise\nWe can confirm that our \\(\\mu_i\\) lines correspond to certain shapes of our original data.\nIf you look carefully though you could also spot two weaknesses.\nWeakness 1: very large sigma values\nThe values for sigma can suddenly spike to unrealistic heights, this is mainly visable in the fourth plot. I’ve introduced some regularisation to see if it helps. The model seems to improve, not just in the \\(\\sigma\\)-space but also in the \\(\\mu\\)-space we are able to see more smooth curves.\n\nThe regularisation can help, but since \\(\\pi_i\\) can be zero (which cancels out the effect of \\(\\sigma_i\\)) you may need to regulise drastically if you want to remove it all together. I wasn’t able to come up with a network regulazier that removes all large sigma values, but I didn’t care too much because I didn’t see it back posterior output (because in those cases, one can imagine that \\(\\pi_i \\approx 0\\)).\nWeakness 2: samples around the edges\nOur model is capable of sampling wrong numbers around the edges of the \\(x\\)-space, this is because \\(\\sum_i \\pi_i = 1\\) which means that for every \\(x\\) the likelihood must be zero. To demonstrate the extremes consider these samples from the previous models;\n\nNote how the model seems to be drawing weird samples around the edges of the known samples. There’s a blob of predicted orange where there are no blue datapoints to start with.\nBecause \\(\\sum \\pi(x) = 1\\) we are always able to generate data in regions where there really should not be any. You can confirm this by looking at the \\(\\mu\\) plots. We could append this shortcomming by forcing that \\(\\sum \\pi(x) = 0\\) if there is no data near \\(x\\). We could “fix” this by appending the softmax part of the \\(\\pi_i\\) nodes with a sigmoid part.\nThis can be done, but it feels like a lot of hacking. I decided not to invest time in that and instead considered comparing mixture density networks to their simpler counterpart; gaussian mixture models.\nSomething Simple?\nIt is fashionable thing to try out a neural approach these days, but if you would’ve asked me the same question three years ago with the same dataset I would’ve proposed to just train a gaussian mixture model. That certainly seems like a right thing to do back then so why would it be wrong now?\nThe idea is that we throw away the neural network and that we train \\(K\\) multivariate gaussian distributions to fit the data. The great thing about this approach that we do not need to implement it in tensorflow either since scikit learn immediately has a great implementation for it.\nfrom sklearn import mixture\nclf = mixture.GaussianMixture(n_components=40, covariance_type='full')\nclf.fit(data)\nWith just that bit of code, very quickly train models on our original data too.\n\n\n\nThe model trains very fast and you get the eyeball impression that it fits the data reasonably.\nSimple Comparison\nLet’s zoom in on predictions from both models to see how different they are.\nIt should be said that we’ll be making comparisons between two approaches without any hypertuning (or even proper convergence checking). This is not very appropriate academically, but it may demonstrate the subtle differences in output better (as well as simulate how industry users will end up applying these models). Below we’ll list predictions from both models.\nIn each case, we’ll be given an \\(x\\) value and we want to predict the \\(y\\) value. The orange line is the neural approach and the blue line is from the scikit model. I’ve normalized both likelihoods on the same interval in order to compare them better.\n\n\n\nIt’s not exactly a full review but just from looking at this we can see that the models show some modest differences. The neural approach seems to be more symmetric (which is correct; I sampled the data that way) but it seems to suffer from unfortunate jitter/spikyness at certain places. More data/regularisation might fix this issue.\nOther than that, I would argue the normal scikit learn model is competitive simply because it trains much faster which could make it feasible to do a grid search on the number of components relatively quickly.\nConclusion\nBoth methods have it’s pros and cons, most notably;\nThe neural approach takes much longer to train and you need to pay some attention to check that your model has actually converged.\nBoth approaches can be tuned further but it seems the neural approach has more dials you can tweak because of the number of hidden neurons/activations.\nPicking the appropriate number of GMM components feels tricky in general, picking the appropriate hidden layer sizes feels less risky because \\(\\pi()\\) nodes can cancel out unwanted effects. One can add too many nodes/layers and still be able to live with it (albeit less ideal).\nIf you sample from the neural approach from a region that does not occur in the original dataset you should expect crazy results. You will get a similar effect from a GMM if you enforce it to sample from a region of extremely low likelihood.\nThe general GMM allows you to infer from \\(x \\to y\\) as well as \\(y \\to x\\) where the basic neural approach does not allow this out of the box.\nWe cannot properly infer the likelihood of being at \\((x,y)\\) with the neural approach either because in a region where the likelihood should be near zero we still enforce \\(\\sum_i \\pi_i = 1\\). So maybe we need to conjure up another network structure to accomodate this but this may require a bit of ugly hacking.\nWe didn’t expand to higher dimensions, but as long as we infer from \\(\\mathbf{x} \\to \\mathbf{y}\\) and if \\(y_i\\) has dimension 1 our neural approach is still applicable. If this is not the case we may need to invest in encoding co-variates as well.\nOne can imagine that the neural approach offers more flexibility in latent space to learn complicated relationships that might not be modelled with a mere combination of gaussians. Personally, I’m having trouble imagining an obvious example though.\nAgain, if you feel like playing around with the code, you can find it here.\n\n\n\n",
    "preview": "posts/feed-forward-posteriors/feed-forward-posteriors_files/mixture-network-0.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 972,
    "preview_height": 430
  },
  {
    "path": "posts/passive-agressive-algorithms/",
    "title": "Passive Agressive Algorithms",
    "description": "A VeryGood[tm] name for a VeryGood[tm] Algorithm.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2017-10-10",
    "categories": [],
    "contents": "\nIn a previous document I described how bayesian models can recursively update, thus making them ideal as a starting point for designing streaming machine learning models. In this document I will describe a different method proposed by Crammer et al. which includes a passive agressive approach to model updates. I will focus on intuition first before moving to the mathy bits. At the end I will demo some sklearn code with implementations of these models.\nIntuition\nStep 1\nLet’s say you’re doing a regression for a single data point \\(d_i\\). If you only have one datapoint then you don’t know what line is best, despite this fact we can come up with lines that will fit the data point perfectly. For example; The yellow line will pass through the point perfectly and so will the blue line. There are many lines we could come up with but we’ll keep these two in mind.\nStep 2\nWe can describe all the lines that will go through the line perfectly by describing the lines in terms of the slope and intercept. If we make a plot of the weight space of the linear regression (\\(w_0\\) is the constant and \\(w_1\\) is the slope) we can describe all the possible perfect fits with a line. You’ll note that our original blue line is now denoted with a blue dot, but we are referring to the same thing. Same for the yellow line and yellow dot.\nStep 3\nAny point on that line is as good as far as \\(d_i\\) is concerned, so how might we go about selecting the optimal one? How about we compare it to the weights that the regression had before it came across this one point? Let’s call these weights \\(w_{\\text{orig}}\\) and in the case of this being the first datapoint we might imagine that \\(w_{\\text{orig}}\\) is in the origin. In that case the blue regression seems better than the yellow one but is it the best choice?\nStep 4\nThis is where we can use maths to find the coordinate on the line that is as close as our original weights \\(w_{\\text{orig}}\\). In a very linear system you can get away with linear algebra but depending on what you are trying to do you may need to introduce more and more maths to keep the update rule consistent.\nStep 5\nTo prevent the system from becomming numerically unstable we may also choose to introduce a limit on how large the step size may be (no larger than \\(C\\)). This way, we don’t massively overfit to outliers. Also, we probably only want to update our model if our algorithm makes a very large mistake. We can then make an somewhat agressive update and remain passive at other times. Hence the name! Note that this approach will for slightly different for system that do linear classification but the idea of passive agressive updating can still be applied.\nFor those who are interested in the formal maths: check the appendix.\nCode\nIt should be relatively easy to implement this algorithm but you don’t need to because scikit learn has support for this algorithm. Scikit learn is great; be sure to thank people who contribute to the project.\nRegression\nFor the regression task let’s compare how well the algorithm performs on some simulated data. We will compare it to a normal, batch oriented, linear regression.\nimport numpy as np \nimport sklearn \nfrom sklearn.datasets import make_regression\nX, y, w = make_regression(n_features=3, n_samples=2000, random_state=42, coef=True, noise=1.0)\n\nmod_lm = sklearn.linear_model.LinearRegression()\nmod_lm.fit(X, y)\nbatch_acc = np.abs(mod_lm.predict(X) - y).sum()\n\nstart_c = <value>\nwarm_c = <value>\n\nmod_pa = sklearn.linear_model.PassiveAggressiveRegressor(C=start_c, warm_start=True)\nacc = []\ncoefs = []\nfor i, x in enumerate(X):\n    mod_pa.partial_fit([x], [y[i]])\n    acc.append(np.abs(mod_pa.predict(X) - y).sum())\n    coefs.append(mod_pa.coef_.flatten())\n    if i == 30:\n        mod_pa.C = warm_c\nYou’ll notice in the code that I’ve added a starting value for \\(C\\) (start_c) and a value for when it has partly converged (warm_c). This code for illustration, as it is a strange assumption that the algorithm is “warm” after 30 iterations.\nYou can see in the plots below what the effect of this is.\n\\[c_{\\text{start}} = 1, c_{\\text{warm}} = 1\\]\n\n\nimgs <- c(\"passive-agressive-algorithms_files/passive-agressive-06.png\", \n          \"passive-agressive-algorithms_files/passive-agressive-07.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nThe first plot shows the mean squared error over the entire set after the regression has seen more data. The orange line demonstrates the baseline performance of the batch algorithm. The second plot demonstrates how the weights change over time. In this case you can confirm that the MSE fluctuates quite a bit. \n\\[c_{\\text{start}} = 0.1, c_{\\text{warm}} = 0.1\\]\n\n\nimgs <- c(\"passive-agressive-algorithms_files/passive-agressive-10.png\", \n          \"passive-agressive-algorithms_files/passive-agressive-11.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nThe fluctuations are small, but the algorithm seems to need a lot of data before the regression starts to become sensible. \n\\[c_{\\text{start}} = 3, c_{\\text{warm}} = 0.1\\]\n\n\nimgs <- c(\"passive-agressive-algorithms_files/passive-agressive-08.png\", \n          \"passive-agressive-algorithms_files/passive-agressive-09.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nYou can now see that the fluctuations are still very small but the large steps that are allowed in the first few iterations ensure that the algorithm can converge a bit globally before it starts to limit itself to only local changes. \nClassification\nWe can repeat this exercise for classification too.\nimport numpy as np \nimport sklearn \nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=4000, n_features=2, n_redundant=0, \n                           random_state=42, n_clusters_per_class=1)\nmod_lmc = sklearn.linear_model.LogisticRegression()\nnormal_acc = np.sum(mod_lmc.fit(X, y).predict(X) == y)\n\nstart_c = <value>\nwarm_c = <value>\n\nmod_pac = sklearn.linear_model.PassiveAggressiveClassifier(C=start_c)\nacc = []\ncoefs = []\nfor i, x in enumerate(X):\n    mod_pac.partial_fit([x], [y[i]], classes=[0,1])\n    coefs.append(mod_pac.coef_.flatten())\n    acc.append(np.sum(mod_pac.predict(X) == y))\n    if i == 30:\n        mod_pac.C = warm_c\nThe code is very similar, the only difference is that we are now working on a classification problem.\n\\[c_{\\text{start}} = 1, c_{\\text{warm}} = 1\\]\n\n\nimgs <- c(\"passive-agressive-algorithms_files/passive-agressive-12.png\", \n          \"passive-agressive-algorithms_files/passive-agressive-13.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nThe first plot denotes performance again but this time it is measured by accuracy. The second plot shows the weights again. In the current setting you see a lot of noise and the performance is all over the place. The effect seems greater than with the regression when \\(c_{\\text{start}} = 1, c_{\\text{warm}} = 1\\). This is because the optimal weights are a lot smaller. In the regression case they were around 30-40 while here they are around 2.5 and 1. This means that a maximum step size of 1 is relatively seen very large. \n\\[c_{\\text{start}} = 0.1, c_{\\text{warm}} = 0.1\\]\n\n\nimgs <- c(\"passive-agressive-algorithms_files/passive-agressive-16.png\", \n          \"passive-agressive-algorithms_files/passive-agressive-17.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nThe performance is much better, especially when you consider that the y-axes on the charts are different. \nIt seems like this \\(C\\) hyperparameter is something to keep an eye on if these sorts of algorithms are within your interest.\nConclusion on Applications\nIt is nice to see that we’re able to have a model work in a streaming setting. I would wonder how often this is useful though, mainly because you need a label to be available in streaming to actually be able to learn from it. One can wonder, if the label is available at streaming then why have the need to predict it?\nThen again, all passive agressive linear models have a nice and simple update rule and this can be extended to include matrix factorisation models. For large webshops this is very interesting because it suggests a method where you may be able to update per click. For a nice hint at this, check out this presentation from flink forward.\nConclusions on Elegance\nThe bayesian inside of me can’t help but see an analogy to something bayesian happening here too. We update our prior belief (weights at time \\(n\\)) if we see something that we did not expect. By doing this repeatedly we come to a somewhat recusive model that feels very similar to bayesian updating.\n\\[ p(w_{n+1}| D, d_{n+1}) \\propto p(d_{n+1} | w_n) p(w_n)\\]\nBayesian updating seems more appealing because we get to learn from every datapoint which also has a regularisation functionality (it removes both the passive and the agressive aspect from the update rule). The main downside will be the datastructure though, which is super easy with this passive aggressive approach.\nThe Maths\nHopefully the intuition makes sense by now which means we can discuss some of the formal maths. Feel free to skip if it does not appeal to you. Again, you can find more details in the original paper found here.\nWe will now discuss linear regression and I’ll leave the classification case up to the paper, but the proofs are very similar. In these tasks we are looking for new weights \\(\\mathbf{w}\\) while we currently have weights \\(\\mathbf{w}_t\\). We will assume that the current timestep we have a true label \\(y_t\\) that belongs to the data we see \\(x_t\\).\nLinear Regression\nWe have a new point at time \\(t+1\\) that we’ve just observed and we’re interested in making the smallest step in the correct direction such that we perfectly fit the new point. In maths we’d like to find \\(\\mathbf{w}_{t+1}\\) that is defined via;\n\\[ \\mathbf{w}_{t+1} = \\text{argmin} \\frac{1}{2} ||\\mathbf{w} - \\mathbf{w}_t||^2 \\]\nWe do want to make sure that we adhere to our constraint, this new datapoint needs to be fitted perfectly such that \\(l(\\mathbf{w}; (\\mathbf{x}_t, y_t)) = 0\\).\nHere \\(l(\\mathbf{w}; (\\mathbf{x}_t, y_t))\\) is the loss function of the regression. Let’s define that more properly. The idea is that we only update our system when our prediction makes an error that is too big.\nIf the error margin is greater than \\(\\epsilon\\);\n\\[ l_t^R = l(\\mathbf{w}; (\\mathbf{x}_t, y_t)) = |\\mathbf{w} \\mathbf{x}_t - y_t| - \\epsilon \\]\nTo optimise these systems we’re going to use a lagrangian trick. It means that we are going to introduce a parameter \\(\\tau\\) which is meant to be a ‘punishment variable’. This will relieve us of our constraint because we will pull into the function that we are optimising. If we search in a region that we cannot use we will be punished by a value of \\(\\tau\\) for every unit of constraint violation.\nWith this trick, we can turn our problem into this;\n\\[ L^R(\\mathbf{w}, \\tau) =  \\frac{1}{2} ||\\mathbf{w} - \\mathbf{w}_t||^2 + \\tau l_t^R \\]\nNow in order to optimise we will start with a differentiation.\n\\[ \\frac{\\delta L^R}{\\delta \\mathbf{w}} =  \\mathbf{w} - \\mathbf{w}_t + \\tau \\mathbf{x}_t \\frac{\\mathbf{w}\\mathbf{x}_t - y_t}{|\\mathbf{w}\\mathbf{x}_t - y_t|} = 0 \\]\n\\[ \\mathbf{w} = \\mathbf{w}_t - \\tau \\mathbf{x}_t \\frac{\\mathbf{w}\\mathbf{x}_t - y_t}{|\\mathbf{w}\\mathbf{x}_t - y_t|} \\]\nWe now have an optimum for \\(\\mathbf{w}\\) but we still have a variable \\(\\tau\\) hanging around. Let’s use our newfound \\(\\mathbf{w}\\) and replace it in \\(L^R(\\mathbf{w}, \\tau)\\).\n\\[\\begin{equation} \\label{eq1}\n\\begin{split}\nL^R(\\tau) & = \\frac{1}{2}\\left(-\\tau \\mathbf{x}_t \\frac{\\mathbf{w}\\mathbf{x}_t - y_t}{|\\mathbf{w}\\mathbf{x}_t - y_t|}\\right)^2 + \\tau |\\mathbf{w} \\mathbf{x}_t - y_t| \\\\\n & = \\frac{1}{2}\\tau^2||\\mathbf{x}_t||^2 + \\tau |\\mathbf{w} \\mathbf{x}_t - y_t|\n\\end{split}\n\\end{equation}\\]\nOnce again, we have something we can differentiate. If we differentiate and solve we get the optimal value for \\(\\tau\\).\n\\[ \\tau = -\\frac{|\\mathbf{w} \\mathbf{x}_t - y_t|}{||\\mathbf{x}_t||^2} = \\frac{l_t^R}{||\\mathbf{x}_t||^2}\\]\nYou can even introduce a maximum stepsize \\(C\\) if you want.\n\\[\\mathbf{w}^* = \\mathbf{w}_t - \\tau \\mathbf{x}_t \\frac{\\mathbf{w}\\mathbf{x}_t - y_t}{|\\mathbf{w}\\mathbf{x}_t - y_t|}\\] \\[ \\tau_t^* = \\text{min} \\left(C, \\frac{l_t^R}{||\\mathbf{x}_t||^2} \\right) \\]\nWe have closed from solutions! For a more formal deep dive I’ll gladly refer you to the original work.\nMaybe\nI think the proof might’ve been simpler with mere linear algebra, it feels like this is just a projection.\n\n\n\n",
    "preview": "posts/passive-agressive-algorithms/passive-agressive-algorithms_files/passive-agressive-01.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 332,
    "preview_height": 284
  },
  {
    "path": "posts/vary-very-optimally/",
    "title": "Vary Very Optimally",
    "description": "How to search in search space.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2017-08-29",
    "categories": [],
    "contents": "\nThis document was inspired by this blogpost by Ferenc Huszár which was in turn inspired by this paper by Wiersta et al and this paper by Barber et al. I was a fascinated by all of this and I just had to reproduce it myself. I’m sharing some personal findings based on that work here but any proper attribution should also link to the previously mentioned documents as I am largely just repeating their work.\nProblem of Hard Functions\nSometimes we are interested in optimising functions that are hard to optimise. This happens often in machine learning but also in decision science. Here are two examples of functions (\\(\\text{sinc}(x)\\) and \\(\\lfloor 10\\text{sinc}(x) + 4\\sin(x)\\rfloor\\)) that could be seen as hard.\n\n\n\nBoth functions have mulitiple peaks which makes them hard to approach via gradient descent. To make things worse, the right chart is not differentiable and has a very thin peak at the top which is hard to find.\nInstead of searching in a one dimensional space though, how about apply a smoothing factor and turn the gradient descent from a one dimensional problem into a two dimensional problem?\nThe plots below translate our original one-dimensional hard functions into easier two-dimensional spaces.\n\n\n\nWe’ll explain how the smoothing works in just a bit but try to pay close attention to what the consequences are of such a smoothed space:\nwe are now optimising a landscape \\(g(x,s)\\) where \\(s\\) is the amount of smoothing we apply\nwhen \\(s=0\\) then we define \\(g(x, 0) = f(x)\\)\nwhen \\(s>>0\\) then \\(g(x, s)\\) starts to resemble an average of sorts\nif we start a gradient descent algorithm in a region where the smoothing parameter is high it is likely that we move towards the actual optimum of a region\nif we start a gradient descent algorithm in a region where \\(s \\approx 0\\) then we are also able to escape the local optima because the gradient search might be interested in searching in a space where \\(s\\) is larger\nThe plots below show how the gradient descent in this space causes us to avoid local optima.\n\n\n\nI hope that by now, we’re all very exited about this special smoother.\nHow to Smooth\nThere are many ways to look at our special smoother. Some people might call it a convolution while others might call it a variational bound. Before delving into the maths of it, I’ll first discuss the idea with images and intuition.\nIntuition\nStep 1\nThe idea is that we take our original function \\(f(x)\\) (blue) and we smoothe it around some value of \\(x\\) with a gaussian that is distributed \\(N(\\mu=x, \\sigma)\\) (orange). In the example here we take \\(\\mu=x=0\\) and \\(\\sigma=1\\). Note that the orange line is not drawn to scale, but we need it to illustrate a point.\nStep 2\nThe idea now is that we smooth our function \\(f(x)\\) by multiplying it by the distribution of \\(N(\\mu=x=0, \\sigma=1)\\). Points that are closer to \\(x\\) will have more influence on the smoothed value, especially when \\(\\sigma\\) is small. We now plot the \\(f(x)N(\\mu, \\sigma)\\) with a green line. Notice that it indeed seems to smoothe our original \\(f(x)\\) function.\nStep 3\nThis green line is cool and all, but we would like this green line to be summarised into a single number. We are looking for a two dimensional function with a smoothing factor \\(s\\) and an \\(x\\) value such that we can perform gradient descent on it. How about we integrate the green area that exists between the green line and the x-axis? That area can be interpreted as a gaussian average of \\(f(x)\\) given \\(\\sigma\\).\nStep 4\nThe integral we just calculated is the value we find in our gradient search space at \\(x=0\\) and \\(\\sigma=1\\). I’ve drawn the location of this pixel in the image to emphesize. Typically we don’t need to generate the entire search space though, we merely need to be able to sample from it for our gradient.\nEffect\nIt helps to look at things from different perspectives so let’s double check what the effect of our smoothing parameter \\(\\sigma\\) is on the function that we are performing a grid search on.\n\n\n\nEach plot shown shows the original function in blue and the smoothed function in dark red. The plot shows similar information as the 2d plot we saw in step 4 but this plot assumes that the smoothing factor is constant. You can see that when the smoothing factor increases we smoothe all the values closer to zero but we still have a glimpse of the actual value.\nAnother way of thinking about this smoothing function is that it is an approximation of our original function.\nEnter Maths\nThe maths surrounding this topic feel like poetry to me, but feel free to skip it if this doesn’t apply to you.\nOur gaussian average can be seen as an estimator for the original function \\(f(x)\\). It is the expected value of \\(f(x)\\) given a value of \\(\\mu\\) and \\(\\sigma\\).\n\\[ \\min f(x) \\leq \\min \\mathbb{E}_{x \\sim p(x|\\mu, \\sigma)}[f(x)] = \\min g(\\mu, \\sigma, f)\\]\nThe idea is that we optimise \\(g(\\mu, \\sigma, f)\\) which will serve as an upper bound for \\(f(x)\\). This means that if we are able to perform a proper gradient search of \\(g\\) then we should be able to find interesting candidates for optimal values of \\(f(x)\\).\nThis means we need to do some maths surrounding the gradient. Let’s combine our \\(\\mu\\) and \\(\\sigma\\) into a single parameter \\(\\theta\\). Then;\n\\[\n\\begin{split}\n\\frac{d}{d\\theta} \\mathbb{E}_{x \\sim p(x|\\theta)} f(x) &= \\frac{d}{d\\theta}\\int p(x|\\theta)f(x)dx \\\\\\\\\n &= \\int \\frac{d}{d\\theta} p(x|\\theta)f(x)dx\n\\end{split}\n\\]\nSo far so good. Let’s now apply a trick. We will rewrite the contents of the last integral in a bit. To understand why we are able to do that, please remember the chain rule of derivatives;\n\\[\n\\frac{d}{dx} f(g(x)) = g'(x) f'(g(x))\n\\]\nWe will now apply the chain rule on the following statement.\n\\[ \n\\frac{d}{d\\theta} \\log p(x|\\theta) = \\frac{\\frac{d}{d\\theta}p(x|\\theta)}{p(x|\\theta)}\n\\]\nYou may begin to wonder why this is such a useful step. Hopefully it comes clear when we multiply both sides by \\(p(x|\\theta)\\).\n\\[\n\\begin{split}\n\\frac{d}{d\\theta} \\log p(x|\\theta) & = \\frac{\\frac{d}{d\\theta}p(x|\\theta)}{p(x|\\theta)} \\\\\\\\\np(x|\\theta) \\frac{d}{d\\theta} \\log p(x|\\theta) & = p(x|\\theta) \\frac{\\frac{d}{d\\theta}p(x|\\theta)}{p(x|\\theta)} \\\\\\\\\np(x|\\theta) \\frac{d}{d\\theta} \\log p(x|\\theta) & = \\frac{d}{d\\theta} p(x|\\theta)\n\\end{split}\n\\]\nLet us now use this to continue our train of thought.\n\\[\n\\begin{split}\n\\frac{d}{d\\theta} \\mathbb{E}_{x \\sim p(x|\\theta)} f(x) &= \\frac{d}{d\\theta}\\int p(x|\\theta)f(x)dx \\\\\\\\\n &= \\int \\frac{d}{d\\theta} p(x|\\theta)f(x)dx \\\\\\\\ \n &= \\int p(x|\\theta)\\frac{d}{d\\theta} \\log p(x|\\theta)f(x)dx\n\\end{split}\n\\]\nThat seems useful, let’s rearrange and conclude!\n\\[\n\\begin{split}\n\\frac{d}{d\\theta} \\mathbb{E}_{x \\sim p(x|\\theta)} f(x) &= \\frac{d}{d\\theta}\\int p(x|\\theta)f(x)dx \\\\\\\\\n&= \\int \\frac{d}{d\\theta} p(x|\\theta)f(x)dx \\\\\\\\ \n&= \\int p(x|\\theta)\\frac{d}{d\\theta} \\log p(x|\\theta)f(x)dx \\\\\\\\ \n&= \\int p(x|\\theta) f(x) \\frac{d}{d\\theta} \\log p(x|\\theta)dx \\\\\\\\\n&= \\mathbb{E}_p \\left< f(x) \\frac{d}{d\\theta} \\log p(x|\\theta) \\right>\n\\end{split}\n\\]\nThe implications of this are pretty interesting because we are very open to whatever function \\(f(x)\\) that we come up with. Also, if we want to estimate the gradient we do not need to calculate the full integral of the expected value; we could also sample it instead. Imagine that we sample lot’s and lot’s of \\(x_s \\sim p(x^s|\\theta)\\) then we can write the gradient update rule as (assuming maximisation):\n\\[\\theta^{\\text{new}} = \\theta + \\frac{\\alpha}{S} \\sum_S f(x^s) \\frac{d}{d\\theta} \\log p(x^s|\\theta)\\]\nFurthermore; we are free to pick whatever smoothing distribution we want but if we pick the gaussian distribution such that \\(\\theta = {\\mu, \\Sigma}\\) we gain some nice closed from solutions.\n\\[ \n\\begin{bmatrix} \\mu \\\\\\\\ \\Sigma \\\\ \\end{bmatrix}^{\\text{new}}\n  = \\begin{bmatrix} \\mu \\\\\\\\ \\Sigma \\\\\\\\\\end{bmatrix} \n    + \\frac{\\alpha}{S} \\sum_S f(x^s) \n     \\begin{bmatrix} \n        \\Sigma^{-1}(x^s-\\mu) \\\\\\\\\n        -\\frac{1}{2}(\\Sigma^{-1} - \\Sigma^{-1}(x^s-\\mu)(x^s-\\mu)^T\\Sigma^{-1}) \\\\\\\\\n     \\end{bmatrix}\n\\]\nIf you are interested in proofs like these, google “Matrix Cookbook”. I didn’t come up with these and neither should you.\nPython Code\nTo proove that this works I’ve written some python code that rougly applies the maths we’ve just discussed.\n\ndef formula_update_orig(f, mu, sigma, alpha=0.05, n=50, max_sigma=10):\n    mu_arr = np.array(mu).reshape(2)\n    dmu, dsigma = np.zeros(shape=mu.shape).T, np.zeros(shape=sigma.shape)\n\n    for i in range(n):\n        xs = np.random.multivariate_normal(mu_arr, sigma)\n        inv_sigma = np.linalg.inv(sigma)\n        dmu += np.matrix(alpha/n * f(xs) * inv_sigma * (xs-mu).T)\n        sigma_part = -alpha/n * f(xs)*(inv_sigma - inv_sigma*(xs-mu).T*(xs-mu)*inv_sigma)/2 \n        dsigma += sigma_part\n    \n    new_mu = (mu.T + dmu).reshape((1,2))\n    new_sigma = sigma + dsigma\n\n    # now comes the dirty numerics to prevent numerical explosions \n    new_sigma[new_sigma > max_sigma] = max_sigma\n    new_sigma[new_sigma > sigma*2] = 0.1\n    new_sigma[new_sigma < 0.05] = 0.05\n    return new_mu, new_sigma\nThe code has some dirty parts because of numerical stability but it serves it’s purpose in demonstrating that we are able to use this tactic on higher dimensional functions too.\nHigher Dimensions\nThe following examples are all two dimensional functions. We plot the actual two dimensional space without any smoothing parameters to show that our method also works in higher dimensions. For every function we see three charts. The first chart is the actualy search space, the second chart contains the search space with the gradient path and the first chart also includes the search space that is defined by \\(\\Sigma\\).\n\\[f(x,y) = \\sqrt{|\\text{sinc}(2x)| * |\\text{sinc}(y)|}\\]\n\n\n\n\\[ f(x,y) = -20\\text{exp}\\left(-0.2*\\sqrt{(0.5*(x^2 + y^2))}\\right)  + \\text{exp}\\left(0.5\\cos(2 \\pi x) + \\cos(2 \\pi y)\\right)\\]\n\n\n\n\\[ f(x,y) = 10 - x^2 - x^y - 10 * \\cos(2 \\pi x) - 10 * \\cos(2 \\pi y)\\]\n\n\n\nNote that the algorithm automatically sweeps the area and converges when it seems that it cannot find better regions.\nConclusions\nAlthough this trick has very pleasing maths to it, there’s a few things that are still worth to note;\nAlthough this trick seems like a viable tactic against local optima we haven’t been given a guarantee of optimality with this approach. If we start our search at the wrong end of the search spectrum with a small \\(\\sigma\\) we are prone to still get stuck in a local optima.\nWhen we increase the dimensionality of our search problem we will also need to increase the amount of samples that we need per gradient step otherwise we risk a bit of bias in our gradient estimation.\nWe are able to parallize parts of the sampling that we do. If each process has a different sampling seed then we can run part of the sampling in parallel.\nImproper parameter tuning is still a potential road to ruin. If the stepsize is too large the system can be very unstable.\nThe approach feels similar to the evolutionary strategies heuristic and simulated annealing except that we’ve thrown a lot of extra maths in the mix such that we’re able to search in our variance/temperature space as well. One can imagine that ES and SA to also have reasonable results on the problems we’ve tried.\nThis method of optimisation seems to be worthwhile if you’ve got a very peaky search space but one that is cheap to evaluate. If it takes a long time to evaluate a function it may be better to try out bayesian optimisation tactics or hyperopt instead.\nIf you want to play with the messy notebook that I used for this blogpost, feel free to play with it.\n\n\n",
    "preview": "posts/vary-very-optimally/vary-very-optimally_files/very-vary-1.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 384,
    "preview_height": 252
  },
  {
    "path": "posts/bayesian-shaving-cream/",
    "title": "Bayesian Shaving Cream",
    "description": "A plausible (and general) method for model selection.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2017-06-14",
    "categories": [],
    "contents": "\nIn this document I will attempt to explain how bayesian samping algorithms give you shaving cream for occrams razor for free. The main idea is to not only look at the posterior distribution of our model parameters but to also compare the likelihood distribution of these parameters to estimate the likelihood of the models themselves. I’ll do my best to keep the maths light and the conclusion needs to be taken with a grain of salt as you can achieve similar conclusions by just properly doing cross validation. Having said that, prepare to be amazed, as getting model comparison for free is no small feat.\nFree Shaving Cream\nUsually when you want to train a model, as a bayesian, you’ll immediately write down the following;\n\\[ p(\\theta | D) \\propto p(D| \\theta)p(\\theta) = \\Pi_i p(d_i | \\theta) p(\\theta)\\]\n\nThis is the maths bit, feel free to skip. Intuintive pictures will follow.\nThis makes sense when you’re looking at parameters in a single model. But how would you go about comparing different models? Well, we could just chug that model choice \\(M\\) as an uncertain variable in next to \\(\\theta\\) as a thing we want to apply inference to. Let’s rewrite \\(\\theta\\) into \\(\\theta_M\\) to keep the formulation clean. Every model has a different set of parameters after all.\n\\[ p(\\theta_m , m| D) \\propto p(D| \\theta_m, m)p(\\theta, m) = \\Pi_i p(d_i | \\theta_m, m) p(\\theta_m, m)\\]\nSo far so good. We still only need to focus in on \\(\\Pi_i p(d_i | \\theta_m, m) p(\\theta_m, m)\\) so not much has changed when you think about it. We could have also declared \\(M = [\\theta_m, m]\\) and our new equation would be strikingly similar to the one we started out with.\n\\[p(M| D) \\propto p(D|M)p(M) = \\Pi_i p(d_i | M) p(M)\\]\nWe could put some bias against complex models into \\(p(M)\\) but this will not be needed as \\(p(D|M)\\) will also prefer simpler models. This can sound very counter intuitive. This is especially true if you come from a train/test machine learning setting and have little experience with the bayesian school of thought. There are a few observations to note when trying to realize why this makes sense.\nAs a bayesian we’re still looking for a distribution \\(p(M| D)\\). This is not a maximum likelihood estimation or the result of a gradient search; this is a distribution we want to infer. This means that we have uncertainty in our parameter space. It is well possible that we come to a model that has a parameter with a very uncertain estimate. This uncertainty will be measureable and trained.\nSince we’re looking for a probability distribution of hyperparameters the model may have a split opinion. There may be two ways to explain an outcome. If this happens the model needs to allocate probability mass across multiple hyperparameters/features. This means that simpler models with simpler explanations will benefit.\nA more complex model, one with more feature/degrees of freedom, risks wasting parameter likelihood mass into parts that do not contribute to a better model. This effect will only be undone if the added complexity makes the model fit the data better.\nWe will demonstrate this with an experiment. In order to understand the setup, consider the following;\nWe can estimate a single likelihood distribution of a model indirectly via MCMC sampling. You can consider a single chain of \\([\\theta_m]\\)-sampled parameters to converge to the true posterior of \\((\\theta_m|D)\\). Every one of these sampled parameters can be mapped to a log-likelihood. You can reduce this back into a histogram to get an impression of how much uncertainty there is in your model.\nYou could do this for all models that you want to consider \\([l(\\theta_{m_1}|D), ..., l(\\theta_{m_k}|D)]\\). You now have multiple distributions of likelihoods of all these models. You now have many likelihood distributions that you could sample from to estimate things like \\(p(M_2 > M_1 | D)\\). By repeating this for all models you can pick the model that fits the data best.\nVisual Explanation\nSuppose that we have our normal MCMC algorithm estimating a model with two parameters. We’d expect to have a timeseries with samples that we can summarise in a histogram.\n\nWe can take the samples of our weights from our model [red]and map it to the log likelihood [blue]. This blue series can be folded into a histogram just like the sampled parameters but it has a slightly different interpretation. This is a distribution of the likelihood of the model.\n\nOnce we have this for one model, we can also do it for another model. Suppose we have another model with three parameters;\n\nWe can make one of these blue histogram for whatever model we can come up with and it is these histograms that we’ll be able to use to compare models.\nAn example\nLet’s pretend that we have three features, four if you include the constant, ready for a linear regression. We don’t know for sure if we need to add all four featuers so we will consider four alternative models.\n\\[\\begin{align}\nH_0 & \\to y \\sim \\beta_0 + N(0, \\sigma_0) \\\\\\\nH_1 & \\to y \\sim \\beta_0 + \\beta_1 x_1 + N(0, \\sigma_1) \\\\\\\nH_2 & \\to y \\sim \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + N(0, \\sigma_2) \\\\\\\nH_3 & \\to y \\sim \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + N(0, \\sigma_3) \n\\end{align}\\]\nLet’s simulate our data in such a way that one of these features should not be included in the model (ie. $_3 = 0 $). The goal is to demonstrate that our model picking technique is robust enough to understand that it should not pick such a model.\nLet’s make some simulated data that will prove our point.\nn = 100 \nb0, b1, b2, b3 = 1, 5, 2, 0\nx1 = np.linspace(1, 10, n)\nx2 = np.sin(x1/4)\nx3 = np.cos(x1/4+1)\ny = b0 + b1 * x1 + b2 * x2 + np.random.normal(0, 0.5, n)\nmodels = {} \nWith the data defined, let’s create some of the models via PyMC3 and lets save all results in an intermediate dictionary. We will use a helper function that takes a trace, casts it to a dataframe and adds the associated loglikelihood to it.\ndef calc_loglik_df(model, trace):\n    df = pm.trace_to_dataframe(trace)\n    df['loglik'] = [model.logp(trace.point(i)) for i in range(len(trace))]\n    return df\nNext we’ll define a few models in pymc3.\nwith pm.Model() as h0: \n    est_b0 = pm.Normal('b0', mu=0, sd=4)\n    sigma = pm.HalfNormal('sigma', sd=4)\n    y_var = pm.Normal('y', mu=est_b0, sd=sigma, observed=y)\n    h0_trace = pm.sample(10000, model=h0, step=pm.Metropolis(), njobs = 1)\n\nmodels['h0'] = {'model': h0, 'trace': h0_trace, \n                'df': calc_loglik_df(h0, h0_trace[1000:])}\n\nwith pm.Model() as h1: \n    est_b0 = pm.Normal('b0', mu=0, sd=4)\n    est_b1 = pm.Normal('b1', mu=0, sd=4)\n    sigma = pm.HalfNormal('sigma', sd=4)\n    y = pm.Normal('y', mu=est_b0 + est_b1*x1, sd=sigma, observed=y)\n    h1_trace = pm.sample(50000, model=h1, step=pm.Metropolis(), njobs = 1)\n    \nmodels['h1'] = {'model': h1, 'trace': h1_trace, \n                'df': calc_loglik_df(h1, h1_trace[1000:])}\npm.traceplot(h1_trace[500:]);\n\nwith pm.Model() as h2: \n    est_b0 = pm.Normal('b0', mu=0, sd=4)\n    est_b1 = pm.Normal('b1', mu=0, sd=4)\n    est_b2 = pm.Normal('b2', mu=0, sd=4)\n    sigma = pm.HalfNormal('sigma', sd=4)\n    y = pm.Normal('y', mu=est_b0 + est_b1*x1 + est_b2*x2, sd=sigma, observed=y)\n    h2_trace = pm.sample(50000, model=h2, step=pm.Metropolis(), njobs = 1)\n\nmodels['h2'] = {'model': h2, 'trace': h2_trace, \n                'df': calc_loglik_df(h2, h2_trace[1000:])}\npm.traceplot(h2_trace[1000:]);\n\nwith pm.Model() as h3: \n    est_b0 = pm.Normal('b0', mu=0, sd=4)\n    est_b1 = pm.Normal('b1', mu=0, sd=4)\n    est_b2 = pm.Normal('b2', mu=0, sd=4)\n    est_b3 = pm.Normal('b3', mu=0, sd=4)\n    sigma = pm.HalfNormal('sigma', sd=4)\n    y = pm.Normal('y', mu=est_b0 + est_b1*x1 + est_b2*x2+est_b3*x3, sd=sigma, observed=y)\n    h3_trace = pm.sample(100000, model=h3, step=pm.Metropolis(), njobs = 1)\n\nmodels['h3'] = {'model': h3, 'trace': h3_trace, \n                'df': calc_loglik_df(h3, h3_trace[1000:])}\npm.traceplot(h3_trace[2000:]);\n\nQuick Observation\nWhen looking at the traces we should already be able to smell that certain models models make more sense than others. The h0 model has a lot of uncertainty of the estimated parameters and the h3 model has a parameter that is equal to its prior. Although this gives us a hint that these models are poor choices, we can apply an even more fancy trick to quantify the model performance.\nLikelihoods Ahoy\nLet’s again apply a helper function.\ndef plot_loglik_df(name):\n    df = models[name]['df']\n    likelihoods = df[df['loglik'] > np.percentile(df['loglik'], 1)]['loglik']\n    plt.hist(likelihoods, bins = 50)\n    plt.title(\"likelihood distribution of {}\".format(name));\nLet’s plot the likelihood distribution of all the models.\n\n\nimgs <- c(\"bayesian-shaving-cream_files/razor-loglik-0.png\", \n          \"bayesian-shaving-cream_files/razor-loglik-1.png\", \n          \"bayesian-shaving-cream_files/razor-loglik-2.png\", \n          \"bayesian-shaving-cream_files/razor-loglik-3.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nThese likelihoods can be interpreted as how likely it is that our model fits the data we’ve seen. These are log likelihoods, which is why the x-axis contains negative numbers. We can compare models by sampling from their respective likelihood distributions.\nYou can also notice that the 3rd model does not seem to fit the data as well as the 2nd model. But we can also estimate \\(p(M_3 > M_2)\\) by sampling these distributions.\nYou could make a simple sampler that estimates how likely it is that one model has a better likelihood score.\ndef compare(mod1, mod2, n = 1000):\n    arr1 = np.array(models[mod1]['df']['loglik'])\n    arr2 = np.array(models[mod2]['df']['loglik'])\n    return np.sum(arr1 > arr2)/len(arr1)\n\n# calculate prob (h2>h3)\ncompare('h2', 'h3', 10000) # 0.90169999999999995\nExtra Benefit\nNote that our method of measuring likelihood is dependant on the amount of data. If we have more data then it will be more strict, if we have less data it will be less prone to suggest one model is better than the other one.\nConsider the same likelihoods when \\(n=10\\).\n\n\nimgs <- c(\"bayesian-shaving-cream_files/razor-loglik-n10-0.png\", \n          \"bayesian-shaving-cream_files/razor-loglik-n10-1.png\", \n          \"bayesian-shaving-cream_files/razor-loglik-n10-2.png\", \n          \"bayesian-shaving-cream_files/razor-loglik-n10-3.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nOr the same likelihoods when \\(n=500\\).\n\n\nimgs <- c(\"bayesian-shaving-cream_files/razor-loglik-n500-0.png\", \n          \"bayesian-shaving-cream_files/razor-loglik-n500-1.png\", \n          \"bayesian-shaving-cream_files/razor-loglik-n500-2.png\", \n          \"bayesian-shaving-cream_files/razor-loglik-n500-3.png\")\nknitr::include_graphics(imgs)\n\n\n\n\nYou cannot compare the likelihoods of different models that saw different data, but you can still compare models that saw the same data. If we have more data then it log likelihood automatically gets smaller. What is interesting to see is that when we have very little data we suddenly have a situation where we may prefer \\(M_1\\) instead of \\(M_2\\). We can measure this preference and come to the conclusion that it is not a strong preference though. In my the above run of \\(n=10\\) I estimated that \\(p(M_1 > M_2) \\approx 0.59\\) which isn’t a very strong preference. When \\(n=500\\) the preference is much more pronounced but not much different from the \\(n=100\\) sitaution.\nTry it out for yourself if you want to get convinced.\nConclusion\nThe goal of this blogpost is to point to an intuition of how the bayesian occrams razor works. My hope is that this document is an inspiring example that explores some lovely things from bayesian school of thought but I would recommend against doing this in production. If I am thinking practically you should first consider that;\nWe’re using sampling as a method of inferring \\(p(M_k | D)\\) which is numerically cumbersome. You could consider apporixmate inference methods, but these have potential downsides caused by their assumptions.\nWe don’t have a clear method of saying when we’ve sampled enough datapoints for all of our models. Peeking at the likelihood histogram seems to be the state of the art.\nYou may be able to obtain a very similar conclusion with a frequentist T-test. I’ve found that this heuristic works well enough in most cases and it is much cheaper. This will only work on linear models though.\nYou could still use train/test/k-fold validation with a proper hyperparameter search on this system and you should also be able to pick the appropriate model. This may also take a lot of time but it requires less math and is much more straightforward to explain to an engineer.\nI hope we’ll get much better at fast inference of a general \\(p(M_k | D)\\) but we’re not there yet.\n\n\n\n",
    "preview": "posts/bayesian-shaving-cream/bayesian-shaving-cream_files/bayesian-razor6.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1461,
    "preview_height": 764
  },
  {
    "path": "posts/bayesian-propto-streaming/",
    "title": "Bayesian/Streaming Algorithms",
    "description": "A convenient overlap.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2016-11-19",
    "categories": [],
    "contents": "\n\n.lemma {\n    border-style: solid;\n    border-width: medium;\n    padding: 8px;\n}\nIn this document I’ll demonstrate that any bayesian algorithm is also an algorithm that can be implemented for streaming. If not for streaming, it also turns out that an alternative benefit is that the algorithm becomes parallel. I’ll demonstrate this with a little bit with maths and then with a demo linear regression application in python. I’ll conclude by preaching a founding idea of apache flink.\nBayes Rule for Parameters\nWhenever a bayesian models they immediately write down the following:\n\\[ p(\\theta | D) \\propto p(D | \\theta) p (\\theta) = \\Pi_i p(d_i | \\theta) p(\\theta) \\]\nThis formula is very important as it gives us another way to make models via inference. In the context of streaming, it even tells us something extra. Let’s consider three independant points of data that we’ve seen; \\(d1, d2, d3\\). Then bayes rule states that;\n\\[ p(\\theta | d_1, d_2, d_3) \\propto p(d_3 | \\theta) p(d_2 | \\theta) p(d_1 | \\theta) p(\\theta) \\]\nYou may recognize a recursive relationship here. When the 3rd datapoint comes in, it moves probability mass based on the prior knowledge known (based on \\(d_2, d_1\\) and the original prior). When the 2nd datapoint comes in, it moves probability mass based on the prior knowledge know (based on just \\(d_1\\) and the original prior).\n\\[p(\\theta | d_1, d_2, d_3) \\propto p(d_3 | \\theta) p(d_2 | \\theta) \\underbrace{p(d_1 | \\theta) p(\\theta)}_{\\text{prior for } d_2} \\]\n\\[p(\\theta | d_1, d_2, d_3) \\propto p(d_3 | \\theta) \\underbrace{p(d_2 | \\theta) p(d_1 | \\theta) p(\\theta)}_{\\text{prior for } d_3} \\]\nWhen a model is written down in this form we gain an interesting property: the model updates after each datapoint comes in. The new model depends only on the previous model and a new datapoint. This means it has one very interesting consequence;\n\nAny ML algorithm that can be updated via \\(p(\\theta | D) \\propto \\Pi_i p(d_i | \\theta) p(\\theta)\\) is automatically a streaming algorithm as well.\n\nTurns out, lots of algorithms can be turned into streaming algorithms this way. Another nice property shows itself: if each \\(p(d_i | \\theta)\\) is independant of eachother then we can also process parts of \\(\\Pi_i p(d_i | \\theta)\\) in parallel as long as we combine it in the end. Map-Reduce ahoy!\nStreaming Regression\nLet’s first generate some simple regression data that we need to work with.\nn = 25\nxs = np.random.uniform(0, 2, n)\nys = 2.5 + 3.5 * xs + np.random.normal(0, 0.3, n)\nplt.scatter(xs, ys)\nWe will now try to model this dataset. Again, we’ll write down what every bayesian does;\n\\[ p(\\theta | D) \\propto p(D | \\theta) p (\\theta) = \\Pi_i p(d_i | \\theta) p(\\theta) \\]\nTo keep things simple, we’ll take \\(\\theta = [w_0, w_1]\\). That is, I’ll not worry about the variance and I am merely interested in figuring out the posterior distribution of the intercept and the slope. I’ll assume \\(p(\\theta)\\) to be uniform. If we keep things basic like this, I’ll only need to worry about the likelihood \\(p(d_i | \\theta)\\).\nHere’s my proposed model:\n\\[ p(d_i | \\theta) \\sim N(w_0 + w_1 x - y, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp{ \\frac{(w_0 + w_1 x - y)^2}{2\\sigma^2} } \\]\nI’ll asumme \\(\\sigma = 1\\) just to keep the plotting simple and two dimensional. You could go a step further and model \\(\\theta = [w_0, w_1, \\sigma]\\) but we’ll skip that in this document.\nWith this likelihood known, we can now write code for it. I’ll write a function that I’ll keep fast by applying numba.\n@nb.jit\ndef likelihood(x, y, n = 101, bmin = -5, bmax = 5):\n    res = np.zeros((n,n))\n    for i, b0 in enumerate(np.linspace(bmin, bmax, n)):\n        for j, b1 in enumerate(np.linspace(bmin, bmax, n)):\n            res[n - 1 - j, i] =  np.exp(-(b0 + b1*x - y)**2/2)/np.sqrt(2*np.pi)\n    return res/np.sum(res)\nLet’s demonstrate the likelihood.\nplt.figure(figsize=(5,5))\n_ = plt.imshow(likelihood(x = 1, y = 2), \n           interpolation='none', extent=[-5, 5, -5, 5])\nplt.xlabel('$w_0$')\nplt.ylabel('$w_1$')\nplt.title('$p(x=1, y=2 | w_0, w_1)$')\n_ = plt.colorbar()\nYou may be wondering about the shape of this distribution. The shape makes sense when you realize \\(w_0\\) and \\(w_1\\) ‘negatively correlated’. Suppose we have a line that goes through the point, then the only way to increase the intercept while still going through the point is to decrease the slope (and vise versa).\nWe just showed a likelihood chart for a single point but we can also make this for every point of data in our dataset.\nf, axes = plt.subplots(5, 5, sharex='col', sharey='row', figsize = (10, 10))\n# flatten the list to make iteration easier \naxes = [item for sublist in axes for item in sublist]\n\n# loop over axes and create the plot \nfor dim, axis in enumerate(axes):\n    axes[dim].imshow(likelihood(xs[dim], ys[dim]), \n                     interpolation='none', extent=[-5, 5, -5, 5])\ninlineLet’s now apply the recursive relationship we mentioned at the beginning of the blogpost. Let \\(D_N\\) be the data seen until now and let \\(d_{N+1}\\) be the new datapoint that is just arriving at the model.\n\\[p(\\theta | D_N, d_{N+1}) \\propto p(d_{N+1} | \\theta) \\underbrace{p(D_N| \\theta) p(\\theta)}_{\\text{prior for new datapoint}} \\]\nWe can show how the posterior distribution changes as we add more and more points. Notice the convergence.\nf, axes = plt.subplots(5, 5, sharex='col', sharey='row', figsize = (10, 10))\naxes = [item for sublist in axes for item in sublist]\n\nres = []\nfor dim, axis in enumerate(axes):\n    if len(res) != 0:\n        res = res * likelihood(xs[dim], ys[dim])\n        res = res/np.sum(res)\n    else: \n        res = likelihood(xs[dim], ys[dim])\n    axes[dim].imshow(res, \n                     interpolation='none', extent=[-5, 5, -5, 5])\ninlineLet’s zoom in on the final posterior.\nplt.figure(figsize=(5,5))\n_ = plt.imshow(res, interpolation='none', extent=[-5, 5, -5, 5])\nplt.xlabel('$w_0$')\nplt.ylabel('$w_1$')\nplt.title('$p(w_0, w_1 | D)$')\ninlineThe true values for \\(w_0, w_1\\) are 2.5 and 3.5. From eyeballing at the posterior I’d say this method seems to work.\nExtra Modelling Options\nCurrently we’ve implemented a streaming model for linear regression that tries to learn static parameters. How would we change this model if these parameters aren’t static?\nAnswer, we can fiddle around with the recursive relationship. Remember;\n\\[p(\\theta | D_N, d_{N+1}) \\propto p(d_{N+1} | \\theta) p(D_N | \\theta)\\]\nLet’s introduce a parameter \\(\\alpha\\) that gives more weight to the likelihood distribution of the most recent datapoint. We can combine the idea of exponentially weighted smoothing with our bayesian mindset to get to a model that will give more mass to points that are recent.\n\\[p(\\theta | D_N, d_{N+1}) \\propto p(d_{N+1} | \\theta) p(D_N | \\theta) + \\alpha p(d_{N+1} | \\theta)\\]\nWe still retain the streaming aspect of the model but we are applying a bit of hack as far as probability theory is concerned.\nA thing of beauty\nLooking at the problem with bayesian glasses gives benefits over just looking at it from the standard ML/frequentist point of view. The alternate way of thinking has supplied us with a neat probabilistic mindset that allows us to tackle streaming problems with ease.\nWe could now apply a koan; instead of asking ourselves how to solve streaming problems we may wonder if streaming problems are any different than batch. The observation being; if you’ve solved streaming then you’ve also solved batch. After all, we can stream a large file into the algorithm line by line.\nMind blown. If you’ve solved streaming, you’ve just solved batch as well.\nExactly this attitude is what makes apache flink such a badass project. Flink offers a framework to do streaming analytics on a distributed system. Spark also has some support for this problem but cheats a little bit by not offering ‘true’ streaming but by offering microbatching. I expect these two framework to fight for control of the ecosystem in the next year. Not everybody has the same latency requirements.\nAnother main benefit of being able to do streaming algorithms is that your system will have less moving parts. Most web applications suffer from all the extra work involved with the integration of caching systems, oozie jobs, model serialisation and monitoring. If your model can just learn on a stream, all this would be done in one go.\nConclusion\nBatch is a subset of streaming and thinking like a bayesian helps when designing models.\n\n\n\n",
    "preview": "posts/bayesian-propto-streaming/bayesian-propto-streaming_files/stream_bayes2.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 478,
    "preview_height": 414
  },
  {
    "path": "posts/avoiding-and-preventing-joins/",
    "title": "Avoiding, and Preventing, Joins",
    "description": "Join me in preventing this.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2016-10-20",
    "categories": [],
    "contents": "\nJoins are expensive and they should be avoided when possible. There are a few ways you can prevent them. In this blogpost I’ll demonstrate two options; one in R and one in spark.\nThe Situation\nLet’s say that we have a database of users that performed some action on a website that we own. For the purpose of our research, we want to omit users that have only had one interaction with the website. If we take the example dataset like below, you will want to remove user c;\n  user val\n1    a   3\n2    a   2\n3    a   1\n4    b   5\n5    b   6\n6    b   4\n7    b   1\n8    c   2\nYou can do this with a join. But you can avoid it in this situation.\nTidy Nests in R\nA standard way to do this in R would be to first create a dataframe that aggregates the number of interactions per user. This dataset can then be joined back to the original dataframe such that it can be filtered.\nlibrary(tidyverse)\n\ndf <- data.frame(\n  user = c(\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"c\"),\n  val  = c(3, 2, 1, 5, 6, 4, 1, 2)\n)\n\nagg <- df %>% \n  group_by(user) %>% \n  summarise(n = n())\n\ndf %>% \n  left_join(agg) %>% \n  filter(n >= 2) %>% \n  select(user, val)\nThe output will be correct, but because of the join the calculation can be rather expensive when the dataset becomes very large. An alternative in R is to use the nest function from the tidyr package (which comes withtidyverse). This nest function allows you have to have a dataframe inside of a dataframe. You can see how it works by calling;\n> df %>% \n  group_by(user) %>% \n  nest() \n\n    user             data\n  <fctr>           <list>\n1      a <tibble [3 x 1]>\n2      b <tibble [4 x 1]>\n3      c <tibble [1 x 1]>\nIn this form you can now create a new column that queries each dataset. The map function from the purrr package (again, comes with tidyverse) will help you do just that.\n> df %>% \n  group_by(user) %>% \n  nest() %>% \n  unnest(n = map(data, ~nrow(.)))\n\n    user             data     n\n  <fctr>           <list> <int>\n1      a <tibble [3 x 1]>     3\n2      b <tibble [4 x 1]>     4\n3      c <tibble [1 x 1]>     1\nThere is no need for a join anymore, the only thing we need to do is to unnest the data column.\n> df %>% \n  group_by(user) %>% \n  nest() %>% \n  unnest(n = map(data, ~nrow(.))) %>% \n  unnest(data)\n\n    user     n   val\n  <fctr> <int> <dbl>\n1      a     3     3\n2      a     3     2\n3      a     3     1\n4      b     4     5\n5      b     4     6\n6      b     4     4\n7      b     4     1\n8      c     1     2\nYou can wrap everything up by removing the columns you don’t need anymore.\n> df %>% \n  group_by(user) %>% \n  nest() %>% \n  unnest(n = map(data, ~nrow(.))) %>% \n  unnest(data) %>% \n  filter(n >= 2) %>% \n  select(user, val)\n    \n    user   val\n  <fctr> <dbl>\n1      a     3\n2      a     2\n3      a     1\n4      b     5\n5      b     6\n6      b     4\n7      b     1\nObviously there is a much better way in R though.\n> df %>% \n  group_by(user) %>% \n  mutate(n = n()) %>% \n  ungroup()\n\n   user   val     n\n  <chr> <dbl> <int>\n1     a     3     3\n2     a     2     3\n3     a     1     3\n4     b     5     4\n5     b     6     4\n6     b     4     4\n7     b     1     4\n8     c     2     1\n\n> df\n  group_by(user) %>% \n  filter(n() >= 2) %>% \n  ungroup()\n\n    user   val\n  <fctr> <dbl>\n1      a     3\n2      a     2\n3      a     1\n4      b     5\n5      b     6\n6      b     4\n7      b     1\nGotta love that dplyr.\nWindow Partitions in Spark\nThe tidyr workflow is awesome but it won’t work everywhere. Spark does not have support for dataframe-in-a-column so we might need to do it some other way. The trick is to use window functions where we partition based on a user.\nLet’s first create the dataframe in pyspark.\nimport pandas as pd \ndf = pd.DataFrame({\n    \"user\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"c\"],\n    \"val\" : [3, 2, 1, 5, 6, 4, 1, 2]\n})\nddf = sqlCtx.createDataFrame(df)\nWe can confirm that this is the same dataframe.\n> ddf.show()\n\n+----+---+\n|user|val|\n+----+---+\n|   a|  3|\n|   a|  2|\n|   a|  1|\n|   b|  5|\n|   b|  6|\n|   b|  4|\n|   b|  1|\n|   c|  2|\n+----+---+\nAgain, the naive way to filter users is to use a join. You could do this via the code below.\nfrom pyspark.sql import functions as sf \nagg = (ddf\n  .groupBy(\"user\")\n  .agg(sf.count(\"user\").alias(\"n\")))\n\nres = (ddf\n  .join(agg, ddf.user == agg.user, \"left\")\n  .filter(sf.col(\"n\") >= 2)\n  .select(ddf.user, ddf.val))\nThis new res dataframe filters out the correct rows.\n> res.show()\n\n+----+---+\n|user|val|\n+----+---+\n|   a|  3|\n|   a|  2|\n|   a|  1|\n|   b|  5|\n|   b|  6|\n|   b|  4|\n|   b|  1|\n+----+---+\nTo prevent the join we need to define a window functions that allows us to apply a function over a partition of data.\nfrom pyspark.sql import Window \nwindow_spec = Window.partitionBy(ddf.user)\nThis window_spec can be used for many functions in spark; min, max, sum and also count. You can see it in action via;\n> (ddf\n  .withColumn(\"n\", sf.count(sf.col(\"user\")).over(window_spec))\\\n  .show())\n\n+----+---+---+\n|user|val|  n|\n+----+---+---+\n|   a|  3|  3|\n|   a|  2|  3|\n|   a|  1|  3|\n|   b|  5|  4|\n|   b|  6|  4|\n|   b|  4|  4|\n|   b|  1|  4|\n|   c|  2|  1|\n+----+---+---+\nThis new n column can be used for filtering, just like we saw in R.\n> (ddf\n  .withColumn(\"n\", sf.count(sf.col(\"user\")).over(window_spec))\n  .filter(sf.col(\"n\") >= 2)\n  .select(\"user\", \"val\")\n  .show())\n\n+----+---+\n|user|val|\n+----+---+\n|   a|  3|\n|   a|  2|\n|   a|  1|\n|   b|  5|\n|   b|  6|\n|   b|  4|\n|   b|  1|\n+----+---+\nConclusion\nYou can’t always prevent joins, but this is a use-case where you definately don’t need them. Joins are error-prone, expensive to calculate and require extra tables to be created. You can apply this trick in smaller datasets in R or even on larger datasets via spark. Please prevent where applicable.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {}
  },
  {
    "path": "posts/hello-deepq/",
    "title": "Hello DeepQ",
    "description": "Stick a Network in the Q-learning algorithm.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2016-10-14",
    "categories": [],
    "contents": "\nIn this document I’ll describe my first attempt at Deep Q Learning (DQL) on the openai gym project. I’ll attempt to explain some theory and I’ll demonstrate where my approach needs some extra work. It is by no means written by an expert and it is likely that I’ve made a small error in maths by applying the laymans maths a bit too much.\nAll the code for this can be found on github.\nThe Problem\nThe cartpole problem is a classic one; given a small cart you need to balance a pole on it.\n\n\nI must admit, it’s a textbook example.\nThere’s a solution that involves physics but instead of doing all the maths ourselves we’d like to design a robot that can instead learn to balance the pole. We’re given an environment that will let us know if the pole is still balanced and some controls that we can play with. It will be the robots job to understand the environment enough such that it can learn how to balance the pole.\nPart of the problem lies in the fact that only after the pole has fallen we’ll learn how well the robot has performed. We don’t know if the pole fell because of some decision that was made in the beginning or if this was due to some decision made at the end of the trial. This makes part of the problem hard, we cannot just blindly apply machine learning or operations research methods because the system doesn’t directly allocate reward to choices that are made.\nAnother part of the problem is that we’d like to have a general way that doesn’t solve just this problem, but many problems. Given some environment, can we have an algorithm that designs the agent for us, without needing us (humans) to understand the environment beforehand. If we’re able to do that, then robots truely become professional companions.\nQ-Learning\nA popular method of looking at these sorts of problems is to consider Q-learning. We try to figure out the value that is obtained given some state of the environment \\(S\\) and some action of the agent \\(a\\). The value of picking an action \\(a\\) from a state \\(S_t\\) is defined via;\n\\[ Q(S_t, a) = R(S_t, a) + \\gamma Q(S_{t+1}, a^*)\\]\n\nThe main observation here is the recursion.\n\\(R(S_t, a)\\) is the short term reward that we get from picking action \\(a\\). By looking at this function you may realize a modelling choice. It tells us that the reward of picking an action from a certain state can be split up into two different pieces: a short term reward and value of being in a new state \\(S_{t+1}\\). You’ll also notice a parameter \\(\\gamma\\) which is used to discount future values.\nThe value of being in a state \\(S_{t+1}\\) is dependant on the optimal action from that state. This action may be probibalistic but we assume that there is some optimal such that we can deduct value of being in a state \\(S_{t+1}\\).\nAn example\nI can imagine the math might distract from the understanding. So I’ll include a simple example to demonstrate how the values are assigned and logged.\nWe’re working with the cartpole. We start an instance of the environment (let’s call this the game). The game ends if the pole seems to be falling beyond repair. There are two inputs; left and right. The environment gives us some state \\(S_t\\) back. Let’s assume a very basic game where we only witnessed 4 states; \\(A\\), \\(B\\), \\(C\\) and \\(D\\). In this case \\(D\\) is the end state, when the pole is falling beyond repair.\n\nWe have lots of \\((S,a)\\) pairs now: \\((A, \\leftarrow)\\), \\((B, \\leftarrow)\\), \\((C, \\leftarrow)\\) and \\((D, \\rightarrow)\\). We now need to assign value to it.\nThe short term reward we get from the system is 1 for every timestep that we survive. This includes the final timestep, so the function that describes the measured value \\(v\\) can be described via;\n\\[ v(D, \\rightarrow) = R(S_t, a) = 0\\]\nThis is because in the final step, there is not further step and thus we get a value of zero. For all the other states \\(R(S_t, a) = 1\\). Let’s say we’ve got a discount rate equal to \\(\\gamma = 0.9\\). We can then propagate the value for the timestep before;\n\\[ v(C, \\leftarrow) = 1 + \\gamma v(D, \\rightarrow) = 1\\]\nThis results moves recursively, such that;\n\\[ v(B, \\leftarrow) = 1 + \\gamma v(C, \\leftarrow) = 1 + \\gamma \\]\nAnd such that;\n\\[ v(A, \\leftarrow) = 1 + \\gamma v(B, \\leftarrow) = 1 + \\gamma(1 + \\gamma)\\]\nThese values all depend on the choice of \\(\\gamma\\). Assuming \\(\\gamma = 0.9\\) we end up with;\n\\[ v(D, \\rightarrow) = 0\\] \\[ v(C, \\leftarrow) = 1\\] \\[ v(B, \\leftarrow) = 1.9\\] \\[ v(A, \\leftarrow) = 2.71\\]\nBy repeating these steps, we can create a set of training data containing triplets of states, actions and values: \\((S, a, v)\\). The goal is to use these datapoints to come up with some model that can describe \\(Q(S_t, a)\\) for us such that we can define a policy to pick the best action. One blunt interpretation of this is to estimate;\n\\[ Q(S_t, a) = \\mathbb{E}[V(S_t, a)]\\]\nSo how would we go about this? We’d need some model …\nUsing a Neural Network\nWe now know how to get a list of \\((S, a, v)\\) triplets via trial and error. The next step is to find a way to predict this. Turns out that a neural network has things to like towards this use-case.\nThe idea behind DQL is to use a deep neural network to estimate the value of using a certain action in a certain state. The neural network tries to compress the environment in an attempt to make the value predictable. There are some considerations though. The neural network will only need to consider two possible action choices. Considering this and the following 3 neural networks, which network might perform best at this task?\nAction Space in Input\n\nAction Space in Output\n\nAction Space with Layer in Output\n\nThe deepmind paper briefly discusses the architecture choice too. They used an architecture in which there is a separate output unit for each possible action and only the state representation is an input to the neural network. The article mentions learning performance as a reason for doing so but it also feels right to have the outputs make use of the same latent feature space that is generated in the network too.\nThere is another curious benefit. Suppose we have a \\((S_t, a_0, v)\\) triplet, then we’ll only need to update weights that can influence the output of \\(a_0\\). This has the benefit that the output nodes push and pull the weights of network in different methods. Potentially, this can lead to a nice latent feature representation.\n\nI’ve chosen to implement this last network, more by intuition then anything else.\nCode\nTurns out that writing such an architecture is relatively straightforward in keras. I’ll list part of the code below.\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout\nfrom keras.optimizers import Adam\n\noutputs = []\nmain_input = Input(shape=(self.input_size,), name = \"input\")\nmod = Dense(output_dim=10, input_dim=self.input_size , activation=\"relu\")(main_input)\nmod = Dense(output_dim=10, activation=\"relu\")(mod)\nself.base_model = mod\nfor i in range(self.output_size):\n    pre_output_layer = Dense(output_dim=10, activation=\"relu\")(self.base_model)\n    output_node = Dense(output_dim=1, activation=\"linear\")(pre_output_layer)\n    output_model = Model(input = main_input, output = output_node)\n    output_model.compile(loss='mse', optimizer = Adam(lr=0.001))\n    outputs.append(output_model)\nThis functional API allows you to be very flexible indeed.\nSome Results\nThe results are mixed it doesn’t always seem to find a good strategy but when it does, I recognize an interesting pattern. I’ll list two plots from two such examples.\n\n\n\n\n\n\n\n\n\nFor each run you’ll see three plots listed;\nThe loss rate of the (deep) neural network; you’ll notice that the loss is very good initially and then suddenly spikes. The x-axis is a bit inconsistent because the training batch size depends on the number of \\((S, a, v)\\) triplets which depends on the performance of the agent. If the agent performs very well, there’s longer lasting instances of games, which leads to more data. If the agent performs poorly then there’s less data during an epoch.\nThe performance of the agent; you’ll notice that the performance initially is very poor, up until a moment where the agent (probably accidentally) performs very well. This is followed by a brief period of confusion after which the agent seems to be performing well.\nThe prediction vs. true value of \\(Q(S, a)\\). You’ll notice that a lot of mass is around the low values but there seems a nice linear relationship between true value and predicted value.\nIn these plots I tend to see a similar pattern. Initially, the algorithm is very good at predicting the value; it will always perform poorly. Later, the robot accidentally did something well which causes the sudden spike. While trying to understand what it just did that caused the high score there is a brief moment of confusion but after a while it learns the correct action.\nGenerality\nThis approach won’t work for continous action spaces. But it is easy to come up with an alternative network representation to facilitate this.\n\nThis method of thinking about networks is relatively general which is why the research interest is so high in this area.\nOr maybe not\nBut this will only work in environments where you can sample and simulate. The gym environment from OpenAI is essentially giving me an infinite amount of data to learn from (and to make mistakes in). This isn’t that realistic as a training procedure for real examples “out there”.\nConclusion\nI’m attempting to write a few general python objects to tackle some open-ai problems. It’s a bit new to me but if you’re interested you’ll find some code on github. My approach seems to show potential, but there still seems to be a lot of tweaking hyperparameters. An obvious one is the shape of the neural network but there’s also things like:\nHow many epochs do we use for learning?\nWhat value of \\(\\gamma\\) is appropriate?\nThe agent sometimes can pick a random action to prevent getting stuck in a local optima (\\(\\epsilon\\) greedy, what value of \\(\\epsilon\\))?\nWhat activation functions made the most sense for the internal networks?\nIs it allright to forget previous trials? If the initial trials are usually achieved by noise, should the neural network consider this data as equal?\nWill this approach work for situations where the state space is only partially observable or will the noise cause too much distress for the NN?\nDo we want to have a deterministic policy, one that always takes the maximum value predicted by the NN or a stochastic policy?\nWhat is an appropriate learning rate for the neural network?\nWhile we’re at it; what is an appropriate gradient method? SGD? Adam?\nIt is also worth noting that the DQL approach seems interesting beause it is general, but it is by no means the only way to do Q-learning. You can also use update rules or policy gradients.\n\n\n\n",
    "preview": "posts/hello-deepq/hello-deepq_files/dql10.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 759,
    "preview_height": 232
  },
  {
    "path": "posts/twain-learning/",
    "title": "Twain Learning",
    "description": "Never let your school get into the way of your regression.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2016-07-26",
    "categories": [],
    "contents": "\nThis document contains three different ways of interpreting a linear regression based on three popular schools of data. The hope is that it may help explain the difference between three schools of thought in data science. It is written from the viewpoint from somebody in industry, not academia.\nSimple Observation\nIn data land, it seems like there are three schools on how to do things.\n\nEach school tends to look at data a bit differently and each school has a small tendancy of pointing out the flaws of the others. The truth is probably in the middle as the schools actually overlap quite a bit. There’s some non overlapping opinion and some techniques that differ though.\nFrequentists are known for hypothesis tests, p-values and significance summary statistics. Bayesians are known for sampling techniques, inference, priors and posteriors. Machine learnings come to the field with ensembler techniques, forests, support vector machines and neural networks.\nIt is a good thing that people have different views otherwise the world would always look the same. Different views lead to good discussions which help us understand the field of data better. What I’m less a fan of is people making claims that they are better at data simply because they’re using a set of models exclusive to their school. Calling a tensorflow model instead of applying probabilistic inference does not make you the Chuck Norris among your peers in the same way that exclusively using an oven doesn’t makes you a better cook.\nInterestingly, all schools use linear regression but interpret at it from a different angle. So let’s use that as a tangible example to show how these three schools think differently about modelling.\nWhere is your schools uncertainty?\nIf I were to look at the schools from a distance, it seems like they all adhere to this basic rule of science.\n\nAll models are wrong. Some models are useful.\n\nThey do however, seem to think differently on how to deal with uncertainty. To figure out where you are, ask yourself. When you look at your model, what are you most uncertain about?\nAre you unsure about the data and are you trying to find the best parameters to describe your model?\nAre you unsure about the model and do you accept the data to be the main truth available?\nAre you willing to ignore all uncertainties when your black box model performs very well against a lot of test sets?\nThere is some sense in all three approaches but they are different. Let’s describe a regression task as an example to make this difference a bit more tangible.\nIllustrative Example: Regression\nSuppose that we have a regression task. We have some data \\(X\\) and some variable we’d like to predict \\(y\\). We’ll assume that this dataset fits into a table and that there are \\(k\\) columns in \\(X\\) and \\(n\\) rows.\nFrequentist View\nA frequentist would look at the regression by looking at it as if it were an hypothesis test. For every column of data we’d want to check if has a significant contribution to the model. We fit the data to the model, optimising for mean squared error (or mean absolute error) and assume some form of standard error on the data. This gives us our estimate \\(\\hat{\\beta}\\), which is different from the true \\(\\beta^*\\). This difference is modelled by including a normal error.\n\\[ y \\approx X\\hat{\\beta} \\] \\[ y = X\\beta + e \\] \\[ e \\sim N(0, \\sigma) \\]\nBy combining a bit of linear algebra and statistics you can use this to come to the conclusion that you can create a hypothesis test for every column \\(k\\) in the model where the zero hypothesis is that \\(\\beta_k\\) should be zero. This test requires a T-distribution and there’s a proof for it. This is why so many statistics papers have summaries like this:\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-103.95  -53.65  -13.64   40.38  230.05 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  102.645      4.674  21.961  < 2e-16 ***\nDiet2         19.971      7.867   2.538   0.0114 *  \nDiet3         40.305      7.867   5.123 4.11e-07 ***\nDiet4         32.617      7.910   4.123 4.29e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 69.33 on 574 degrees of freedom\nMultiple R-squared:  0.05348,   Adjusted R-squared:  0.04853 \nF-statistic: 10.81 on 3 and 574 DF,  p-value: 6.433e-07\nThere’s a lot of assumptions involved don’t feel obvious to me but can hold for a lot of cases. Assuming normality is a thing but assuming that all columns are independant is a big one. Frequentists are usually aware of this which is why after modelling the regression with good columns one usually runs hypothesis tests on the assumptions that went into the model. This is why you’ll notice quite a few statistics listed in the regression output.\nBayesian View\nA bayesian would look at the task and immediately write down;\n\\[ p(\\theta | D) = \\frac{p(D | \\theta)p(\\theta)}{p(D)} \\propto p(D | \\theta)p(\\theta) =  p(\\theta) \\Pi_i p(x_i| \\theta)\\]\nHere, \\(\\theta\\) are the parameters of the model that need to be learned from the available data \\(D\\). By applying bayes rule the bayesian would argue that this is proportional to a likelihood \\(p(D | \\theta)\\) and a prior \\(p(\\theta)\\). In linear regression \\(\\theta = (\\beta, \\sigma\\)).\nThe bayesian would not be interested in maximum likely value of \\(\\theta\\). In fact, the bayesian would not even consider \\(\\hat{\\theta}\\) that much of a special citizen. The goal is to derive the distribution of \\(p(\\theta | D)\\) instead because this holds much more information.\nThe downside is that this task is harder from a maths perspective because \\(p(\\theta | D)\\) is a \\((k + 1)\\)-dimensional probability distribution. Luckily, because you can model \\(\\Pi_i p(x_i| \\beta)\\) you can use sampling techniques like MCMC or variational inference to infer what \\(p(\\beta | D)\\) might be. A helpful realisation is that you’ll only need to figure out this bit yourself;\n\\[ p(x_i| \\beta, \\sigma) \\sim N(X\\beta, \\sigma) \\]\nThis defines the likelihood that given certain model parameters you’ll witness a point \\(x_i\\) in your data. The derivation of this is somewhat obvious for a linear regression, but less obvious for other models.\nA benefit of looking at it from this way is that you are able to apply inference on a stream of data instead of just a batch of data. You can learn quite a bit from small datasets and you usually get a good opportunity to learn from the world instead of merely trying to predict it.\nPeople sometimes critique the bayesian approach because you need to supply it with a prior \\(p(\\theta)\\). Bayesian usually respond by saying that all models require some assumptions and that it is a sensible thing to put domain knowledge into a model when applicable. I personally find the choice of model part \\(p(x_i | \\beta)\\) to be the biggest assumption in the entire model to such an extend that it seems worthless to even worry about the prior \\(p(\\theta)\\).\nMachine Learning View\n\nThe machine learning specialist won’t mind to view the problem as a black box. A typical approach would go as far as not generting one model but many. After all, training many models on subsets of the data (both rows and columns) might give better performance in the test set. The output of these regressions may then potentially be used as input for ever more regressions. One of these regressions might actually be a tree model in disguise, we can apply many tricks to prevent overfitting (L1/L2 regularisation) and we may even go a bit crazy in terms of feature engineering. We might even do something fancy by keeping track of all the mistakes that we make and try to focus on those as we’re training (boosting, confusion matrix reweighting).\nTypically the machine learning specialist would ignore any statistical properties of the regression. Instead, one would run many versions of this algorithm many many times on random subsets of the data by splitting it up into training and test sets. One would then accept the model that performs best in the test set as the best model and this would be a candidate for production.\nConclusion\nWhen looking at the regression example I would argue that these three approaches overlap more than they differ but I hope that the explanation does show the subtle differences in the three schools. Depending on your domain you may have a preference to the modelling method. In the end, the only thing that matters is that you solve problems. The rest is just theoretical banther.\n\nIf I have to place myself on the triangle, I’d be at the bottom somewhere between bayes and ML. The only reason for being there is that I’m able to solve the most problems that way and the assumptions frequentist sometimes gets in my way (link to rather impolite rant). I feel comfortable near the edge of the triangle but there is danger in being stuck in a single corner. You may an epic ensembling machine learning wizard, if that is all you can do then you’re missing out.\nCool things happen when you combine methods from different schools. Many recent NIPS papers dicuss bayesian interpretation of neural networks; variational inference, sampling techniques and networks as generative approximators are all trends that combine knowledge from both schools. It just goes to show that if you try to learn from another school, you might just be able to solve different problems.\nRemark\nThe entire post can be summerised in the words of Mark Twain;\n\n\nNever let your school get in the way of your education.\n\n\n\n\n",
    "preview": "posts/twain-learning/twain-learning_files/triangle_smoll.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1000,
    "preview_height": 378
  },
  {
    "path": "posts/switching-to-sampling-in-order-to-switch/",
    "title": "Switching to Sampling in Order to Switch",
    "description": "A simple introduction to PyMC3.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2016-07-14",
    "categories": [],
    "contents": "\nIn this document I’ll demonstrate how sampling can help you solve a simple switchpoint-timeseries task. We’ll also review how to code this into PyMC3. If you want the code to this, you can find it here.\nWait Sampling?!\nIf you’re from the machine learning school, you might be slightly suprised that sampling can be used as a modelling technique. Usually one might define some loss function that describes how well your model fits your dataset and you just optimize the parameter of that loss followed by lots of crossvalidation. What I’ll be describing in this document is a different approach to modelling.\nInstead of taking a model and fitting it, we’re interested in finding:\n\\[ \\mathbb{P}(\\theta | D) \\]\nwhere \\(D\\) is the data given to us and \\(\\theta\\) is the set of parameters that need to be chosen for the model. These might be the regression coefficients for a linear regression or even the weights of a neural network. Note that we’re not interested in finding the best value of \\(\\theta\\), instead we want to know the distribution of $ (| D)$ because we want to understand the uncertainties in our model. In this framework, we’re putting more uncertainty in our model and trusting the data more as the only source of information.\nData\nThis might still sound a bit vague, so let’s first generate the data that we want to analyse. An example usually helps a lot in explaining things.\n\n\nr = np.random.randn\nn1, n2 = 50, 150\nx1 = r(n1) - 1\nx2 = r(n2)*1.4 + 1\nx = np.hstack([x1, x2])\nThis is a timeseries with a switchpoint in it. With a blind eye, you should be able to verify that there a moment where the series seems to take a new form. The goal for us is to find out where the changepoint is algorithmically as well as derive the mean and variance of the timeseries before and after the changepoint. In this case \\(\\theta = \\{ s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2\\}\\), where \\(s\\) is the moment in time where the switch occured, \\(\\mu_1, \\sigma_1\\) correspond to the mean and variance before the switchpoint and \\(\\mu_2, \\sigma_2\\) correspond to the mean and variance after the switchpoint.\nThis means we can translate \\(\\mathbb{P}(\\theta | D)\\) into;\n\\[ \\mathbb{P}(s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2 | D) \\]\nThe nice thing about looking at the world with bayesian glasses is that you can use maths to help you think about what this means.\n\\[\n\\begin{aligned} \n\\mathbb{P}(s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2 | D) &\\propto \\mathbb{P}(D | s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2)\\mathbb{P}(s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2) \\\\\\\n& = \\Pi_i \\mathbb{P}( y_i | s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2)\\mathbb{P}(s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2)\n\\end{aligned}\n\\]\nNow let’s think about the two parts in that equation\nPart 1: \\(\\Pi_i \\mathbb{P}( y_i | s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2)\\)\nThis part of the equation describes how likely it is that we see a datapoint \\(y_i\\) in our dataset given the model parameters. How would we describe the likelihood of a point \\(y_i\\) given the model parameters? We are free to model it any way we like, but for our setting I feel that this seems reasonable.\n\\[ \\mathbb{P}( y_i | s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2) = \n    \\begin{cases} \n        y_i \\sim N(\\mu_1, \\sigma_1) &\\mbox{if } i \\leq s \\\\\\\n        y_i \\sim N(\\mu_2, \\sigma_2) & \\mbox{if } i > s \n    \\end{cases}\\]\nPart 2: \\(\\mathbb{P}(s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2)\\)\nThis part of the equation describes our prior belief; what do we know about the parameters before looking at any data?\nYou might be temped to think; “I don’t know anything! Putting data into the model is bias!”. This thought is not unreasonable, but then again; every model has bias. In this case I think it is fair to argue that we actually know some things;\nWe know that the switchpoint \\(s\\) is an integer between 0 and the length of the timeseries.\nFrom just eyeballing the timeseries, it seems like all the \\(\\mu\\) values are probably somewhere in \\([-2, 2]\\). It also seems like the \\(\\sigma\\) is probably not too big either.\nUsing this information in our model does not sound too unreasonble to me. In fact, being able to use this information seems like a reasonable requirement. I may even go as far as suggesting that it is a bit flawed if your model doesn’t allow you to do this in any way whatsoever.\nYes, Sampling\nSo far, we have defined our model mathematically but we have done nothing with it algorithmically. At this stage very little is different from the standard machine learning approach. We have a mathematical setup, albeit one with a bit of our own bias in it and we are just about to search for the best values of \\(s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2\\). People might complain about the need to do a bit of math but that’s it. Just use gradient descrent and you’d be done right? I’ll give two reasons why this might be a suboptimal way of thinking about it.\nThe loss function of our model is not convex and this is the case in general. There is inherent risk in that we are not able to find a set of optimal parameters and that we will end up using a flawed model. Even when the model is modest in size like here.\nWe will have no information known to us about the uncertainty of our paramters. In this particular example, the quantiles of \\(s\\) might be more valuable to me than the most likely value of \\(s\\).\nSo instead of finding an optimal solution, we will try to create samples from \\(\\mathbb{P}(s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2 | D)\\). The hope is, that after many many samples, we get an idea of the distribution of the parameters. The nice thing about sampling is that we do not need to worry about deriving the distribution with maths. The nasty thing is that we may need to sample much data before we achieve any convergence.\nA popular tactic for sampling is to take a MCMC approach. The idea is that we start out at some random allocation of \\(s, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2\\) and then make mini steps that are determined by the likelihood. If a step is likely compared to the current parameters we take it, otherwise we might reconsider stepping somewhere else. This approach is strictly different than genetic algorithms in the sense that we are intend on also visiting areas with low likelihood (but with a low probability of staying there long). The idea behind the sampler is to approximate the underlying distribution, not to merely find optimal values.\nPYMC3\nIn the python world, there are three libraries to do this sort of probibalistic sampling in:\nPyMC3\nemcee\nPyStan\nI’ll be using PyMC3 here but for no particular reason whatsoever, I guess because it is most represented in the blog-o-sphere. Emcee is a worthwhile alternative in the sense that it may offer a bit more speed because it comes with easy parallisation options but it seems to have less freedom in choice of distributions. Stan is pretty great too, but I have some minor issues with the api and documentation.\nIn the code below you’ll see all the math describe above written into code.\n\nimport pymc3 as pm\nbasic_model = pm.Model()\nwith basic_model:\n    # define all of my priors\n    mu1 = pm.Normal('mu1', mu=0, sd=2)\n    mu2 = pm.Normal('mu2', mu=0, sd=2)\n    sigma1 = pm.HalfNormal('sigma1', sd=2)\n    sigma2 = pm.HalfNormal('sigma2', sd=2)\n    switchpoint = pm.DiscreteUniform('switchpoint', 0, time.max())\n\n    # formally define the effect from the switchpoint\n    tau_mu = pm.switch(time >= switchpoint, mu2, mu1)\n    tau_sigma = pm.switch(time >= switchpoint, sigma2, sigma1)\n    \n    # define the relationship between observed data and input\n    y = pm.Normal('y1', mu=tau_mu, sd=tau_sigma, observed=x)\n    trace = pm.sample(10000)\nThis code will huff, puff and sample a bit. Once that is done, we can see the history of the sampler as well as a histogram of our parameter estimates.\n\n_ = pm.traceplot(trace)\n\nWe seem to find the appropriate switchpoint, which is nice. More sampling would give more convergence and it would help if we had more data too. Note that if the difference between before and after the switchpoint is very subtle it will become rather hard infer the correct switchpoint.\nWe may also attempt a maximum aposteri approach, this has risks when the posteriori function becomes less and less convex.\n\n> pm.find_MAP(model=basic_model)\n{'mu1': array(-0.10742096434465985),\n 'mu2': array(1.0232122174722886),\n 'sigma1_log_': array(0.44914063604250837),\n 'sigma2_log_': array(0.2821837005109984),\n 'switchpoint': array(99)}\nNotice:\nthat the MAP estimate gives us very little information compared to the estimated distribution that we get out from sampling.\nthe switchpoint parameter is way off, this is due to the fact that it has a bit of trouble with discrete inference.\nMultiple Switchpoints\nWe can extend the model very easily to contain multiple switchpoints too. In fact, let’s make this switchpoint interesting. Let’s assume that after the second switchpoint the timeseries switches back into it’s original generative form.\n\nPay attention to how the code is now different.\n\nadv_model = pm.Model()\nwith adv_model:\n    # define all of my priors\n    mu1 = pm.Normal('mu1', mu=0, sd=4)\n    mu2 = pm.Normal('mu2', mu=0, sd=4)\n    sigma1 = pm.HalfNormal('sigma1', sd=4)\n    sigma2 = pm.HalfNormal('sigma2', sd=4)\n    switchpoint1 = pm.DiscreteUniform('switchpoint1', 0, time.max() - 1)\n    switchpoint2 = pm.DiscreteUniform('switchpoint2', switchpoint1, time.max())\n\n    # formally define the effect from the switchpoints, be careful about order! \n    tau_mu1 = pm.switch(time >= switchpoint1, mu2, mu1)\n    tau_mu2 = pm.switch(time >= switchpoint2, mu1, tau_mu1)\n    tau_sigma1 = pm.switch(time >= switchpoint1, sigma2, sigma1)\n    tau_sigma2 = pm.switch(time >= switchpoint2, sigma1, tau_sigma1)\n    \n    # define the relationship between observed data and input\n    y = pm.Normal('y1', mu=tau_mu2, sd=tau_sigma2, observed=x)\n    adv_trace = pm.sample(5000)\nAgain, the huff and the puff will start. While you’re waiting; it seems good to point out the flexibility of the code. I am able to use variables that are stochastic to define other variables that are stochastically dependant. Variable switchpoint2 is defined via switchpoint1 even though they are stochastic! The sampler will deal with the inference from this point.\nOnce it’s done we can plot the trace again.\n\n_ = pm.traceplot(adv_trace)\n\nIt seems like we get a better convergence in what appears to be a more complex model. Be mindful, consider:\nThis timeseries has more data and the differences between two switchpoints are less subtle.\nBecause we know that the generative process before the first switchpoint and after the last switchpoint are the same, we have more information available in the system.\nThe stacktrace has a bit of visual bias. You’ll notice a different pattern from the first samples of the trace. These ‘different’ samples are also drawn in the histogram which gives the impression that the distributions are very thin while it is largely due to the inclusion of outliers.\nTo combat the last point, we should plot the traces while omitting the first 1000 samples and get a better image. It is a good practice to ignore the first x%-ish of generated samples to reduce the initial value bias.\n\n_ = pm.traceplot(trace[1000:])\n\nAgain, the MAP fails when it comes to switchpoints.\n\n> pm.find_MAP(model=adv_model)\n{'mu1': array(1.3253008730440974),\n 'mu2': array(0.0),\n 'sigma1_log_': array(0.8639975314985741),\n 'sigma2_log_': array(1.3862943591402144),\n 'switchpoint1': array(224),\n 'switchpoint2': array(224)}\nSmall Bonus\nBecause we have samples over multiple parameters over time, we can plot the crosscorrelation of them as well. This is not the case in our current dataset, but being able to look into this is a nice property as well.\n\nimport corner\nvals = ['mu1', 'mu2', 'sigma1_log_', 'sigma2_log_']\nlabels = [r\"$\\mu_1$\", r\"$\\mu_2$\", \n          r\"$\\log \\sigma_1$\", r\"$\\log \\sigma_2$\"]\nsamples = np.array([trace.get_values(p) for p in vals]).T[1000:]\ncorner.corner(samples, \n              labels= labels, \n              quantiles= [0.25, 0.5, 0.75],\n              show_titles= True)\n\nConclusion\nNot everybody enjoys this sampling approach. Some people don’t like doing the math (bad reason) and other people worry about the convergence and training speed (very good reason). The thing that appeals to me is the flexibility of the modelling technique and some of the intellectual freedom that it gives me as a data person.\nI am very flexible in designing my model with this technique. I’m not confined to a predefined mold of an ML model. Where in the standard ML-world I sometimes feel like all of my time is spent in designing features for a model I can now spend more time designing a model around the features that I have. This feels very liberating. Though the worry about convergence is a very legitimate one. Hopefully the field of variational inference will find a general solution to that someday.\n\n\n",
    "preview": "posts/switching-to-sampling-in-order-to-switch/switching-to-sampling-in-order-to-switch_files/switch3.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 991,
    "preview_height": 200
  },
  {
    "path": "posts/pokemon-recommendations-part-2/",
    "title": "Pokemon Recommendations, Part 2",
    "description": "Even more of a Sequel than SQL.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2016-05-05",
    "categories": [],
    "contents": "\n\nIn a previous document I’ve explained how to use the CAPM model to create naive ranking for pokemon. Even though the approach had some logic to it, I wanted to explore if a naive probibalistic implementation of trueskill could do the job as well. This document contains a python implementation my naive approach as well as an application to a pokemon dataset. The first part will explain how it is made and tested. The second part will analyse the ranking of all 750 pokemon.\nStarting Data\nBefore writing any application code, it seems sensible to first think about how to test if the code works. It would be a good idea to simulate some random players with predetermined skill levels such that we can test if our ranking algorithm can at least rank simulated data well enough. The code below generates this data.\nimport uuid\nimport numpy as np\nimport pandas as pd\nimport json \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn; seaborn.set()\nSIZE_ARR = 100\n\ndef sim(p1, p2, entropy = 1):\n    ''' \n    Given two players (tuples: <name, skill>) simulate \n    a game between them. Returns (winner-name, loser-name)\n    '''\n    r = np.random.randint(-entropy, entropy + 1)\n    if p1[1] + r > p2[1]:\n    return p1[0], p2[0]\n    else:\n    return p2[0], p1[0]  \n\ndef gen_data(n_users = 20, n_games = 100, entropy = 1):\n    '''\n    Generate a lot of games and outputs a list of user tuples\n    (<name>, <skill>)as well as a list of games played.\n    '''\n    data = []\n    users = [(str(uuid.uuid4())[:6], i) for i in range(n_users)]\n    for  in range(n_games): \n    i, j = np.random.choice(range(len(users)), 2)\n    u1, u2 = users[i], users[j] \n    data.append(sim(u1, u2, entropy))\n    return users, pd.DataFrame(data, columns = ['winner', 'loser'])\nCode that does the math\nWe’ll now concern ourselves with writing code that updates a system of beliefs. The belief will be captured in a numpy array and can be interpreted as a histogram of likelihood values for a certain level of skill. The algorithm is explained in the previous blogpost as well but schematically, this is what the belief update looks like;\n\nWe start out with two arrays of prior skill beliefs for player 1 and player 2. We then combine them in a two dimensional density. This is our prior belief of the skill levels of the two players. Once a player wins, we can update this belief. One player has a lower skill level. To translate this to our belief system, we cut away all likelihood in the triangle of the losing player. Then we marginalise our belief back to the two players. By repeating this for all the games, we hope to achieve a belief of skill that can be updated in real time.\nTowards Code\nNext we’ll define all the functions that do the ranking. These functions are meant to be minimal, as we only want to check if this algorithm can work. It makes sense to think about what our app code might look like and to think about how to test individual components before writing the code though. I’m not particularily religious about testing but I do see some value for writing a test before writing code here. We’re going to be munging with numpy matrices and typos in indices will be unforgiving.\np1 = np.array([1.,2.,3.])\np2 = np.array([3.,2.,1.])\np3 = np.array([1.,1.,1.])\nmat1 = gen_prior(p1, p2)\nmat2 = gen_prior(p2, p1)\nmat3 = gen_prior(p3, p3)\nm1, m2 = gen_marginals(mat1)\nm3, m4 = gen_marginals(mat2)\nnp1, np2 = gen_marginals(cut_matrix(mat1))\nassert all(m1 == m2[::-1]) #marginals should still be same shape \nassert all(m3 == m4[::-1]) #marginals should still be same shape \nassert np.sum(mat1) == np.sum(mat2) #matrices should have equal values\nassert np.sum(mat3) == 9 #all ones should sum to 9 \nassert all(np1 == np2[::-1]) #marginals should still be same shape\nWith these tests in place, I could move on to the code.\ndef get_player_stats(name):\n    '''\n    Check if a player exists in the dictionary. If not, add it. \n    Then returns the player belief. \n    '''\n    if name not in d.keys(): \n    d[name] = np.ones(SIZE_ARR)/np.sum(np.ones(SIZE_ARR))\n    return d[name]\n\ndef gen_prior(arr1, arr2):\n    return np.matrix(arr1).T  np.matrix(arr2)\n\ndef cut_matrix(mat):\n    posterior = np.triu(mat) + 0.000001\n    posterior = posterior/np.sum(posterior)\n    return posterior\n    \ndef gen_marginals(posterior_mat):\n    winner = np.squeeze(np.asarray(np.sum(posterior_mat, 0)))\n    loser = np.squeeze(np.asarray(np.sum(posterior_mat, 1)))\n    return winner, loser\n\ndef update(winner, loser):\n    winner_arr = get_player_stats(winner)\n    loser_arr = get_player_stats(loser)\n    prior_mat = gen_prior(loser_arr, winner_arr)\n    posterior_mat = cut_matrix(prior_mat)\n    new_winner, new_loser = gen_marginals(posterior_mat)\n    d[loser] = new_loser\n    d[winner] = new_winner\nI wrote the code and the tests passed. For now this is enough. Before running this code against the pokemon dataset, let us first run it against some simulated data.\nRunning the code a few times.\nFirst let’s define a few functions that automate plotting.\ndef plot_scores(plot_idx): \n    plt.subplot(plot_idx)\n    plt.scatter(\n        np.arange(len(users)), \n        [np.mean(d[usr[0]]  np.arange(SIZE_ARR)) for usr in users])\n\ndef plot_dist(plot_idx):\n    plt.subplot(plot_idx)\n    for usr in users:\n        plt.plot(np.arange(SIZE_ARR), d[usr[0]], alpha = 0.5)\nNext we’ll run some experiments.\nSimulate n_players = 100, n_games = 400, entropy = 1\n\nSimulate n_players = 100, n_games = 400/10000, entropy = 15\n\nSimulate n_players = 100, n_games = 400/10000, entropy = 50\n\nThere’s still some room for improvement to this system to make the convergence better, but it seems well enough for my purposes. I want to use this metric to rank pokemon so if I need to simulate some more outcomes between two pokemon then I won’t mind. For more real-time purposes, the following tweaks seem to be important;\nNew matches are allocated randomly, which is suboptimal from a learning perspective. If you want to fine tune the top 10 players it makes sense to have them play against eachother if you are unsure who is number 1. If instead you’d have one of these players compete against somebody mediocre you can predict the game outcome before it even started. This means you would not learn anything from that game.\nI am using buckets to simulate a distribution for a player. It may be better to use Beta distributions instead and save it’s parameters. This would make the blogpost harder to read for more novice readers which is the main reason of not doing it, but this should improve the stability and remove the need for the hard coded smoothing parameter in the cut_matrix function.\nFor the pokemon dataset I only have one concern; the effect of rock paper scissors.\nPokemon Data\nI took the liberty of preparing a dataset. It’s easy to do yourself with a bit of python and this project. I’ll use the formula from my previous post here again to create a function that can simulate the outcome of a pokemon battle. It will be based on the number of turns that one pokemon can outlast the other.\n\\[ T_{ij} = \\frac{HP_i}{DMG_{ji}} \\]\nwhere \\(T_{ij}\\) is the number of turns the pokemon \\(i\\) would be able to survive the opponent \\(j\\) if it were to be attacked indefinately,\\(HP_i\\) is the amount of hitpoints that pokemon \\(i\\) has and \\(DMG_{ji}\\) is the amount of damage a basic attack pokemon \\(j\\) would deal to pokekom \\(i\\). This damage is defined via:\n\\[ DMG_{ji} = \\frac{2L_j + 10}{250} \\times \\frac{A_j}{D_i} \\times w_{ji} \\]\nThe situation will very much assume some core aspects of the game away. The hope is that my making this assumption we’ll still leave much of the actual pokemon game intact. For ease of use, you can read the datasets from my hosted blog.\npoke_df = pd.read_csv(\"http://koaning.io/theme/data/full_pokemon.csv\")\nweak_df = pd.read_csv(\"http://koaning.io/theme/data/pokemon_weakness.csv\")\n\ndef weakness_weight(types_j, types_i): \n    res = 1.\n    possible_types = [t for t in weak_df['type']]\n    for i in filter(lambda t: t in possible_types, types_i):\n    for j in filter(lambda t: t in possible_types, types_j):\n    res = float(weak_df[weak_df['type'] == j][i])\n    return res\n\ndef calc_tij(poke_i, poke_j):\n    poke_row_i = poke_df[poke_df['name'] == poke_i].iloc[0]\n    poke_row_j = poke_df[poke_df['name'] == poke_j].iloc[0]\n    dmg_ji = (200. + 10)/250\n    dmg_ji = float(poke_row_j['attack'])/float(poke_row_i['defence'])\n    dmg_ji *= weakness_weight(eval(poke_row_j['type']), \n                              eval(poke_row_i['type']))\n    return poke_row_i['hp']/dmg_ji\n\ndef sim_poke_battle(poke_i, poke_j):\n    if calc_tij(poke_i, poke_j) > calc_tij(poke_j, poke_i):\n        return poke_i, poke_j\n    return poke_j, poke_i\nI ran some of the code, and the posterior output looked like this:\n\nThe maximum likelihood then gives an insight for the best/worst pokemon.\n          name       mle                 name       mle\n128   Magikarp  0.011007        142     Snorlax  0.877423\n112    Chansey  0.013735        248       Lugia  0.882689\n348     Feebas  0.015845        487   Cresselia  0.883819\n171      Pichu  0.019108        66      Machoke  0.887577\n291   Shedinja  0.020473        713     Avalugg  0.890049\n439    Happiny  0.026763        380      Latios  0.893087\n241    Blissey  0.037009        148   Dragonite  0.895297\n172     Cleffa  0.042802        611     Haxorus  0.895832\n234   Smeargle  0.048575        290     Ninjask  0.897319\n49     Diglett  0.054010        388    Torterra  0.902103\n400  Kricketot  0.057252        717     Yveltal  0.904122\n297    Azurill  0.058177        646      Kyurem  0.914460\n235    Tyrogue  0.060337        485   Regigigas  0.916059\n659   Bunnelby  0.060847        305      Aggron  0.916220\n62        Abra  0.064720        597  Ferrothorn  0.916377\n173  Igglybuff  0.068975        425    Drifblim  0.917240\n190    Sunkern  0.071101        644      Zekrom  0.919065\n279      Ralts  0.081876        537       Throh  0.921175\n160    Sentret  0.082374        482      Dialga  0.928502\n51      Meowth  0.083082        149      Mewtwo  0.951472\n174     Togepi  0.087797        483      Palkia  0.953823\n581  Vanillite  0.091351        288     Slaking  0.956629\n212    Shuckle  0.095570        375   Metagross  0.957383\n18     Rattata  0.100470        492      Arceus  0.957729\n330     Cacnea  0.100798        716     Xerneas  0.960467\nConclusion\nI have run this algorithm a few times and it seems doesn’t seem to converge very consistently. It does some things very well; the fact that magicarp is generally performing poorly is good news. I’ve also noticed Mew/Mewtwo in the higher skilled area. I’m still concerned that this algorithm fails to grasp the rock/paper scissors element that is in the game. But the fact that I am assuming that every pokemon is fighting with only basic attacks also does not help.\nAs a final push one might consider making an ensemble of these pokemon rankings to see if the convergence can be make to go faster. Another tweak to consider is to have the matching of these pokemon not be random during the learning. I like the results of the exercise, but it seems that we are not done with ranking pokemon.\n\n\n\n",
    "preview": "posts/pokemon-recommendations-part-2/pokemon-recommendations-part-2_files/pokemon-part2-01.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 1112,
    "preview_height": 462
  },
  {
    "path": "posts/lego-minifigures/",
    "title": "Lego Minifigures",
    "description": "An investment opportunity and sampling.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2016-01-26",
    "categories": [],
    "contents": "\n\nIn this document I’ll explain a fun scraping/simulation exercize where I try to investigate the value of investing in lego minifigures. The scraping result will immediately suggest that investing in minifigures might be a profitable venture. The stochastic process behind it requires more work, but might have interesting side effects for decision theorists.\nLego Minifigures\nI bought myself a simpsons minifigure for my birthday. I grabbed a shiny package, bought it and was lucky enough to get the actual Homer Simpson figure. Legos are a popular collectible and so are all things simpsons. I then went to ebay to discover that there might be a surplus in buying packages and selling complete minifigure sets.\nEach set contains 16 figures and each figure costs $3. Let’s download some data. We’ll use common R packages.\nlibrary(purrr) \nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(rvest)\nlibrary(parallel)\nI’ve taken the liberty of acquiring some prices for full sets of lego minifigures. I’ve got a set of prices for a set of simpsons minifigures and a set of any lego series.\nsimpson_prices <- c(58.99, 59.99, 120.00, 65.00, 100.00, 74.99, 115.00, 114.95, 129.99, 60.00, 92.72, 99.87, 49.92, 55.63, 114.14, 142.67, 71.34, 57.07, 54.20, 49.22, 57.05, 65.61, 57.05, 78.40, 121.26, 57.05, 51.29, 142.66, 71.32, 78.45, 106.99, 189.98, 92.99, 59.99, 76.87, 90.00, 324.99, 82.49, 59.88, 75.00, 78.00, 117.28, 50.00, 129.99, 137.77)\n\nall_series_prices <- c(59.99, 58.99, 59.99, 120.00, 65.00, 100.00, 69.95, 76.99, 74.99, 114.95, 129.99, 76.87, 69.90, 74.95, 115.00, 82.99, 78.45, 84.00, 60.00, 185.46, 60.00, 71.99, 75.99, 299.99, 85.59, 85.60, 179.99, 72.69, 109.99, 89.99, 92.72, 87.99, 84.29, 199.00, 258.70, 122.00)\nThese prices can a distribution, to make it a bit smoother I use bootstrapping before plotting.\nn_samples <- 20000\ndf <- data.frame(\n  simpsons_prices = 1:n_samples %>% \n    map_dbl(~simpson_prices %>% sample(30, replace = TRUE) %>% mean),\n  all_prices = 1:n_samples %>% \n    map_dbl(~all_series_prices %>% sample(30, replace = TRUE) %>% mean)\n)\n\nggplot() + \n  geom_histogram(data = df %>% gather(key, value), \n  aes(value), binwidth = 1) + \n  facet_grid(key ~ .)\n\nThe complete simpsons set seems to average about 96 dollars for a full set, the which is lower than the average for all minifigures $110. This is probably due to the simpsons set being more recent and therefore less rare.\nOne might wonder though, given these numbers, can we earn a margin by investing in lego minifigures? You can try to tackle this problem with math, but I prefer sampling to keep things simple. Distributed sampling FTW!\ncores <- detectCores() # 8 cores on my machine\nk <- 16 \nnew_row <- function(n){\n  s <- 3000\n  p <- 1:s %>% \n    map_dbl(~sample(1:k, n, replace = TRUE) %>% unique %>% length) %>% \n    map_dbl(~. == 16) %>% \n    sum\n  c <- 1:s %>% \n    map_dbl(~sample(1:k, n, replace = TRUE) %>% \n              factor(levels=as.character(1:16)) %>% \n              table %>% min) %>% \n    mean\n  data.frame(n = n, p = p/s, c = c)\n}\n\nres <- mclapply(1:800, new_row, mc.cores = cores)\n\ndf <- res %>% reduce(rbind)\nI can now plot the estimated probability of getting a set after ‘n’ figures as well as the expected number of sets.\nggplot(data=df %>% filter(n < 100)) + \n  geom_point(aes(n, c)) + \n  geom_line(aes(n, p)) + \n  ggtitle('estimated likelihoods after buying \"n\" packets')\n\nThe profit that we can make depends on what price we can sell our excess minifigures for. If we can sell them for 2 dollars (instead of the original 3 dollars) then we are definately in the money if we buy in bulk.\nBy creating a slightly different visualisation, we can see that the number of minifigures needed to get a new set gets shorter the more minifigures we have.\ndf <- df %>% \n  mutate(sets = round(c))\n\nsamplr <- data.frame(\n  n = runif(10000, 16, 200) %>% round\n) %>% \n  mutate(s = n %>% map_dbl(~sample(1:k, ., replace = TRUE) %>% \n              factor(levels=as.character(1:16)) %>% \n              table %>% min)\n)\n\nggplot() + \n  geom_point(data=samplr, aes(n, s), alpha = 0.1) +\n  geom_line(data=df %>% filter(n < 200), aes(n, sets), \n  color = 'steelblue', size = 2) \n\nProblem with interesting side effects\nSuppose that this is an investment opportunity. An interesting property of this investment problem is that the profit from investment increases with the size of the investment. It becomes easier to see if we numerically differentiate the expected number of sets over the number of figures bought.\nma <- function(arr, n=15){\n  res = arr\n  for(i in n:length(arr)){\n    res[i] = mean(arr[(i-n):i])\n  }\n  res\n}\n\ndf <- df %>% \n  mutate(diff = c - lag(c)) %>% \n  filter(!is.na(diff), diff > -1) %>% \n  mutate(ma1 = ma(diff), ma2 = ma(ma1), ma3 = ma(ma2))\n  \nggplot() + \n  geom_point(data=df, aes(n, diff)) + \n  geom_line(data=df, aes(n, ma1))\n\nggplot() + \n  geom_line(data=df, aes(n, ma3)) + \n  ggtitle(\"estimated increased number of sets per minifigure bought, more smoothing\")\n\n\nIt hasn’t completelty seemed to converge. I suppose it might make sense that if you do this until infinity that the convergence would be 1/16 because in the limit you expect to have so many minifigures as a spare that it converges to getting a full set every 16 items you buy (assuming that the likelihood of every figure is equal).\nConclusion\nStochastics like this are very interesting. When you are trying to get your first set, it is hard to get the 16th figure. Once you have your first set, it becomes easier because you’ve probably got some minifigures left from the last set. This effect stacks. In fact it stacks sofar that you get an unfair advantage over people who have a limited budget. This might explain the suplus on ebay.\nI may just invest in lego’s this year.\n\n\n\n",
    "preview": "posts/lego-minifigures/lego-minifigures_files/minifigs.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 673,
    "preview_height": 271
  },
  {
    "path": "posts/custom-machine-learning-objects-in-r/",
    "title": "Custom Predictors in R",
    "description": "It's different than Python, but S3 isn't *that* bad.",
    "author": [
      {
        "name": "Vincent D. Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2015-11-05",
    "categories": [],
    "contents": "\nBefore writing this blogpost I did very little with object oriented code in R. I never really saw it as a useful feature because I am mostly using R as an analysis tool. The power of R comes partly from the fact that other people have done this work for you. Recently though, I’ve been tasked to write a custom machine learning library that needed to support the predict function.\nThis document will describe the simplest machine learning method ever and some quick details on how to implement it in R via object oriented coding. This post is heavily inspired by this pdf and this tutorial. My goal is to have a similar document but a bit shorter to make it easier for my own reference. I will try to compare python to R wherever possible such that other people may find it useful too.\nS3 Classes\nWhere python has dictionaries, R has lists.\nobj <- list(a = 1, b = 2)\nInstead of having methods within these objects, R uses functions that can accept many different types of objects. Notice below that I am using the same function on a list as I am on a dataframe.\n> names(obj) \n# \"a\" \"b\"\n> names(ChickWeight)\n# \"weight\" \"Time\"   \"Chick\"  \"Diet\" \nWhere python has polymorphism, R (and Julia by the way) has multiple dispatch. Methods do not belong to objects, they belong to functions. Instead of binding a method to an object, R allows you to write many functions that share the same name but refer to different objects. For example, if you want to check out what objects can be handed to summary:\n> methods(summary)\n [1] summary.aov                    summary.aovlist*               summary.aspell*               \n [4] summary.check_packages_in_dir* summary.connection             summary.data.frame            \n [7] summary.Date                   summary.default                summary.ecdf*             \n ...   \nYou could also check for all methods that below to a certain class.\n> methods(class = \"Date\")\n [1] -             [             [[            [<-           +             as.character \n [7] as.data.frame as.list       as.POSIXct    as.POSIXlt    Axis          c            \n[13] coerce        cut           diff          format        hist          initialize   \n[19] is.numeric    julian        Math          mean          months\n...\nIf the function mean() were called on a Date object it would internally call mean.Date(). To keep track of what method a function should use R looks at the name of the function. In the case of mean.Date() the S3 class in R would recognize that the function mean can be used for Date by looking at the name. A silly example; foo.bar() would allow R to recognize that the function foo can be used on a bar object.\nThis should feel very odd if you are a python programmer because it puts a lot of functions in the global namespace.\nLet’s create an object of type foo again, just to be explicit.\nobj <- list(a = 1, b = 2)\nclass(obj) <- 'foo' \n\n> class(obj) \n\"foo\"\nWe will now create a method mean.foo which will be called by the generic function mean if it is passed an object of class foo.\nmean.foo <- function(x){\n  (x$a + x$b)/2\n}\n\n> mean(obj) \n1.5\nThe only thing missing right now is a way to generate our own generic function. Just like mean was a generic function here, we might want to create a generic function that can be used on multiple objects.\nf <- function(x) UseMethod(\"f\")\nf.foo <- function(x){\n  paste(x$a, \"and\",  x$b, \"are in this foo obj\")\n}\nf.numeric <- function(x){\n  paste(\"this numeric has value\", x)\n}\n\n> f(obj)\n[1] \"1 and 2 are in this foo obj\"\n> f(2)\n[1] \"this numeric has value 2\"\nThe model\nI want to make a model that assumes a continous variable \\(y\\) and a discrete input \\(X\\). It will average \\(y\\) over all the \\(X\\) combinations.\nTo create this machine learnin model I want a generic function that returns an object with a class. As long as I create a function via UseMethod that returns a list with an assigned class this should work.\nlibrary(dplyr)\n\naggmod <- function(x, ...) UseMethod(\"aggmod\")\n\naggmod.default <- function(form, data, ...){\n  res <- list()\n  \n  agg <- aggregate(formula = form, FUN = mean, data = data)\n  colnames(agg) <- c(form %>% all.vars %>% tail(-1), \"pred\")\n  res$agg <- agg\n  \n  res$call <- match.call()\n  res$formula <- form\n  res$fitted.values <- data %>% left_join(res$agg) %>% .$pred\n  res$y <- data %>% select_(form %>% all.vars %>% head(1))\n  res$residuals <- res$y - res$fitted.values\n  res$mae <- mean(sum(abs(res$residuals)/length(res$residuals)))\n  res$mse <- mean(sum(res$residuals^2)/length(res$residuals))\n  \n  class(res) <- \"aggmod\"\n  res\n}\nThe .default method can be seen as a constructor. When we call aggmod() function it will point to the aggmod.default method and return a list of class aggmod. This object still needs some utility generics. Currently, this object has no pretty print representation and can also not be passed into the predict function.\nprint.aggmod <- function(x, ...){\n  cat(\"Call:\\n\")\n  print(x$call)\n  cat(\"\\nMSE:\")\n  print(x$mse)\n  cat(\"MAE:\")\n  print(x$mae)\n}\n\npredict.aggmod <- function(x, newdata = NULL, ...){\n  if(is.null(newdata)) return(fitted(x))\n  newdata %>% left_join(x$agg) %>% .$pred\n}\nWith this in place, it starts to feel like using the lm function.\nmodl <- aggmod(weight ~ Time + Diet, ChickWeight) \n> modl %>% print\nCall:\naggmod.default(form = weight ~ Time + Diet, data = ChickWeight)\n\nMSE:[1] 631109.7\nMAE:[1] 11692.11\n\n> predict(modl, newdata = ChickWeight %>% sample_n(5))\nJoining by: c(\"Time\", \"Diet\")\n[1] 187.70000  47.25000  79.68421  64.50000  66.78947\nConclusion\nI found this exercize very helpful in understanding the R way of dealing with objects. If you are from a different programming language this may feel like a very strange way of doing things but it has it’s benefits. By allowing our code to be written this way, we can do the following;\nformulas <- c(weight ~ Time, weight ~ Time + Diet, weight ~ Diet)\nml_methods <- list(lm, aggmod)\ndf <- data.frame(variables = as.character(), \n                 model = as.character(), \n                 median_mse = as.numeric())\n\nmse <- function(x,y){\n  diff <- x - y\n  mean(sum(diff^2)/length(diff))\n}\n\nfor(f in formulas){\n  for(m in ml_methods){\n    mod <- m(f, ChickWeight)\n    df <- df %>% rbind(data.frame(\n      variables = f %>% all.vars %>% tail(-1) %>% paste(collapse=' '),\n      model = mod$call %>% as.character %>% .[1], \n      median_mse = mse(mod %>% predict, ChickWeight$weight)\n    ))\n  }\n}\nHaving a generic predict allows an R user to focus on the statistics because there is a common expectation of how an object should interact with it. Not all programmers will like this style, some may say that it offers too much sugar. Another example of things that feel trippy to programmers;\n`%+%` <- function(a, b){\n  paste(a,b, sep ='')\n}\n\n> 'a' %+% 'b' %+% 'c'\n[1] \"abc\"\nWhere python can make use of polymorphism for it’s operators, R imposes different rules, but allows you to write your own operators. Most statistician will enjoy this because this syntax allows them only worry about doing statistics with code that feels natural to them.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {}
  },
  {
    "path": "posts/ensemble-yourself/",
    "title": "Ensemble Yourself",
    "description": "Scribbles as an Algorithm Service.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2015-09-17",
    "categories": [],
    "contents": "\nThis document should feel like a game. One that will help you understand how an ensemble algorithm might work. It might even help demonstrate how these algorithms might perform better.\nHuman Algorithm\nYou should see a few points and a grayed canvas that you can draw on with the mouse (or finger on a table). It is a random, noisy, subset of a larger set of data points. The subset has noise and we are going to try to get a good prediction out of it while using a “suboptimal” machine learning model (namely, you). Feel free to draw as complex as possible, but try to draw lines that summarise the dots.\n\n.newline {\n  fill: none;\n  stroke: #000;\n  stroke-width: 2px;\n  stroke-linejoin: round;\n  stroke-linecap: round;\n}\n.oldline {\n  fill: none;\n  stroke: #444;\n  stroke-width: 2px;\n  stroke-linejoin: round;\n  stroke-linecap: round;\n}\nUse the mouse to draw a line that matches the pattern you think fits on that small bit of data. When you are done drawing, another sample will appear. Try to continue drawing lines until a pattern emerges.\n\n\nShow prediction \n\nArt of combining\nWhen you’ve done this a few times; hit the show prediction button on top to see the mean of all the small predictions you’ve made. Even if every small prediction has some form of error, by making many of these predictions, the combined error cancels out quite nicely. Below you can see the difference between what you’ve predicted and what the true function was.\nAutomation for Profit\nNaturally, the more samples you’ll try to fit, the better the prediction will be because you’ll cancel out any noise generated (either from the sample or from the prediction). At some point, you may imagine that it gets rather bothersome to have human make these small approximations. So instead usually, we’d have a computer do it. Computers lack the intuition to draw pretty curves, so per sample they will produce a worce model but they make up for it in speed. If you press the play button, the computer will attemp to beat your prediction by drawing straight lines only.\n\nPlay.\n\n\nThis is the intuition behind ensemble models. By combining a lot of simple models, we may compete with a very complicated one. This is why ensemble algorithms in machine learning are like the genetic algorithms in operations research. The mathematical foundation is a bit uninspiring but they tend to work very well for a lot of problems.\nSide Note\nIt should be clear that the method of selecting subsets is paramount to the performance of the algorithm. If I select points at random over the entire landscape then my model will come with relatively “flat” lines that need to be ensemble. By sampling items close together, every part of the function gets mapped properly.\n\n\nvar activeLine,\n    line_data = [],\n    ml_data = [];\n\nvar w = d3.select(\"svg\").node().clientWidth,\n    h = d3.select(\"svg\").node().clientHeight;\n    \nconsole.log(h, w);\n\nvar renderPath = d3.svg.line()\n    .x(function(d) { return d[0]; })\n    .y(function(d) { return d[1]; })\n    .interpolate(\"line\");\n\nvar svg_draw = d3.select(\"svg.draw\")\n      .call(d3.behavior.drag()\n      .on(\"dragstart\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"dragend\", dragended));\n\nvar svg_show = d3.select(\"svg.show\"),\n    svg_automate = d3.select(\"svg.automate\");\n\nvar g_newcircle = svg_draw.append(\"g\")\n    .attr(\"class\", \"temp-circle\");\n\nvar g_showpattern = svg_draw.append(\"g\")\n    .attr(\"class\", \"pattern-circle\");\n\nvar g_autocircle = svg_automate.append(\"g\")\n    .attr(\"class\", \"temp-auto-circle\");\n\nvar g_autoline = svg_automate.append(\"g\")\n    .attr(\"class\", \"automated-lines\");\n\nfunction dragstarted() {\n  activeLine = svg_draw.append(\"path\")\n    .datum([])\n    .attr(\"class\", \"newline\");\n}\n\nfunction dragged() {\n  activeLine.datum().push(d3.mouse(this));\n  activeLine.attr(\"d\", renderPath);\n}\n\nfunction dragended() {\n  activeLine = null;\n  var new_data = _.last(d3.selectAll(\"path.newline\").data());\n  line_data.push(new_data);\n  svg_draw.selectAll(\"path\")\n    .style(\"stroke-width\", 1)\n    .style(\"stroke\", \"#999\")\n    .style(\"stroke-opacity\", 0.7);\n  svg_draw.selectAll(\"g.temp-circle circle\").remove();\n  draw_sample(g_newcircle, sample_data());\n  if(d3.select(\"input[name=predictions]\").property(\"checked\")){\n    draw_mean(svg_draw, line_data)\n  }\n  draw_mean(svg_show, line_data)\n}\n\nvar func_dict = {\n    \"func1\" : function(x){\n        var trans_x = x/w*2*Math.PI\n        var res = Math.sin(trans_x)*h*0.3 + h/2;\n        return res + Math.cos(trans_x*3)*h*0.2\n    }\n}\n\nfunction sample_data(proximity){\n    proximity = typeof proximity !== 'undefined' ? proximity : 10;\n    var orig_func = func_dict[\"func1\"];\n    var noise_func = function(x){\n        return orig_func(x) + d3.random.normal(0,h/proximity)();\n    }\n\n    var rand_func = d3.random.normal(Math.random()*w, w/proximity)\n\n    return d3.range(25)\n        .map(function(d){ return rand_func()})\n        .map(function(x){ return{'x':x, 'y': noise_func(x)}});\n}\n\nfunction draw_sample(svg_elem, data){\n  svg_elem.selectAll(\"circle\")\n    .data(data)\n    .enter().append(\"circle\")\n    .attr(\"class\", \"temp-circle\")\n    .attr(\"cx\", function (d) { return d.x; })\n    .attr(\"cy\", function (d) { return d.y; })\n    .attr(\"r\", 3)\n    .style(\"fill\", function(d) { return 'steelblue'; });\n}\n\ndraw_sample(g_newcircle, sample_data());\n\nfunction make_model(data_input){\n    var flat_list =  _.flatten(data_input),\n        grouped = _.groupBy(flat_list, function(x){\n            return d3.round(x[0]/5) * 5\n        }),\n        agg = _.mapValues(grouped, function(x){\n            return d3.mean(x.map(function(d){return d[1]}))\n        });\n\n    var smooth = function(d,i,l){\n        if(i == 0){\n            i = 1;\n        }\n        if(i === l.length - 1){\n            i = l.length - 2\n        }\n        return [d[0], d3.mean([l[i-1][1], d[1], l[i+1][1]])]\n    }\n\n    return _.chain(agg)\n        .pairs()\n        .map(function(x){ return [Number(x[0]), x[1]]})\n        .value().map(smooth);\n}\n\nfunction draw_mean(svg_elem, line_data, input_data){\n    svg_elem.selectAll(\"g.mean-container\").remove();\n    svg_elem.append(\"g\")\n        .attr(\"class\",\"mean-container\")\n        .selectAll(\"circle\")\n        .data(make_model(line_data))\n        .enter().append(\"circle\")\n        .attr(\"class\", \"mean-circle\")\n        .attr(\"cx\", function (d) { return d[0]; })\n        .attr(\"cy\", function (d) { return d[1]; })\n        .attr(\"r\", 2)\n        .style(\"fill\", function(d) { return 'red'; });\n}\n\nfunction draw_pattern(svg_elem){\n    var func = func_dict[\"func1\"];\n\n    var data = d3.range(1,w).map(function(x){ \n        return {'x':x, 'y':func(x)}\n    });\n\n    var line = d3.svg.line()\n        .x(function(d){return d.x})\n        .y(function(d){return d.y})\n        .interpolate(\"linear\");\n\n    svg_show.append(\"path\")\n        .attr(\"d\", line(data))\n        .style(\"stroke\", function(d) { return 'green'; })\n        .style(\"fill\", \"none\");\n}\n\nfunction lm(x,y){\n    var mod = {},\n        n = y.length,\n        sum_x = 0,\n        sum_y = 0,\n        sum_xy = 0,\n        sum_xx = 0,\n        sum_yy = 0;\n\n    for (var i = 0; i < y.length; i++) {\n        sum_x += x[i];\n        sum_y += y[i];\n        sum_xy += (x[i]*y[i]);\n        sum_xx += (x[i]*x[i]);\n        sum_yy += (y[i]*y[i]);\n    } \n\n    mod['slope'] = (n * sum_xy - sum_x * sum_y) / (n*sum_xx - sum_x * sum_x);\n    mod['intercept'] = (sum_y - mod.slope * sum_x)/n;\n\n    return function(d){ return mod['intercept'] + mod['slope'] * d};\n};\n\nfunction gen_lm_data(data){\n    var x = data.map(function(d){return d.x}),\n        y = data.map(function(d){return d.y}),\n        xbounds = d3.extent(x);\n\n    var xs = d3.range(xbounds[0], xbounds[1])\n    var ys = xs.map(lm(x,y));\n    return _.zip(xs, ys).map(function(d){return {x:d[0], y:d[1]}})\n}\n\nfunction draw_line(svg_elem, data){\n    var sorted = _.sortBy(data, function(d){return d.x});\n    var first = data[2],\n        last = data[data.length -2];\n\n    var line = d3.svg.line()\n        .x(function(d) { return d.x; })\n        .y(function(d) { return d.y; })\n        .interpolate(\"linear\");\n\n    svg_elem.append(\"path\")\n        .data([first, last])\n        .attr(\"d\", line([first, last]))\n        .style(\"stroke-width\", 1)\n        .style(\"stroke\", \"#999\")\n        .style(\"stroke-opacity\", 0.7);\n}\n\nfunction ml_play(){\n    var n = 0; \n    svg_automate.selectAll(\"path\").remove();\n    ml_data = [];\n    var interval = setInterval(function(){\n            svg_automate.selectAll(\"g.temp-auto-circle circle\").remove();\n            var s_data = sample_data(20);\n            var mod_data = gen_lm_data(s_data);\n            ml_data.push(mod_data.map(function(d){return [d.x, d.y]}));\n            draw_sample(g_autocircle, s_data);\n            draw_line(g_autoline, mod_data);\n            n = n + 1; \n            if(n > 200){\n                clearInterval(interval)\n            }\n            draw_mean(svg_automate, ml_data)\n        }, 50);\n}\n\ndraw_pattern(g_showpattern)\n\nd3.select(\"input[name=predictions]\").on(\"click\", function(){ \n    if(d3.select(\"input[name=predictions]\").property(\"checked\")){\n        draw_mean(svg_draw, line_data)\n    }else{\n        svg_draw.selectAll(\"g.mean-container\").remove();\n    }\n})\n\nd3.select(\"button.btn\").on(\"click\", ml_play)\n\n\n\n\n",
    "preview": "posts/ensemble-yourself/ensemble-yourself_files/img.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 795,
    "preview_height": 366
  },
  {
    "path": "posts/pokemon-recommendations-part-1/",
    "title": "Pokemon Recommendations, Part 1",
    "description": "An Attempt at an OptimalPortfolio[tm]",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2015-05-06",
    "categories": [],
    "contents": "\nThe goal of this document is to try to explain how you can apply a bit of mathematics to pick an optimal portfolio of pokemon. Why? Pokemon has left a mark on this world and even though I’ve never been the biggest fan, it’s part of my generation. Everybody that I can relate with knows what pokemon is. This makes a pokemon dataset very nice to play and teach with. It is an accessible dataset for a lot of people which makes the math somewhat easier to swallow.\nPokemon theory.\nFor those not familiar: there exists such thing as a pokemon type There are grass pokemon, fire pokemon, water pokemon, etc. It is a bit like rock/paper/scissors but for genetically engineered superanimals. For example; plant pokemon are weak against fire pokemon, which in turn are weak against water pokmon which in turn are weak against plant pokemon.\nTo give an impression of how complex the pokemon weakness system can be (source):\n\nA pokemon also has other characteristics that are non specefic to its type. Some are inheretly stronger than others and each pokemon has different properties (some have more health, some have more attack).\nThe goal of this document is to figure out what an optimal portfolio of pokemon is. After all; a pokemon master can only have 5 pokeballs (prisons for pokemon that you can carry around) and it is unknown which pokemon the opponent has.\nSome Math\nYou can google how damage between pokemon is determined and you can download pokemon information from a reliable api. To compare pokemon I will take the number of turns that a pokemon would be able to stand against an opponent. Because I want to capture that a single pokemon might be able to beat two other pokemon in succession but might be beaten as well this seems like a good metric.\nNote that for the remained of this document I assume that each pokemon will only use it’s basic attack and no items will be used during combat. Without these assumption the computation becomes a whole lot harder (maybe even NP-Hard).\nThis ‘turns that a pokemon lasts’ metric is defined by the following formulae:\n\\[ T_{ij} = \\frac{HP_i}{DMG_{ji}} \\]\nwhere \\(T_{ij}\\) is the number of turns the pokemon \\(i\\) would be able to survive the opponent \\(j\\) if it were to be attacked indefinately, \\(HP_i\\) is the amount of hitpoints that pokemon \\(i\\) has and \\(DMG_ji\\) is the amount of damage a basic attack pokemon \\(j\\) would deal to pokemon \\(i\\). This damage is defined via:\n\\[ DMG_{ji} = \\frac{2L_j+10}{250} \\times \\frac{A_j}{D_i} \\times w_{ji}\\]\nwhere \\(L_a\\) is the level of pokemon \\(a\\), \\(A_a\\) is the attack power of pokemon \\(a\\), \\(D_a\\) is the defencive power of pokemon \\(a\\) and \\(w_{ab}\\) is the weakness coefficient between pokemon \\(a\\) and \\(b\\) which is defined by the types of the pokemon. The data that I used for these weaknesses can be found here.\nThis is a simplified model where I assume that all pokemon use simple basic attacks continously. Without this assumption the computation becomes a whole lot harder (maybe even NP-Hard).\nThis is what they look like if you put them in pokedex order.\n\n You could also sort this matrix per group, which might allow you to see the effect of the type of pokemon.\nNotice that this graph is assymetric by design. A green cell indicates that the row will win from the column, a yellow colour indicates a draw and a red cell indicates that the row will loose.\nImmediately you should be able to see that a few pokemon will rarely win (this corresponds to a horizontal red line). You don’t need the data to know that Diglet, Abra and Magicarp suck, but it is a good confirmation.\nOptimal Portfolio\nAssuming that when you are walking the plains of Pokemon-land you can’t predict which pokemon will attack you, you need to average the performance of the pokemon over all others to be able say something about how good of a choice a pokemon is. The average performance is not enough though because we’d also want to be able to account for the risk that a pokemon encounters it’s weakness. A pokemon might perform well on average unless it meets its ultimate foe.\nIn financial mathematics a similar problem occurs with the valuation of a stock portfolio. The stock might perform well on average but it will most likely have a lot of volatility.\nFor this dataset we calculate two values per pokemon:\n\n\\(\\mu_p\\): the average performance of the assosciated pokemon\n\n\n\\(\\sigma_p\\): the variance (or risk) of the assosciated pokemon\n\n\\[\\mu_i = \\frac{\\sum_{j=1}^N T_{ij}}{N} \\] \\[\\sigma_i = \\sqrt{\\frac{\\sum_{j=1}^N (T_{ij} - \\mu_i)^2}{N - 1}} \\]\nThe top 5 performning pokemon according to \\(T_{ij}\\) are:\n                 mu      sigma\nname                          \nMewtwo     2.827446   5.332813\nLapras     2.947499   6.977950\nDragonite  3.099368  11.387448\nSnorlax    3.267974   3.917765\nRhydon     3.341182  12.839635\nNotice that the \\(\\sigma_p\\) values vary. If we plot these values of all pokemon with a positive \\(\\mu_p\\) we can see that a disparity exists.\n\nSome pokemon are more efficient in the return/volatility ratio than others. If I now select all pokemon that have a high return but a low volatility I end up with a different top 5:\n                 mu    sigma\nWigglytuff 2.296484 3.173598\nSnorlax    3.267974 3.917765\nExeggutor  2.349300 5.135687\nMewtwo     2.827446 5.332813\nMuk        2.370720 5.680507\nConclusion\nSo yeah, turns out that you can apply the CAPM model to pokemon.\n\n\n\n",
    "preview": "posts/pokemon-recommendations-part-1/pokemon-recommendations-part-1_files/pokemon_graph.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 568,
    "preview_height": 371
  },
  {
    "path": "posts/linear-models-solving-non-linear-problems/",
    "title": "Linear Models Solving Non-Linear Problems",
    "description": "XOR turns out to be a bad argument.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2015-01-09",
    "categories": [],
    "contents": "\nIn this document I will illustrate that how a logistic regression can hold up against a support vector machine in a situation where you would expect a support vector machine to perform better. Typically logistic regression fails due to the XOR phenomenon that can occur in data, but there is a trick around it.\nThe goal of this document is to convince you that you may need to worry more about the features that go into a model, less about which model to pick and how to tune it.\nDependencies\nFor this experiment I will use python and I’ll assume that the following libraries are loaded:\n    import numpy as np \n    import pandas as pd\n    import patsy \n    from ggplot import * \n    from sklearn.linear_model import LogisticRegression\n    from sklearn.svm import SVC\n    from sklearn.metrics import confusion_matrix\nGenerate demo data\nda = np.random.multivariate_normal([1,1], [[1, 0.7],[0.7, 1]], 300)\ndfa = pd.DataFrame({'x1':da[:,0], 'x2': da[:,1], 'type' : 1})\n\ndb1 = np.random.multivariate_normal([3,0], [[0.2, 0],[0, 0.2]], 150)\ndfb1 = pd.DataFrame({'x1':db1[:,0], 'x2': db1[:,1], 'type' : 0})\n\ndb2 = np.random.multivariate_normal([0,3], [[0.2, 0],[0, 0.2]], 150)\ndfb2 = pd.DataFrame({'x1':db2[:,0], 'x2': db2[:,1], 'type' : 0})\n\ndf = pd.concat([dfa, dfb1, dfb2])\n\n(ggplot(aes(x='x1', y='x2', color=\"type\"), data=df) + \n  geom_point() + \n  ggtitle(\"sampled data\"))\nThis should show a dataset similar to this one:\n\n\n\n\nThis is a textbook XOR problem right here.\nWe’ve generated a classification problem that is impossible to split linearly. Typically we would expect the (linear) logistic regression to perform poorly here and we would expect a (non-linear) support vector machine to perform well.\ny,X = patsy.dmatrices(\"type ~ x1 + x2\", df)\nLet’s see how both models perform now.\nLogistic Regression\n> pred = LogisticRegression().fit(X,ravel(y)).predict(X)\n> confusion_matrix(y,pred)\n    array([[223,  77],\n            [118, 182]])\nSupport Vector Machine\n> pred = SVC().fit(X,ravel(y)).predict(X)\n> confusion_matrix(y, pred)\n    array([[294,   6],\n           [  2, 298]])\nThe SVM performs much better than the LR. How might we help this?\nFeature Trick\nLet’s see if we can help the logistic regression out a bit. Maybe if we combine x1 and x2 into something nonlinear we may be able capture the problem in this particular dataset better.\n> df['x1x2'] = df['x1'] * df['x2']\n> y,X = patsy.dmatrices(\"type ~ x1 + x2 + x1x2\", df)\n> pred = LogisticRegression().fit(X,ravel(y)).predict(X)\n> confusion_matrix(y,pred)\n  array([[290,  10],\n         [  3, 297]])\nI am feeding different data to the logistic regression, but by combining x1 and x2 we have suddenly been able to get a non-linear classification out of a linear model. I am still using the same dataset however, which goes to show that being creative with your data features can have more of an effect than you might expect.\nNotice that the support vector machine doesn’t show considerable improvement when applying the same trick.\n> pred = SVC().fit(X,ravel(y)).predict(X)\n> confusion_matrix(y, pred)\n> array([[294,   6], \n         [  2, 298]])\nConclusion\nWhy is this trick so useful?\nYou can apply more statistical theory to the regression model which is something a lot of clients (especially those who believe in econometrics) find very comforting. It is less a black box and feels like you might be better prepared for when something goes wrong.\nThe main lesson here is, before you judge a method useless, it might be better to worry about putting useful data in it first.\n\n\n\n",
    "preview": "posts/linear-models-solving-non-linear-problems/linear-models-solving-non-linear-problems_files/non-linear-data.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 540,
    "preview_height": 504
  },
  {
    "path": "posts/variable-selection/",
    "title": "Variable Selection in Machine Learning",
    "description": "Merely *a* argument, but one that I like.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2014-12-18",
    "categories": [],
    "contents": "\nI’ve had a discussion with a colleague on the selection of variables in a model. The discussion boils down to the following question:\nWhich is better?\nsupply all the variables that you have into the model and remove the ones that add a risk of overfitting\nstart out small and add values to make the model more and more complex?\nYou should always use a test set to determine the performance of your models and you should apply strategies to prevent it from overfitting, but starting our small and growing the model brings inherit bias into the model. In this document I will provide a mathematical proof of what might be dangerous of this approach.\nLinear Algrebra/Regression Primer\nIn this document I’ve assumed that you have some remembrance of college level linear algebra. The following equations should feel readable to you:\n\nWarning, this document is heavy on math.\n\\[\n\\begin{aligned}\nM_x & = I_n - X(X'X)^{-1}X' \\\\\\\nM_x X & = I_nX - X(X'X)^{-1}X'X \\\\\\\n& = X - XI_n\\\\\\\n& = 0 \n\\end{aligned}\n\\]\nFrom statistics you should hopyfully feel familiar with the following:\n\\[\n\\begin{aligned}\nY & = X\\beta + \\epsilon  \\text{        where    } \\epsilon \\sim N(0,\\sigma) \\\\\\\n\\hat{\\beta} & = (X'X)^{-1} X'Y \\\\\\\n\\mathbb{E}(\\hat{\\beta} ) & = \\mathbb{E}\\big((X'X)^{-1} X'Y)\\big)\n\\end{aligned}\n\\]\n\nI am going to proove that for linear models you will introduce bias if you use few variables and include more and more as you are building a model and that this will not happen when you start with a lot of variables and reduce. For each case I will show what goes wrong in terms of the expected value of the \\(\\beta\\) variables.\n\nA few things on notation, I will refer to the following linear regression formula:\n\\[Y = X_1\\beta_1 + X_2\\beta_2 + \\epsilon\\]\nIn this notation, \\(X_1,X_2\\) are matrices containing data, not vectors, such that \\(\\beta_1,\\beta_2\\) are matrices as well.\n\nSmall to Large Problems\n\nSuppose that the true model is given through:\n\\[Y = X_1\\beta_1 + X_2\\beta_2 + \\epsilon\\]\nIf we start out with a smaller model, say by only looking at \\(\\beta_1\\) we would estimate for $ Y = X_1_1 + $ while the whole model should be $ Y = X_1_1 + X_2_2 + $. Then our expected value of \\(\\beta_1\\) can be derived analytically.\n\\[\n\\begin{aligned}\n\\mathbb{E}(\\beta_1) & = \\mathbb{E}\\big((X_1'X_1)^{-1} X_1'Y)\\big)\\\\\\\n& = \\mathbb{E}\\Big((X_1'X_1)^{-1} X_1'\\big(X_1\\beta_1 + X_2\\beta_2 + \\epsilon\\big)\\Big)\\\\\\\n& = \\mathbb{E}\\Big((X_1'X_1)^{-1} X_1'X_1\\beta_1 + (X_1'X_1)^{-1} X_1'X_2\\beta_2 + (X_1'X_1)^{-1} X_1'\\epsilon\\big)\\Big)\\\\\\\n& = \\mathbb{E}\\Big(\\beta_1 + (X_1'X_1)^{-1} X_1'X_2\\beta_2 + (X_1'X_1)^{-1} X_1'\\epsilon\\big)\\Big)  \\\\\\\n& = \\beta_1 + (X_1'X_1)^{-1} X_1'X_2\\beta_2 + (X_1'X_1)^{-1} X_1'\\mathbb{E}(\\epsilon) \\\\\\\n& = \\beta_1 + (X_1'X_1)^{-1} X_1'X_2\\beta_2 \\\\\\\n& \\ne \\beta_1\n\\end{aligned}\n\\]\nSo our estimate of \\(\\beta_1\\) is biased. This holds for every subset of variables \\(\\{\\beta_1, \\beta_2\\}\\) that make up \\(\\beta\\).\nLarge to Small Solution\nSuppose that the true model is given through:\n\\[ Y = X_1\\beta_1 + \\epsilon \\]\nIf we start out with a larger model, say by including some parameters \\(\\beta_2\\) as well while they do not have any influence on the model then we will initially estimate a wrong model \\(Y = X_1\\beta_1 + X_2\\beta_2 + \\epsilon\\).\nA lemma in between\nLet’s define a matrix \\(M_{X_1} = I_n -X_1(X_1'X_1)^{-1}X_1'\\). We can use this matrix to get an estimate of \\(\\beta_2\\).\nStart out with the original formula.\n\\[\n\\begin{aligned}\nM_{X_1}Y & = M_{X_1}X_1\\beta_1 + M_{X_1}X_2\\beta_2 + M_{X_1}\\epsilon \\\\\\\nM_{X_1}Y & = M_{X_1}X_2\\beta_2 + \\epsilon \\\\\\\nX_2'M_{X_1}Y & = X_2'M_{X_1}X_2\\beta_2 + X_2'\\epsilon \\\\\\\nX_2'M_{X_1}Y & = X_2'M_{X_1}X_2\\beta_2 \\\\\\\n\\beta_2 & = ( X_2'M_{X_1}X_2)^{-1}X_2'M_{X_1}Y \\\\\\\n\\end{aligned}\n\\]\nNotice that \\(M_{X_1}X_1 = 0\\) and that \\(M_{X_1}\\epsilon = \\epsilon\\) because of the definition while \\(X_2\\epsilon = 0\\) because \\(\\epsilon\\) is normally distributed around zero and orthogonal to any of the explanatory variables.\nThe derivation for large to small\nWith this definition of \\(\\beta_2\\) we can analyse it to confirm that it should not converge to any other value than zero.\n\\[\n\\begin{aligned}\n\\mathbb{E}(\\beta_2) & = \\mathbb{E}\\big(( X_2'M_{X_1}X_2)^{-1}X_2'M_{X_1}Y\\big) \\\\\\\n& = \\mathbb{E}\\big(( X_2'M_{X_1}X_2)^{-1}X_2'M_{X_1}\\big(X_1\\beta_1 + \\epsilon\\big)\\big) \\\\\\\n& = \\mathbb{E}\\big(( X_2'M_{X_1}X_2)^{-1}X_2'M_{X_1}X_1\\beta_1 + ( X_2'M_{X_1}X_2)^{-1}X_2'M_{X_1}\\epsilon\\big) \\\\\\\n& = \\mathbb{E}\\big(( X_2'M_{X_1}X_2)^{-1}X_2'M_{X_1}\\epsilon\\big) \\\\\\\n& = ( X_2'M_{X_1}X_2)^{-1}X_2'M_{X_1}\\mathbb{E}(\\epsilon) \\\\\\\n& = 0\n\\end{aligned}\n\\]\n\nNotice that \\(( X_2'M_{X_1}X_2)^{-1}X_2'M_{X_1}X_1\\beta_1 = 0\\) because \\(M_{X_1}X_1 = 0\\). So we see that \\(\\beta_2\\) is correctly estimated, what about \\(\\beta_1\\)?\n\n\\[\n\\begin{aligned}\n\\mathbb{E}(\\beta_1) & = \\mathbb{E}\\big((X_1'X_1)^{-1} X_1'Y)\\big)\\\\\\\n& = \\mathbb{E}\\Big((X_1'X_1)^{-1} X_1'\\big(X_1\\beta_1 + \\epsilon\\big)\\Big)\\\\\\\n& = \\mathbb{E}\\Big(\\beta_1 + (X_1'X_1)^{-1} X_1'\\epsilon\\Big)\\\\\\\n& = \\beta_1\n\\end{aligned}\n\\]\nSo in this case we would remove the variables \\(\\beta_2\\) that are not of influence while our estimate of \\(\\beta_1\\) does not have any bias. This is exactly what we want.\nConclusion\nI’ve shown that by starting only a few variables and then adding them to the model has a bias risk in linear models. One can imagine a similar thing happening in other models.\n\n\n",
    "preview": {},
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {}
  },
  {
    "path": "posts/digital-nomad/",
    "title": "Digital Nomad",
    "description": "Some observations but also downsides.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2013-10-14",
    "categories": [],
    "contents": "\nIn 2013 I travelled from Buenos Aires to Patagonia to Lima to Machu Pichu to the Amazones to Colombia to San Fransisco. The trip lasted six months and I worked for two days a week while I was travelling over whatever wifi I could find. I was an independant contractor and lecturer and I still had some clients from Holland.\nThere are a few things I learned from this;\nIt has never been easier to travel and work. If you need a place to live for a month you can just check up on airbnb and if you need a more professional place to work from then there’s plenty of flexible workplaces. You can communicate code with basecamp, call people on skype and you can use git to keep your source code in check.\nTim Ferris actually had a point. I only worked for two days a week and that was enough to live the good life in South America. Earning euro’s and spending pesos gives you a lot of purchasing power.\nIf you are a developer and want honest travel advice your best chance of finding it is at stackexchange and reddit. The sites themselves rock, the travel sections are no exception.\nWifi can both fail and work at suprising places. One town can have 10 Mb/s speeds and the town 10 miles further might have no internet at whatsoever. The wifi in the amazones was better than the wifi in Lima. I remember sitting on a couch in the hostel next to a monkey. As I was feeding him a banana I was pushing commits to the github repo. Patagonia had the worst wifi.\n\n\nThis was the actual postcard that I actually took and sent to my friends. The wifi was indeed horrible.\nYou learn a lot from coding if there is no wifi. Instead of relying on stackoverflow for everything you don’t know you learn think for yourself again. You find the need to save docs beforehand to fully grasp a library instead of just copying snippets of others. This prevents some intellectual lazyness and teaches you to use snippet collector.\nCoding is awesome. Unfortunately certain destinations don’t facilitate this. A place can be amazing to be a tourist in but horrible to be working from. That surfing spot needs to have wifi, that party city needs to have some stability during the week and that in the middle of nature in the middle of nowhere place becomes a risky work location should hardware fail.\nIf you are working on your own there aren’t many collegues that you can learn from. Some cities offer meetups so you can talk to like minded professionals but it’s different. There seems to be a small shortage of like minded developers who are pushing limits that travel the world. The people that you learn from are usually older, have a wife and kids and are probably not travelling the world.\nLearning a computer language and learning a natural language is a similar thing. It gets easier the more and more you use it and you will only really start to understand it if you are forcibly required to do so. A deadline really helps you understand that library in the same fashion that that really cute Argentinian girl motivates you to improve your Spanish.\nYou cannot start a company when teammembers are not sitting next to eachother. It is a nice dream but not a reality in my mind. In early stages of building a startup too many things change too often. You cannot afford ambiguity in the earliest stage. Once a platform is stable however, it would definately be do-able.\nThe relationship between employee and employer requires a huge amount of trust at this distance. The guy on the other side trusts that you will do work for him and you need to trust him that the money will be paid. Not all my clients paid on time, but the ones that did are gained my trust just as I gained theirs.\nFor me the trip would have been worse if I didn’t bring work with me. It was a trial to see if coding was going to be my trade or if something else would make more sense. It was less of a vacation and more of a pilgrimage. By the end of the trip, bars seemed less interesting and books started showing it’s appeal. I learned that I should never have studied econometrics and that computer science or physics would’ve made more sense in hindsight.\nI can recommend such a trip it to anyone, but I cannot imagine (or recommend) anyone doing this for their entire lives. Having to explain yourself over and over again to people can be quite a drag and I really started to miss having proper friends and family around.\n\n\n\n",
    "preview": "posts/digital-nomad/digital-nomad_files/digital-nomad.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 526
  },
  {
    "path": "posts/vanity-metrics/",
    "title": "Vanity Metrics",
    "description": "How I got an A+ for measuring the wrong thing.",
    "author": [
      {
        "name": "Vincent Warmerdam",
        "url": "koaning.io"
      }
    ],
    "date": "2012-09-23",
    "categories": [],
    "contents": "\nAs a 1st year grad student I had a small nighttime job in a local theatre. For one of my courses I needed to apply my first year knowledge of statistics to some real data. A lot of guys would do regressions on stock markets, others went ahead looking for patterns in olympic medalists but I thought it might be nice to do a small analysis on the ticket sales at the local theatre that I was working for.\nMy boss was happy to give me some data because he wanted advice. Every year my boss saw an increase in visitors at his theatre and he was wondering if expanding was a good idea. The theatre was willing to supply me with all the data I needed as long as I reported back to them what my findings were with regards to the expansion.\nSweet, after only one year of college I became a data consultant.\nThe main focus was to predict the amount visitors for the next few years. I knew the amount of visitors per show for every show that the theatre had in the last 10 years. I would use this to do some regression analysis in the hope of being able to get a good trend model.\nBeing a first year grad student I was especially focussed on getting the numbers right. I made sure that the linear and logistic regression output were correct and I triple checked if the outcome was statistically significant.\nHere’s the short story of what I discovered;\n\n\nThis image was made in a time where they taught you illustrator in order to make charts.\nIt seemed like a case closed. My boss might have the intuition that the shows were getting more and more visitors but the growth was stagnating. If there are fewer people going to the theatre it would seem like a bad time to invest in a larger theatre. My recommendation was to invest in marketing instead.\nI saw the data, analyized it (as thuroughly as you could expect from a first year student at uni) and the outcome was obvious. I even got a very nice fit with some regression functions. All the exams had thought me to be very picky when selecting a model but my model prooved to fit very nicely.\nI reported this to my boss, he was happy and impressed. I reported this to my prof and even he was even more happy and even more impressed. He gave me an A+ for the report. He did this largely due to the fact that I found a good way to implement what I have learned in such a relevant fashion.\nVanity\nTwo days later I was working the bar at the theater and the house was packed. The the week after that, same story and the week after that was no different.\nIt hit me. The growth that I had been measuring was a vanity metric.\nThe rise in visitors was declining because the theatre was getting full during each and every show, not because the market of visitors has shrunk. We were measuring the visitors that actually were able to get a seat, not the visitors that wanted to get one!\nNow, five years later, it actually turned out to be a good thing the theatre did not expand. The years after this work would turn out to be the years where the Dutch government started massively reducing the subsidies on the cultural sector. If the theatre would’ve expanded it would have had a very rough future trying to earn back it’s invested money.\nStill, I learned a lesson here. No matter how good you assume you are with the data be sure to keep your eyes open to all things that the data does not tell you. Do not put blind faith in what numbers seem to tell you until you are quite certain that you know what they don’t.\nOne More Thing\nPlease go to your local theatre. There’s real beauty to be seen on stage.\n\n\n",
    "preview": "posts/vanity-metrics/vanity-metrics_files/vanity-metrics.png",
    "last_modified": "2022-02-10T16:02:06+01:00",
    "input_file": {},
    "preview_width": 803,
    "preview_height": 484
  }
]
